{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402379c7-1fa8-42cb-adc5-90d8e7d706c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip list | grep fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2ea7eea-6539-4a55-8f20-5b4fc41ee29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MongoDB successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from uuid import uuid4\n",
    "from pprint import pprint\n",
    "from pymongo import MongoClient, DESCENDING\n",
    "from argparse import ArgumentParser\n",
    "from config import settings\n",
    "collection_name = 'blocks'\n",
    "\n",
    "connection_string = settings.MONGO_CONNECTION_STRING\n",
    "client = MongoClient(connection_string)\n",
    "print(\"Connected to MongoDB successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22dede31-8903-4a4f-9083-31e804117ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.archive\n",
    "docs = db[collection_name].find().sort({'created_at': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "353ec5f1-de6e-4647-8a76-69c2d14b924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>content</th>\n",
       "      <th>created_at</th>\n",
       "      <th>creator</th>\n",
       "      <th>topic</th>\n",
       "      <th>main_thread_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c3b41f3c-692d-4a27-9d98-04b1fa0919d0</td>\n",
       "      <td>Hi &lt;@U066Q9JAU3B&gt;\\nNice to see you at our proj...</td>\n",
       "      <td>2024-04-11T07:49:45.714149</td>\n",
       "      <td>UU43NJY8K</td>\n",
       "      <td>Project Kickoff and Preparation</td>\n",
       "      <td>199c34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bfa16fbb-2201-4863-abfa-6cc9ec25ce33</td>\n",
       "      <td>Hi &lt;@UU43NJY8K&gt;,\\nThanks for inviting us.</td>\n",
       "      <td>2024-04-11T08:22:09.374109</td>\n",
       "      <td>U066Q9JAU3B</td>\n",
       "      <td>Project Kickoff and Preparation</td>\n",
       "      <td>199c34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21e66f4a-fbfc-4bc7-91d6-cc54ec427d57</td>\n",
       "      <td>Hi &lt;@U066Q9JAU3B&gt; &lt;@U06855K24SE&gt;\\nDon't you mi...</td>\n",
       "      <td>2024-04-11T08:59:32.370529</td>\n",
       "      <td>UU43NJY8K</td>\n",
       "      <td>Project Kickoff and Preparation</td>\n",
       "      <td>199c34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>effa84d1-cb39-4ea5-937d-95ea328afe5c</td>\n",
       "      <td>Works for me.</td>\n",
       "      <td>2024-04-11T09:02:28.000959</td>\n",
       "      <td>U066Q9JAU3B</td>\n",
       "      <td>Project Kickoff and Preparation</td>\n",
       "      <td>199c34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7e92acd1-4103-49d0-97db-1e4ebd891e69</td>\n",
       "      <td>Hi Yogesh,\\nPlease do not hesitate to share al...</td>\n",
       "      <td>2024-04-12T08:04:20.532329</td>\n",
       "      <td>UU43NJY8K</td>\n",
       "      <td>Project Kickoff and Preparation</td>\n",
       "      <td>199c34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>2348069f-592d-4de9-94ee-136cee9b9907</td>\n",
       "      <td>No, currently there is no overlap (I need to a...</td>\n",
       "      <td>2024-05-16T07:33:00.645309</td>\n",
       "      <td>U05R0KHPU8N</td>\n",
       "      <td>Going back and forth on the best approach for ...</td>\n",
       "      <td>f0c9ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>6363572d-f176-4851-8592-d46b29c4f6ae</td>\n",
       "      <td>without an overlapping window, we have no way ...</td>\n",
       "      <td>2024-05-16T11:24:58.207359</td>\n",
       "      <td>U066Q9JAU3B</td>\n",
       "      <td>Going back and forth on the best approach for ...</td>\n",
       "      <td>f0c9ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>da42b289-45b7-4e06-88e6-7b469a362188</td>\n",
       "      <td>&lt;@U066Q9JAU3B&gt; Indeed, my recent experiments s...</td>\n",
       "      <td>2024-05-16T11:27:03.332109</td>\n",
       "      <td>U05R0KHPU8N</td>\n",
       "      <td>Going back and forth on the best approach for ...</td>\n",
       "      <td>f0c9ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>ce942fb1-0179-4655-8d38-f9f41475af6d</td>\n",
       "      <td>with the overlap, we can even increase the bat...</td>\n",
       "      <td>2024-05-16T11:30:49.711009</td>\n",
       "      <td>U066Q9JAU3B</td>\n",
       "      <td>Going back and forth on the best approach for ...</td>\n",
       "      <td>f0c9ed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>06f8c352-e986-4934-ab68-5a18ce6b5289</td>\n",
       "      <td>Ok, I'll start with that tomorrow.\\nI am of fo...</td>\n",
       "      <td>2024-05-16T12:26:44.536339</td>\n",
       "      <td>U05R0KHPU8N</td>\n",
       "      <td>Discussion on models and server options</td>\n",
       "      <td>9d3e7c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      _id  \\\n",
       "0    c3b41f3c-692d-4a27-9d98-04b1fa0919d0   \n",
       "1    bfa16fbb-2201-4863-abfa-6cc9ec25ce33   \n",
       "2    21e66f4a-fbfc-4bc7-91d6-cc54ec427d57   \n",
       "3    effa84d1-cb39-4ea5-937d-95ea328afe5c   \n",
       "4    7e92acd1-4103-49d0-97db-1e4ebd891e69   \n",
       "..                                    ...   \n",
       "245  2348069f-592d-4de9-94ee-136cee9b9907   \n",
       "246  6363572d-f176-4851-8592-d46b29c4f6ae   \n",
       "247  da42b289-45b7-4e06-88e6-7b469a362188   \n",
       "248  ce942fb1-0179-4655-8d38-f9f41475af6d   \n",
       "249  06f8c352-e986-4934-ab68-5a18ce6b5289   \n",
       "\n",
       "                                               content  \\\n",
       "0    Hi <@U066Q9JAU3B>\\nNice to see you at our proj...   \n",
       "1            Hi <@UU43NJY8K>,\\nThanks for inviting us.   \n",
       "2    Hi <@U066Q9JAU3B> <@U06855K24SE>\\nDon't you mi...   \n",
       "3                                        Works for me.   \n",
       "4    Hi Yogesh,\\nPlease do not hesitate to share al...   \n",
       "..                                                 ...   \n",
       "245  No, currently there is no overlap (I need to a...   \n",
       "246  without an overlapping window, we have no way ...   \n",
       "247  <@U066Q9JAU3B> Indeed, my recent experiments s...   \n",
       "248  with the overlap, we can even increase the bat...   \n",
       "249  Ok, I'll start with that tomorrow.\\nI am of fo...   \n",
       "\n",
       "                     created_at      creator  \\\n",
       "0    2024-04-11T07:49:45.714149    UU43NJY8K   \n",
       "1    2024-04-11T08:22:09.374109  U066Q9JAU3B   \n",
       "2    2024-04-11T08:59:32.370529    UU43NJY8K   \n",
       "3    2024-04-11T09:02:28.000959  U066Q9JAU3B   \n",
       "4    2024-04-12T08:04:20.532329    UU43NJY8K   \n",
       "..                          ...          ...   \n",
       "245  2024-05-16T07:33:00.645309  U05R0KHPU8N   \n",
       "246  2024-05-16T11:24:58.207359  U066Q9JAU3B   \n",
       "247  2024-05-16T11:27:03.332109  U05R0KHPU8N   \n",
       "248  2024-05-16T11:30:49.711009  U066Q9JAU3B   \n",
       "249  2024-05-16T12:26:44.536339  U05R0KHPU8N   \n",
       "\n",
       "                                                 topic main_thread_id  \n",
       "0                      Project Kickoff and Preparation   199c34  \n",
       "1                      Project Kickoff and Preparation   199c34  \n",
       "2                      Project Kickoff and Preparation   199c34  \n",
       "3                      Project Kickoff and Preparation   199c34  \n",
       "4                      Project Kickoff and Preparation   199c34  \n",
       "..                                                 ...      ...  \n",
       "245  Going back and forth on the best approach for ...   f0c9ed  \n",
       "246  Going back and forth on the best approach for ...   f0c9ed  \n",
       "247  Going back and forth on the best approach for ...   f0c9ed  \n",
       "248  Going back and forth on the best approach for ...   f0c9ed  \n",
       "249            Discussion on models and server options   9d3e7c  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(list(docs)[::-1])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016f675d-dcbb-4dec-9e01-89a6c41bbb12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Discussion about Googles Meet link and code sharing': 'cfe87f',\n",
       " 'Discussion about additional metrics for evaluating similarity of messages': '07807d',\n",
       " 'Discussion about experimenting with AI models and scheduling a meeting': '6c9f78',\n",
       " 'Discussion about project results and feedback': '70737b',\n",
       " 'Discussion on LLM and BERTopic performance': '6e364e',\n",
       " 'Discussion on Model Deployment and Evaluation': '808935',\n",
       " 'Discussion on clustering algorithm and features': '714e81',\n",
       " 'Discussion on models and server options': '9d3e7c',\n",
       " 'GitHub and MongoDB access': 'e264d1',\n",
       " 'Go programming and hybrid search': '6946b3',\n",
       " 'Going back and forth on the best approach for handling machine learning models': 'f0c9ed',\n",
       " 'GoogMeet Invitation and Discussion on Costs': '3ffd05',\n",
       " 'Googledocs and model deployment': '22a66b',\n",
       " 'Greetings and well wishes': '1b3652',\n",
       " 'Issues with Cloudflare and server deployment': '4b0065',\n",
       " 'LLM experimentation': '09e0bd',\n",
       " 'Manual clustering and comparing results': '3b0ebf',\n",
       " 'Meeting Agenda and Next Steps': '96c1af',\n",
       " 'Meeting preparation and clustering project': '786abf',\n",
       " 'Meeting schedule': 'b3c255',\n",
       " 'MongoDB Connection': '271f07',\n",
       " 'MongoDB integration': 'daab1c',\n",
       " 'MongoDB integration and null handling': 'c89633',\n",
       " 'MongoDB integration and server api': 'ad280f',\n",
       " 'Null topics': '94fa2d',\n",
       " 'Performance evaluation and dataset': '018658',\n",
       " 'Project Development': 'aa3e21',\n",
       " 'Project Discussion and Implementation': 'c40fab',\n",
       " 'Project Discussion and Setup': '190061',\n",
       " 'Project Discussion and Understanding': 'e07e36',\n",
       " 'Project Kickoff and Preparation': '199c34',\n",
       " 'Project Update  REST API and development progress': '98b75f',\n",
       " 'Project progress and error rate': 'b4fad4',\n",
       " 'Project progress and updates': '9703ca',\n",
       " 'REST API issues and improvements': '6fbc03',\n",
       " 'RunPod deployment and usage': '0e620d',\n",
       " 'Scheduling a meeting to discuss potential features for improving the clustering algorithm': '0257c3',\n",
       " 'Scheduling meeting and sharing updates': '1af0e0',\n",
       " 'Testing and Integration of REST API': 'c680d4',\n",
       " 'Weekly status reports': '2a0cc3',\n",
       " nan: '716c6e'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = dict(list(df.groupby(['topic', 'main_thread_id']).groups))\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "628f336d-8c24-431c-9c2f-780b4f1025ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_id': 'acc7439c-b',\n",
      "  'topic': ['Project Kickoff and Preparation'],\n",
      "  'topic_messages': ['Hi <@U066Q9JAU3B>\\n'\n",
      "                     'Nice to see you at our project channel :tada:\\n'\n",
      "                     'The other members of our team will be added asap after '\n",
      "                     'our kick-off meeting.',\n",
      "                     '<@U05B5FXLP62> <@U06RU2JKMDG> Here is the dataset for '\n",
      "                     'the project',\n",
      "                     'Hi <@UU43NJY8K>,\\nThanks for inviting us.',\n",
      "                     'Hi <@U066Q9JAU3B> <@U06855K24SE>\\n'\n",
      "                     \"Don't you mind if we shift our kick-off meeting 30 \"\n",
      "                     'minutes tomorrow?\\n'\n",
      "                     'For 7:30 am your time please?',\n",
      "                     'Thank you for your flexibility to get on a call on a '\n",
      "                     'Friday evening. I lived many years in Europe and I know '\n",
      "                     'that this is a big deal!',\n",
      "                     'Here are few files from the folder that I think is a '\n",
      "                     'good mix. I just gave them a cursory look. Please feel '\n",
      "                     'free to choose your own csv files (and let me know why '\n",
      "                     'you chose them).',\n",
      "                     'Hi Yogesh,\\n'\n",
      "                     'Please do not hesitate to share all the information '\n",
      "                     'here.\\n'\n",
      "                     \"I am adding all the team's members to the channel -\\n\"\n",
      "                     '<@U05B5FXLP62> <@U06RU2JKMDG> <@U047WSYBE5T>',\n",
      "                     \"<!here> Hi team :spock-hand: I'm thrilled to be joining \"\n",
      "                     'you all! Looking forward to launching work together.',\n",
      "                     '<@U047WSYBE5T> <@U06RU2JKMDG> How are we going to fund '\n",
      "                     'the GPUs?',\n",
      "                     'the zip folder contains several csv files of different '\n",
      "                     'length (some contain few hundred messages, while the '\n",
      "                     'others contain more than hundred thousand messages.). To '\n",
      "                     'begin with we can start with smaller files and then move '\n",
      "                     'to bigger files.',\n",
      "                     'Works for me.',\n",
      "                     '<@U05B5FXLP62> I was referring to this article. '\n",
      "                     '<https://freedium.cfd/https://generativeai.pub/advanced-rag-retrieval-strategies-auto-merging-retrieval-dc3f869654c4>\\n'\n",
      "                     'Imagine a query like \"Give me the status of project '\n",
      "                     'xyz\". Now there can be multiple '\n",
      "                     \"'topics/conversations'  'related' to project xyz. If we \"\n",
      "                     'provide those topics as context, then the query response '\n",
      "                     'can be quite accurate (Vs providing all messages in all '\n",
      "                     'channels as context)']},\n",
      " {'_id': '0a76a334-c',\n",
      "  'topic': ['Going back and forth on the best approach for handling machine '\n",
      "            'learning models'],\n",
      "  'topic_messages': ['yeah.. making batch size too small will mean we are not '\n",
      "                     'casting big enough net to catch all messages in a '\n",
      "                     'conversation.. looks like a second llm may be a better '\n",
      "                     'option.\\n'\n",
      "                     'Are your batches overlapping? they should be i think.',\n",
      "                     'No, currently there is no overlap (I need to adjust the '\n",
      "                     'backend to allow overlap and handle it correctly).',\n",
      "                     '<@U066Q9JAU3B> Indeed, my recent experiments show this:\\n'\n",
      "                     'batch size 12\\n'\n",
      "                     '\\n'\n",
      "                     'overlap\\n'\n",
      "                     '0 -- 17.4%\\n'\n",
      "                     '2 -- 14.6%\\n'\n",
      "                     '4 -- 11.9%\\n'\n",
      "                     '6 -- 4%\\n'\n",
      "                     '8 -- 1.5%\\n'\n",
      "                     '10 -- 0.2%\\n'\n",
      "                     '\\n'\n",
      "                     'which I believe supports your statement.',\n",
      "                     'without an overlapping window, we have no way to '\n",
      "                     'determine conversation boundaries and we risk forcing a '\n",
      "                     'message inside a conversation it does not belong.\\n'\n",
      "                     'The overlap should be more intelligent (instead of '\n",
      "                     'hardcoding to, say, 10 messages, we could simply include '\n",
      "                     'all messages from last 2 topics with the new batch.. '\n",
      "                     'this gives a chance to the LLMs to pick up new topics '\n",
      "                     'for the messages misclassified in the previous run).',\n",
      "                     'I got what you mean.\\n'\n",
      "                     \"It's nice to have 2 options and being able to switch \"\n",
      "                     'easily.',\n",
      "                     'Hey, my update for today:\\n'\n",
      "                     'I experimented with batch size and this proved out '\n",
      "                     'hypothesis that the batch size should be less than 40. '\n",
      "                     'But if it too small (e.g. 5), LLM starts to break '\n",
      "                     'formatting of the output json, consequently messages '\n",
      "                     'need to be clustered by LLM again.\\n'\n",
      "                     'It looks like the number between 5-15 is optimal.\\n'\n",
      "                     'batch size -- null %\\n'\n",
      "                     '40 -- 34.3%\\n'\n",
      "                     '20 -- 25.7%\\n'\n",
      "                     '10 -- 16.9%\\n'\n",
      "                     '5 -- 7%\\n'\n",
      "                     '\\n'\n",
      "                     '\\n'\n",
      "                     'I plan to spend next 4-5 hours on experiments with '\n",
      "                     'overlapping and adding reasoning to prompt. If null '\n",
      "                     \"still appear in the output, I'll create a second call to \"\n",
      "                     'LLM to classify nulls, having messages from previous 2-3 '\n",
      "                     'topics.\\n'\n",
      "                     \"I'll write another update at the end of my workday \"\n",
      "                     'today.',\n",
      "                     'thanks for the update <@U05R0KHPU8N>.\\n'\n",
      "                     \"It's a possibility that switching from runpod dedicated \"\n",
      "                     'to serverless may slow down the inference throughput.\\n'\n",
      "                     'It will be good to keep our options open - as in, keep '\n",
      "                     'the code/functionality to call the llm instantiated by '\n",
      "                     'model deployed on dedicated VM/GPU.\\n'\n",
      "                     '\\n'\n",
      "                     'and just using a flag to switch between the two. Here is '\n",
      "                     'the psuedo code\\n'\n",
      "                     '\\n'\n",
      "                     '```launch_fast_api_server_on_port_8000()\\n'\n",
      "                     '\\n'\n",
      "                     'use_runpod = os.getenv(\"USE_RUNPOD\")\\n'\n",
      "                     '\\n'\n",
      "                     '\\n'\n",
      "                     'api_to_create_topic_cluster(input):\\n'\n",
      "                     '   if use_runpod:\\n'\n",
      "                     '      llm = LLM(\\'mistral on vLLM\")\\n'\n",
      "                     '      do_topic_clustering_on_runpod(input, llm)\\n'\n",
      "                     '   else:\\n'\n",
      "                     '      do_topic_clustering_on_cloudflare(input) ## use '\n",
      "                     'cloudflare rest api```\\n'\n",
      "                     'We can then seamlessly switch between running on runpod '\n",
      "                     '(or even Azure) and cloudflare. Hope this helps.',\n",
      "                     'with the overlap, we can even increase the batch size .. '\n",
      "                     'infact I would love to see now the figures with batch '\n",
      "                     'size of 25, 30, 40, 50, 60 etc..']}]\n"
     ]
    }
   ],
   "source": [
    "# selected_topics_ids = ['ed387b76-1', '948edadf-5', '9f2fb729-3']\n",
    "selected_topics_ids = ['acc7439c-b', '0a76a334-c']\n",
    "\n",
    "stage_matching_ids = {'$match': {'main_thread_id': {'$in': selected_topics_ids}}}\n",
    "\n",
    "stage_grouping = {'$group': {\n",
    "    '_id': '$main_thread_id',\n",
    "    'topic': {'$addToSet': '$topic'},\n",
    "    'topic_messages': {'$push': '$content'}\n",
    "}}\n",
    "\n",
    "stage_sorting = {\n",
    "    \"$sort\": { \"main_thread_id\": DESCENDING }\n",
    "}\n",
    "\n",
    "pipeline = [\n",
    "    stage_matching_ids,\n",
    "    stage_sorting,\n",
    "    stage_grouping,\n",
    "]\n",
    "\n",
    "result = list(db[collection_name].aggregate(pipeline))\n",
    "pprint(list(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea96def9-882a-4a6c-b7b0-8409c78701a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going back and forth on the best approach for handling machine learning models\n",
      "\n",
      "\n",
      "\n",
      " yeah.. making batch size too small will mean we are not casting big enough net to catch all messages in a conversation.. looks like a second llm may be a better option.\n",
      "Are your batches overlapping? they should be i think.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "No, currently there is no overlap (I need to adjust the backend to allow overlap and handle it correctly).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "<@U066Q9JAU3B> Indeed, my recent experiments show this:\n",
      "batch size 12\n",
      "\n",
      "overlap\n",
      "0 -- 17.4%\n",
      "2 -- 14.6%\n",
      "4 -- 11.9%\n",
      "6 -- 4%\n",
      "8 -- 1.5%\n",
      "10 -- 0.2%\n",
      "\n",
      "which I believe supports your statement.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "without an overlapping window, we have no way to determine conversation boundaries and we risk forcing a message inside a conversation it does not belong.\n",
      "The overlap should be more intelligent (instead of hardcoding to, say, 10 messages, we could simply include all messages from last 2 topics with the new batch.. this gives a chance to the LLMs to pick up new topics for the messages misclassified in the previous run).\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I got what you mean.\n",
      "It's nice to have 2 options and being able to switch easily.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Hey, my update for today:\n",
      "I experimented with batch size and this proved out hypothesis that the batch size should be less than 40. But if it too small (e.g. 5), LLM starts to break formatting of the output json, consequently messages need to be clustered by LLM again.\n",
      "It looks like the number between 5-15 is optimal.\n",
      "batch size -- null %\n",
      "40 -- 34.3%\n",
      "20 -- 25.7%\n",
      "10 -- 16.9%\n",
      "5 -- 7%\n",
      "\n",
      "\n",
      "I plan to spend next 4-5 hours on experiments with overlapping and adding reasoning to prompt. If null still appear in the output, I'll create a second call to LLM to classify nulls, having messages from previous 2-3 topics.\n",
      "I'll write another update at the end of my workday today.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "thanks for the update <@U05R0KHPU8N>.\n",
      "It's a possibility that switching from runpod dedicated to serverless may slow down the inference throughput.\n",
      "It will be good to keep our options open - as in, keep the code/functionality to call the llm instantiated by model deployed on dedicated VM/GPU.\n",
      "\n",
      "and just using a flag to switch between the two. Here is the psuedo code\n",
      "\n",
      "```launch_fast_api_server_on_port_8000()\n",
      "\n",
      "use_runpod = os.getenv(\"USE_RUNPOD\")\n",
      "\n",
      "\n",
      "api_to_create_topic_cluster(input):\n",
      "   if use_runpod:\n",
      "      llm = LLM('mistral on vLLM\")\n",
      "      do_topic_clustering_on_runpod(input, llm)\n",
      "   else:\n",
      "      do_topic_clustering_on_cloudflare(input) ## use cloudflare rest api```\n",
      "We can then seamlessly switch between running on runpod (or even Azure) and cloudflare. Hope this helps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "with the overlap, we can even increase the batch size .. infact I would love to see now the figures with batch size of 25, 30, 40, 50, 60 etc..\n"
     ]
    }
   ],
   "source": [
    "print(f'{result[0][\"topic\"][0]}\\n\\n\\n\\n', f'\\n{\"-\" * 100}\\n'.join(result[0]['topic_messages']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3295d6-a308-47e3-987b-1c398c5ff34e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
