{"message":{"0":"555fcbaf27740b8f26db1e14","1":"555fd967c13de0840ffc2993","2":"555fee5a727234850fc771a4","3":"555fee9729921ce910487ac9","4":"555feed6a26bc38d26818897","5":"555ff06bd3bef97f1b59a248","6":"555ff205727234850fc771c1","7":"555ff216c42aede80f337105","8":"555ff23ac42aede80f337106","9":"555ff2a9727234850fc771c9","10":"555ff2c329921ce910487af2","11":"555ffac19439f229424d41b6","12":"555ffad417f8c79a4f8beb3d","13":"555ffad87890a72772d460c0","14":"5560077229921ce910487b8b","15":"556124ad17f8c79a4f8bfb7a","16":"556133cd533895a93087de28","17":"5565f763d21e5ed02ff06287","18":"5565f77bd21e5ed02ff06289","19":"5565f7937a71f1612c266df4","20":"5565f7adc1d06dce2f3967ec","21":"55660e387a5422696cc01e81","22":"55660e3c7a71f1612c267119","23":"55660e4b7a71f1612c26711c","24":"556611f0743a2a6c6c127bf5","25":"556611fe15218a642c9cd80e","26":"55661208d21e5ed02ff06663","27":"5566153115218a642c9cd879","28":"556618f815218a642c9cd901","29":"55661a847a71f1612c2672d6","30":"55661a90d21e5ed02ff0679e","31":"55661cb5743a2a6c6c127d74","32":"55661e44d21e5ed02ff0681f","33":"55661eaa743a2a6c6c127db9","34":"55661f137a71f1612c267372","35":"55661f7715218a642c9cda07","36":"55661fa0743a2a6c6c127dec","37":"55661fc17a5422696cc020d1","38":"55661fcdc1d06dce2f396d96","39":"5566203c743a2a6c6c127e03","40":"5566206f7a5422696cc020e9","41":"55662098c1d06dce2f396da9","42":"556621b4c1d06dce2f396ddd","43":"556621e47a71f1612c2673e5","44":"5566221b15218a642c9cda87","45":"5566223ac1d06dce2f396dec","46":"556623607a5422696cc02166","47":"55662378c1d06dce2f396e19","48":"556624337a5422696cc0217d","49":"5566243fc1d06dce2f396e3a","50":"556624467a71f1612c26744e","51":"5566244dd21e5ed02ff06933","52":"55662453743a2a6c6c127eda","53":"5566247c15218a642c9cdb04","54":"556624dd15218a642c9cdb0e","55":"556624ed7a71f1612c267460","56":"5566b51976f387d042581886","57":"55674b07def7b30f79ba269d","58":"55674b66def7b30f79ba26a3","59":"556754677f82721279ac7aa2","60":"55675d8cbeed96397ea72e4e","61":"556ee3788d1a52c22e872d00","62":"556ee44f8d1a52c22e872d22","63":"556ee53305c872ce6ac7b432","64":"556f07c5777c17d06a13c575","65":"556f4dfa05c872ce6ac7cd9a","66":"556f4e13f40a067d1c9b5540","67":"556f511205c872ce6ac7ce01","68":"556f6dd8777c17d06a13d655","69":"5572d81027d2203776cccdb8","70":"5572d81705c872ce6ac81e33","71":"5572d82605c872ce6ac81e34","72":"5573245af40a067d1c9ba802","73":"55754abb463d0c7c066e6038","74":"5576050a05c872ce6ac85275","75":"55764f69463d0c7c066e7c08","76":"557676ef777c17d06a145e3e","77":"5579d3db05c872ce6ac8b712","78":"557afa1931e09edf0c0ce508","79":"557b08386fee6add0cc21410","80":"557b0c3eb62c8de7435cb3ab","81":"557b1bc2392218e94305d489","82":"557b4081873bcf056fb4e98a","83":"557f12b234b840066f5dfe71","84":"557f12e27d5adaae3b4f2127","85":"557f147c34b840066f5dfe98","86":"557f147e873bcf056fb5215b","87":"557f2207873bcf056fb522ff","88":"557fd075f1cd32e97eca7c2f","89":"558067f11c3ba5ef5bfed388","90":"558068f91c3ba5ef5bfed3a4","91":"558080576f7465873a35a91a","92":"558088141c3ba5ef5bfed700","93":"55808b93f207aa853a8bb175","94":"55819e968fe007833a4ada52","95":"55819eb31c3ba5ef5bfef05a","96":"55819f7a3039387b1577ac53","97":"55819fb7deac73ee5b85802d","98":"5581a8176f7465873a35c561","99":"5581c3256f7465873a35c8bc","100":"5581cc461c3ba5ef5bfef6aa","101":"5581e3081c3ba5ef5bfef998","102":"5581e335deac73ee5b858925","103":"5581e3c06f7465873a35ccf7","104":"5581e437deac73ee5b85893e","105":"5581e43a3039387b1577b52c","106":"5581e43df207aa853a8bd418","107":"5581e8b96f7465873a35cd6c","108":"5581e8d01c3ba5ef5bfefa0c","109":"5581eb776f7465873a35cda5","110":"5582bde33039387b1577c5fd","111":"5582cf4cac9e2e79150db661","112":"5582cf7df207aa853a8be725","113":"5582d0243039387b1577c860","114":"5582e79df207aa853a8beaec","115":"5582e7ae6e4c427a15f468f1","116":"5582e800f207aa853a8beafd","117":"5582e80a6e4c427a15f468f9","118":"5582eb6bf207aa853a8beb9e","119":"5582eb76bb2c3e7c15868261","120":"5582f6121c3ba5ef5bff1220","121":"5582f69e6f7465873a35e5b4","122":"5582fa09f207aa853a8bed33","123":"55834d2f3039387b1577d7d2","124":"55834d7c6f7465873a35efec","125":"55834e75bb2c3e7c15868e2e","126":"55834ea03039387b1577d7ed","127":"5585bb2f6f7465873a361916","128":"55884740bb2c3e7c1586e320","129":"55884e873039387b15782e86","130":"558853556f7465873a3647ba","131":"5588583ff207aa853a8c5024","132":"55887fec6f7465873a364f59","133":"5588a4081c3ba5ef5bff80a7","134":"5588a90a6f7465873a365395","135":"558b3018461e01f542c87c06","136":"558b469f38e37bf74261cf63","137":"558c4f75e6702c3a57648927","138":"558c7262609a063b57876954","139":"558c744187625063017625bc","140":"558c84f3e6702c3a576490c5","141":"5591c41a92e368b167bd26af","142":"5595ad61b634f09d21d98aa8","143":"559acbd052cc8c664f50deff","144":"559ace645331f9985a7fa368","145":"559adb490edc4b6a7986ec6f","146":"559adc301c1634674f8a5068","147":"559b2832e9c8fd6779dc4bdd","148":"559b51c552cc8c664f50ebcf","149":"559e54ec9399a9015e9b5ad3","150":"559ea8160689b34a3bccc138","151":"559eaa0aac5618dd7b629627","152":"559eabeaac5618dd7b629680","153":"559eacea1bbf5a493b658b67","154":"559eb04d1bbf5a493b658bf3","155":"559f1dca36d6aaad30431e79","156":"559f1dcbc67809ab306357bd","157":"559f7567846f9d040c5260dd","158":"559f816ebc5b6ab156b42760","159":"559faebcc67809ab30636420","160":"55a1a26205d3e1f54c9f0df0","161":"55a221a7b8b45ca15bc7b5e6","162":"55a3df878e28b0c71ac96c21","163":"55a68415ea224d3609775036","164":"55a7151f584eabeb554a4fc7","165":"55a718fb584eabeb554a4ff9","166":"55a7198619007a694c57236b","167":"55a71aec0e1787706acd277e","168":"55a786b42fde815212f0fa6f","169":"55a7d7071a5d2fe320741e9b","170":"55a7f7a08f0333fe6bf77182","171":"55a7f833afbc665466c578d7","172":"55a8294ead99869443da9b20","173":"55a87460b83437005acb5b7d","174":"55a87e016551f5f12e71461d","175":"55a88a3a6551f5f12e7146ad","176":"55a8e54ac9a01815286cf38e","177":"55a8e8f0972e11b24adc1aad","178":"55a9594710521a7e525582c0","179":"55a959b610521a7e525582d8","180":"55ad5485ea4b0b3b25a4f0fd","181":"55aec38b3c1189fb1dc11091","182":"55aee43a3c1189fb1dc1139d","183":"55b102e45992e2977c418120","184":"55b11a69f2cdad46058b1e86","185":"55b11ff76e982043058b0e74","186":"55b1369a6e982043058b12f4","187":"55b138466e982043058b1344","188":"55b13a0f16ac52ff650f8269","189":"55b13ac56e982043058b13bc","190":"55b16a4716ac52ff650f89d1","191":"55b6772e7962623b2a0db2ba","192":"55b7b06f4c04f0cc22e70401","193":"55b8e02f4c04f0cc22e72c17","194":"55b94921b49857ca22385d33","195":"55b9591eaaa7fab9633df82d","196":"55ba80b112f77ab279a946d3","197":"55ba88f67978296537a2ba42","198":"55ba978143481e53375fef6a","199":"55ba98c2a0587bc54d68c1e0","200":"55ba99358deffbc44d8dde0b","201":"55ba9942a0587bc54d68c1f5","202":"55ba9bdd7978296537a2bd74","203":"55babe977978296537a2c111","204":"55bc65f28deffbc44d8e0bc0","205":"55be6dea12f77ab279a997ae","206":"55be865a8deffbc44d8e2edd","207":"55bfffaa68c869d67cf0133a","208":"55c032182c1b3bec31988202","209":"55c084ae7a6037e67c5971a6","210":"55c0850237816be77caf87a2","211":"55c3ab3c428d74fe28f76ace","212":"55c3ab5e9ecc6dfc28b58ac3","213":"55c3c74d5dab14832485c903","214":"55c3fa8ccac3038224f5f747","215":"55c40f97cac3038224f5f8de","216":"55c4f6b82ee3da6275c31d3f","217":"55d2857c9b45e15c4264a785","218":"55d2859d255950880cfbf4d2","219":"55d4c35131c67ec1498a0ddd","220":"55d4f005a92a8b4b219afaa5","221":"55d4f01b11c7afc2497862c1","222":"55df38d0e1e902fd09cf078a","223":"55df3d527ba498ed43f2c49c","224":"55df3d5c7ba498ed43f2c4a0","225":"55df3d7fc5601f830c8706ee","226":"55df4ccd3d0b019620af46f1","227":"55df4d53069069633605f837","228":"55df4db23d0b019620af4720","229":"55df515f069069633605f91f","230":"55df5ace069069633605fb7d","231":"55df5ed83d0b019620af4b6f","232":"55df63e77ba498ed43f2ccd8","233":"55df6433c5601f830c870f6b","234":"55df648cc5601f830c870f7d","235":"55df64bfe40e943746621167","236":"55df64c7e1e902fd09cf10ce","237":"55df64db069069633605fdae","238":"55df651b069069633605fdbf","239":"55df6546e1e902fd09cf10fb","240":"55df6593c5601f830c870fc0","241":"55df6617e1e902fd09cf111d","242":"55df664de1e902fd09cf1125","243":"55df665ae40e9437466211b6","244":"55df66857ba498ed43f2cd55","245":"55df66b5e40e9437466211d5","246":"55df66e1c5601f830c870ff7","247":"55df6704e40e9437466211e7","248":"55df67247ba498ed43f2cd7b","249":"55df6739e1e902fd09cf114c","250":"55df674bc5601f830c871012","251":"55df6769e40e9437466211f8","252":"55df6785c5601f830c871021","253":"55df6799c5601f830c871023","254":"55df67c5c5601f830c87102d","255":"55df78bf31f3baee64cab492","256":"55df79ceed5c1a937b95a6d1","257":"55df926c31f3baee64cab8e1","258":"55df946f33e556c746e34782","259":"55df9e3faa53caef647d3426","260":"55df9e401b9798c846ec7d8a","261":"55df9e6bcbf802f022f0b8b7","262":"55df9e921b9798c846ec7d9a","263":"55df9e9433e556c746e3488a","264":"55df9ed731f3baee64caba62","265":"55df9f8531f3baee64caba74","266":"55df9feb31f3baee64caba79","267":"55e0b5ad1b9798c846eca1fe","268":"55e0b65c33e556c746e36c8c","269":"55e0bd666d5732de5b76f08d","270":"55e0c1fd3d8fc2d12eaa2a29","271":"55e72041d231aa8e5918c822","272":"55e73d60a75db4b375c3b1a3","273":"55e743eff6fb4f034f6b5d6a","274":"55e74d40a75db4b375c3b502","275":"55e74d494efdf7f978eb23e5","276":"55e74d4fd231aa8e5918d1ee","277":"55e74d93a75db4b375c3b516","278":"55e772f317b2081605a56d52","279":"55e773272ec6bacd1e2da0e6","280":"55e77368f6fb4f034f6b66c0","281":"55e775012ec6bacd1e2da12e","282":"55e775c917b2081605a56dcf","283":"55e77762f6fb4f034f6b675e","284":"55e779754efdf7f978eb2c67","285":"55e779a54efdf7f978eb2c6e","286":"55e77bfca75db4b375c3bdd5","287":"55e77c02a75db4b375c3bdd7","288":"55e77c5ad231aa8e5918db40","289":"55e77cca2ec6bacd1e2da271","290":"55e77e444efdf7f978eb2d25","291":"55e77e85a75db4b375c3be2e","292":"55e77ef62ec6bacd1e2da2b6","293":"55e77f01d231aa8e5918db90","294":"55e77f4e2ec6bacd1e2da2c2","295":"55e77fa6a75db4b375c3be4b","296":"55e78077a75db4b375c3be63","297":"55e78090a75db4b375c3be64","298":"55e780cbd231aa8e5918dbc8","299":"55e7811ba75db4b375c3be7c","300":"55e781a017b2081605a56f6e","301":"55e78374f6fb4f034f6b68ee","302":"55e783d9d231aa8e5918dc32","303":"55e784b24efdf7f978eb2deb","304":"55e784cca75db4b375c3bedc","305":"55e78520a75db4b375c3bee2","306":"55e785cad231aa8e5918dc68","307":"55e78797f6fb4f034f6b6965","308":"55e787c62ec6bacd1e2da3ab","309":"55e78815d231aa8e5918dc90","310":"55e797e617b2081605a571a9","311":"55e86eb346dfeb9c3dbbeed6","312":"55e89bf2d231aa8e591907c3","313":"55e89bfed231aa8e591907c5","314":"55e8df0cd231aa8e59191285","315":"55e9f092b5c3114f7efe7965","316":"55ea2b3daa0b93be49621dc8","317":"55ef350ab69ff6ab0ec54695","318":"55ef409624362d5253fe4400","319":"55ef425396450ece4d8801bd","320":"55f1bc204b090e3d0be42386","321":"56046bf25c1379fe64597125","322":"56046c05131b784f781fc16f","323":"56046e9f5c1379fe645971a0","324":"56046ec8131b784f781fc1f4","325":"560471c35c1379fe6459723f","326":"560479688d1ef72d5a86d6de","327":"56098e6c329b1da05c3b600b","328":"56098ec8519547fc1e3add83","329":"5609be0a329b1da05c3b6908","330":"5609c1df519547fc1e3ae659","331":"5609c246a8546c0d12c50763","332":"5609c2532241dbf91ee57eac","333":"5609c3752241dbf91ee57ecc","334":"5609c393230869a25cc3e7dc","335":"5609c3e2329b1da05c3b69c9","336":"5609c3eca8546c0d12c50795","337":"5609c43da5b78d0e12a3f358","338":"5609c448230869a25cc3e7f3","339":"5609c450519547fc1e3ae6b0","340":"5609c4752241dbf91ee57ee4","341":"5609c884329b1da05c3b6a3c","342":"5609df53a8546c0d12c50a25","343":"5609e4d2a5b78d0e12a3f622","344":"560a1873519547fc1e3aeeb8","345":"560a3c35a5b78d0e12a3fd81","346":"560acc96552ed7913279d774","347":"560aceed081f3a9c044d75c5","348":"560ad0ab95756f1402bc7f59","349":"560aed3495756f1402bc8602","350":"560aed68081f3a9c044d7cbf","351":"560af37b95756f1402bc878a","352":"560af3cc95756f1402bc8798","353":"560b0d5ef4b61c106fb2b0ba","354":"560b38e7dfb3151302856e05","355":"560c11f3552ed791327a07ee","356":"560c24dedfb3151302858f56","357":"560c31c5f4b61c106fb2d9db","358":"560c33b7dfb3151302859257","359":"560c34a7dfb3151302859288","360":"560dc724bd0a2d24271887a9","361":"560dd884b6a13ea04467baf8","362":"560ddef2ff22c70f6faba914","363":"560de076bd0a2d24271889ca","364":"560de1a7ff22c70f6faba954","365":"560ebcf5f4b61c106fb33810","366":"56120b4776931cb7479f0717","367":"5612556276931cb7479f0faf","368":"5612abdcce6e633c45187410","369":"5612b36afd5c74d71413ee51","370":"5612c83076d984a358755167","371":"5612f1a1fd5c74d71413fafe","372":"5612f42d261e77ba2dbad4ae","373":"5612fd7ece6e633c451883f0","374":"5616f1991b0e279854bd76f9","375":"5616f1b50376066b0f8bb925","376":"5616f4c599bbd76f0f306d28","377":"5617029e0376066b0f8bbad4","378":"561798924e0fa3e55447cb55","379":"561798cd99bbd76f0f307f56","380":"561854f983b69fe7548d4f88","381":"56185a2c4e0fa3e55447eb72","382":"56185a386dc64436714a9c8a","383":"56185c604e0fa3e55447ebcb","384":"56185e074e0fa3e55447ebf4","385":"5618693483b69fe7548d519e","386":"5618693b4e0fa3e55447ed22","387":"5618b0b283b69fe7548d56af","388":"5618fa3183b69fe7548d5b2c","389":"5618fa691b0e279854bdb52b","390":"561918920376066b0f8bf96e","391":"561919650376066b0f8bf985","392":"561952d283b69fe7548d6428","393":"561952da99bbd76f0f30b2cc","394":"5619fa9f83b69fe7548d7226","395":"5619faab1b0e279854bdcc7d","396":"5619fae51b0e279854bdcc85","397":"5619fcfe1b0e279854bdcc9a","398":"561a024f4e0fa3e554480e79","399":"561a0c1f1b0e279854bdcd75"},"time":{"0":"2015-05-23T00:37:03.643Z","1":"2015-05-23T01:35:35.876Z","2":"2015-05-23T03:04:58.626Z","3":"2015-05-23T03:05:59.997Z","4":"2015-05-23T03:07:02.755Z","5":"2015-05-23T03:13:47.338Z","6":"2015-05-23T03:20:37.819Z","7":"2015-05-23T03:20:54.616Z","8":"2015-05-23T03:21:30.852Z","9":"2015-05-23T03:23:21.237Z","10":"2015-05-23T03:23:47.904Z","11":"2015-05-23T03:57:53.884Z","12":"2015-05-23T03:58:12.860Z","13":"2015-05-23T03:58:16.571Z","14":"2015-05-23T04:52:02.669Z","15":"2015-05-24T01:09:01.436Z","16":"2015-05-24T02:13:33.164Z","17":"2015-05-27T16:57:07.901Z","18":"2015-05-27T16:57:31.353Z","19":"2015-05-27T16:57:55.814Z","20":"2015-05-27T16:58:21.674Z","21":"2015-05-27T18:34:32.375Z","22":"2015-05-27T18:34:36.947Z","23":"2015-05-27T18:34:51.573Z","24":"2015-05-27T18:50:24.017Z","25":"2015-05-27T18:50:38.923Z","26":"2015-05-27T18:50:48.298Z","27":"2015-05-27T19:04:17.674Z","28":"2015-05-27T19:20:24.453Z","29":"2015-05-27T19:27:00.572Z","30":"2015-05-27T19:27:12.216Z","31":"2015-05-27T19:36:21.759Z","32":"2015-05-27T19:43:00.171Z","33":"2015-05-27T19:44:42.897Z","34":"2015-05-27T19:46:27.639Z","35":"2015-05-27T19:48:07.538Z","36":"2015-05-27T19:48:48.828Z","37":"2015-05-27T19:49:21.280Z","38":"2015-05-27T19:49:33.240Z","39":"2015-05-27T19:51:24.550Z","40":"2015-05-27T19:52:15.708Z","41":"2015-05-27T19:52:56.170Z","42":"2015-05-27T19:57:40.587Z","43":"2015-05-27T19:58:28.653Z","44":"2015-05-27T19:59:23.492Z","45":"2015-05-27T19:59:54.845Z","46":"2015-05-27T20:04:48.722Z","47":"2015-05-27T20:05:12.415Z","48":"2015-05-27T20:08:19.873Z","49":"2015-05-27T20:08:31.505Z","50":"2015-05-27T20:08:38.688Z","51":"2015-05-27T20:08:45.809Z","52":"2015-05-27T20:08:51.683Z","53":"2015-05-27T20:09:32.378Z","54":"2015-05-27T20:11:09.677Z","55":"2015-05-27T20:11:25.956Z","56":"2015-05-28T06:26:33.528Z","57":"2015-05-28T17:06:15.828Z","58":"2015-05-28T17:07:50.635Z","59":"2015-05-28T17:46:15.029Z","60":"2015-05-28T18:25:16.063Z","61":"2015-06-03T11:22:32.523Z","62":"2015-06-03T11:26:07.433Z","63":"2015-06-03T11:29:55.680Z","64":"2015-06-03T13:57:25.742Z","65":"2015-06-03T18:56:58.294Z","66":"2015-06-03T18:57:23.996Z","67":"2015-06-03T19:10:10.204Z","68":"2015-06-03T21:12:56.374Z","69":"2015-06-06T11:22:56.257Z","70":"2015-06-06T11:23:03.720Z","71":"2015-06-06T11:23:18.778Z","72":"2015-06-06T16:48:26.512Z","73":"2015-06-08T07:56:43.536Z","74":"2015-06-08T21:11:38.595Z","75":"2015-06-09T02:28:57.765Z","76":"2015-06-09T05:17:35.623Z","77":"2015-06-11T18:30:51.946Z","78":"2015-06-12T15:26:17.849Z","79":"2015-06-12T16:26:32.912Z","80":"2015-06-12T16:43:42.398Z","81":"2015-06-12T17:49:54.161Z","82":"2015-06-12T20:26:41.856Z","83":"2015-06-15T18:00:18.316Z","84":"2015-06-15T18:01:06.439Z","85":"2015-06-15T18:07:56.070Z","86":"2015-06-15T18:07:58.099Z","87":"2015-06-15T19:05:43.853Z","88":"2015-06-16T07:29:57.790Z","89":"2015-06-16T18:16:17.595Z","90":"2015-06-16T18:20:41.496Z","91":"2015-06-16T20:00:23.695Z","92":"2015-06-16T20:33:24.903Z","93":"2015-06-16T20:48:19.721Z","94":"2015-06-17T16:21:42.451Z","95":"2015-06-17T16:22:11.935Z","96":"2015-06-17T16:25:30.907Z","97":"2015-06-17T16:26:31.987Z","98":"2015-06-17T17:02:15.488Z","99":"2015-06-17T18:57:41.746Z","100":"2015-06-17T19:36:38.616Z","101":"2015-06-17T21:13:44.592Z","102":"2015-06-17T21:14:29.716Z","103":"2015-06-17T21:16:48.467Z","104":"2015-06-17T21:18:47.619Z","105":"2015-06-17T21:18:50.128Z","106":"2015-06-17T21:18:53.366Z","107":"2015-06-17T21:38:01.492Z","108":"2015-06-17T21:38:24.749Z","109":"2015-06-17T21:49:43.177Z","110":"2015-06-18T12:47:31.839Z","111":"2015-06-18T14:01:48.644Z","112":"2015-06-18T14:02:37.952Z","113":"2015-06-18T14:05:24.136Z","114":"2015-06-18T15:45:33.114Z","115":"2015-06-18T15:45:50.472Z","116":"2015-06-18T15:47:12.426Z","117":"2015-06-18T15:47:22.307Z","118":"2015-06-18T16:01:47.811Z","119":"2015-06-18T16:01:58.527Z","120":"2015-06-18T16:47:14.306Z","121":"2015-06-18T16:49:34.991Z","122":"2015-06-18T17:04:09.690Z","123":"2015-06-18T22:58:55.578Z","124":"2015-06-18T23:00:12.120Z","125":"2015-06-18T23:04:21.360Z","126":"2015-06-18T23:05:04.814Z","127":"2015-06-20T19:12:47.771Z","128":"2015-06-22T17:34:56.060Z","129":"2015-06-22T18:05:59.413Z","130":"2015-06-22T18:26:29.825Z","131":"2015-06-22T18:47:27.011Z","132":"2015-06-22T21:36:44.827Z","133":"2015-06-23T00:10:48.348Z","134":"2015-06-23T00:32:10.706Z","135":"2015-06-24T22:32:56.310Z","136":"2015-06-25T00:09:03.880Z","137":"2015-06-25T18:59:01.635Z","138":"2015-06-25T21:28:02.142Z","139":"2015-06-25T21:36:01.974Z","140":"2015-06-25T22:47:15.701Z","141":"2015-06-29T22:18:02.976Z","142":"2015-07-02T21:30:09.092Z","143":"2015-07-06T18:41:20.078Z","144":"2015-07-06T18:52:20.288Z","145":"2015-07-06T19:47:21.348Z","146":"2015-07-06T19:51:12.996Z","147":"2015-07-07T01:15:30.237Z","148":"2015-07-07T04:12:53.159Z","149":"2015-07-09T11:03:08.735Z","150":"2015-07-09T16:57:58.254Z","151":"2015-07-09T17:06:18.567Z","152":"2015-07-09T17:14:18.941Z","153":"2015-07-09T17:18:34.240Z","154":"2015-07-09T17:33:01.641Z","155":"2015-07-10T01:20:10.572Z","156":"2015-07-10T01:20:11.966Z","157":"2015-07-10T07:33:59.085Z","158":"2015-07-10T08:25:18.588Z","159":"2015-07-10T11:38:36.351Z","160":"2015-07-11T23:10:26.710Z","161":"2015-07-12T08:13:27.727Z","162":"2015-07-13T15:55:51.201Z","163":"2015-07-15T16:02:29.774Z","164":"2015-07-16T02:21:19.241Z","165":"2015-07-16T02:37:47.132Z","166":"2015-07-16T02:40:06.152Z","167":"2015-07-16T02:46:04.679Z","168":"2015-07-16T10:25:56.720Z","169":"2015-07-16T16:08:39.711Z","170":"2015-07-16T18:27:44.058Z","171":"2015-07-16T18:30:11.300Z","172":"2015-07-16T21:59:42.639Z","173":"2015-07-17T03:20:00.984Z","174":"2015-07-17T04:01:05.543Z","175":"2015-07-17T04:53:14.350Z","176":"2015-07-17T11:21:46.378Z","177":"2015-07-17T11:37:20.302Z","178":"2015-07-17T19:36:39.468Z","179":"2015-07-17T19:38:30.131Z","180":"2015-07-20T20:05:25.361Z","181":"2015-07-21T22:11:23.265Z","182":"2015-07-22T00:30:50.737Z","183":"2015-07-23T15:06:12.388Z","184":"2015-07-23T16:46:33.484Z","185":"2015-07-23T17:10:15.933Z","186":"2015-07-23T18:46:50.697Z","187":"2015-07-23T18:53:58.092Z","188":"2015-07-23T19:01:35.201Z","189":"2015-07-23T19:04:37.113Z","190":"2015-07-23T22:27:19.839Z","191":"2015-07-27T18:23:42.087Z","192":"2015-07-28T16:40:15.642Z","193":"2015-07-29T14:16:15.554Z","194":"2015-07-29T21:44:01.383Z","195":"2015-07-29T22:52:14.834Z","196":"2015-07-30T19:53:21.875Z","197":"2015-07-30T20:28:38.710Z","198":"2015-07-30T21:30:41.387Z","199":"2015-07-30T21:36:02.952Z","200":"2015-07-30T21:37:57.002Z","201":"2015-07-30T21:38:10.081Z","202":"2015-07-30T21:49:17.525Z","203":"2015-07-31T00:17:27.306Z","204":"2015-08-01T06:23:46.159Z","205":"2015-08-02T19:22:18.405Z","206":"2015-08-02T21:06:34.281Z","207":"2015-08-03T23:56:26.832Z","208":"2015-08-04T03:31:36.033Z","209":"2015-08-04T09:23:58.052Z","210":"2015-08-04T09:25:22.125Z","211":"2015-08-06T18:45:16.371Z","212":"2015-08-06T18:45:50.359Z","213":"2015-08-06T20:45:01.490Z","214":"2015-08-07T00:23:40.356Z","215":"2015-08-07T01:53:27.172Z","216":"2015-08-07T18:19:36.627Z","217":"2015-08-18T01:08:12.918Z","218":"2015-08-18T01:08:45.347Z","219":"2015-08-19T17:56:33.203Z","220":"2015-08-19T21:07:17.809Z","221":"2015-08-19T21:07:39.495Z","222":"2015-08-27T16:20:32.873Z","223":"2015-08-27T16:39:46.179Z","224":"2015-08-27T16:39:56.856Z","225":"2015-08-27T16:40:31.482Z","226":"2015-08-27T17:45:49.010Z","227":"2015-08-27T17:48:03.928Z","228":"2015-08-27T17:49:38.014Z","229":"2015-08-27T18:05:19.161Z","230":"2015-08-27T18:45:34.925Z","231":"2015-08-27T19:02:48.392Z","232":"2015-08-27T19:24:23.813Z","233":"2015-08-27T19:25:39.964Z","234":"2015-08-27T19:27:08.009Z","235":"2015-08-27T19:27:59.695Z","236":"2015-08-27T19:28:07.069Z","237":"2015-08-27T19:28:27.066Z","238":"2015-08-27T19:29:31.831Z","239":"2015-08-27T19:30:14.450Z","240":"2015-08-27T19:31:31.953Z","241":"2015-08-27T19:33:43.343Z","242":"2015-08-27T19:34:37.990Z","243":"2015-08-27T19:34:50.677Z","244":"2015-08-27T19:35:33.122Z","245":"2015-08-27T19:36:21.853Z","246":"2015-08-27T19:37:05.187Z","247":"2015-08-27T19:37:40.590Z","248":"2015-08-27T19:38:12.033Z","249":"2015-08-27T19:38:33.049Z","250":"2015-08-27T19:38:51.903Z","251":"2015-08-27T19:39:21.484Z","252":"2015-08-27T19:39:49.343Z","253":"2015-08-27T19:40:09.261Z","254":"2015-08-27T19:40:53.830Z","255":"2015-08-27T20:53:19.035Z","256":"2015-08-27T20:57:50.943Z","257":"2015-08-27T22:42:52.946Z","258":"2015-08-27T22:51:27.323Z","259":"2015-08-27T23:33:19.122Z","260":"2015-08-27T23:33:20.079Z","261":"2015-08-27T23:34:03.729Z","262":"2015-08-27T23:34:42.007Z","263":"2015-08-27T23:34:44.052Z","264":"2015-08-27T23:35:51.806Z","265":"2015-08-27T23:38:45.274Z","266":"2015-08-27T23:40:27.048Z","267":"2015-08-28T19:25:33.782Z","268":"2015-08-28T19:28:28.217Z","269":"2015-08-28T19:58:30.299Z","270":"2015-08-28T20:18:05.994Z","271":"2015-09-02T16:13:53.817Z","272":"2015-09-02T18:18:08.862Z","273":"2015-09-02T18:46:07.686Z","274":"2015-09-02T19:25:52.729Z","275":"2015-09-02T19:26:01.467Z","276":"2015-09-02T19:26:07.165Z","277":"2015-09-02T19:27:15.805Z","278":"2015-09-02T22:06:43.975Z","279":"2015-09-02T22:07:35.601Z","280":"2015-09-02T22:08:40.384Z","281":"2015-09-02T22:15:29.029Z","282":"2015-09-02T22:18:49.204Z","283":"2015-09-02T22:25:38.554Z","284":"2015-09-02T22:34:29.925Z","285":"2015-09-02T22:35:17.849Z","286":"2015-09-02T22:45:16.069Z","287":"2015-09-02T22:45:22.048Z","288":"2015-09-02T22:46:50.403Z","289":"2015-09-02T22:48:42.715Z","290":"2015-09-02T22:55:00.850Z","291":"2015-09-02T22:56:05.182Z","292":"2015-09-02T22:57:58.224Z","293":"2015-09-02T22:58:09.870Z","294":"2015-09-02T22:59:26.292Z","295":"2015-09-02T23:00:54.736Z","296":"2015-09-02T23:04:23.392Z","297":"2015-09-02T23:04:48.160Z","298":"2015-09-02T23:05:47.021Z","299":"2015-09-02T23:07:07.691Z","300":"2015-09-02T23:09:20.855Z","301":"2015-09-02T23:17:08.699Z","302":"2015-09-02T23:18:49.001Z","303":"2015-09-02T23:22:26.152Z","304":"2015-09-02T23:22:52.086Z","305":"2015-09-02T23:24:16.126Z","306":"2015-09-02T23:27:06.435Z","307":"2015-09-02T23:34:47.219Z","308":"2015-09-02T23:35:34.472Z","309":"2015-09-02T23:36:53.241Z","310":"2015-09-03T00:44:22.628Z","311":"2015-09-03T16:00:51.201Z","312":"2015-09-03T19:13:54.290Z","313":"2015-09-03T19:14:06.453Z","314":"2015-09-04T00:00:12.643Z","315":"2015-09-04T19:27:14.769Z","316":"2015-09-04T23:37:33.365Z","317":"2015-09-08T19:20:42.314Z","318":"2015-09-08T20:09:58.089Z","319":"2015-09-08T20:17:23.474Z","320":"2015-09-10T17:21:36.830Z","321":"2015-09-24T21:32:34.098Z","322":"2015-09-24T21:32:53.831Z","323":"2015-09-24T21:43:59.214Z","324":"2015-09-24T21:44:40.192Z","325":"2015-09-24T21:57:23.705Z","326":"2015-09-24T22:30:00.895Z","327":"2015-09-28T19:01:00.953Z","328":"2015-09-28T19:02:32.136Z","329":"2015-09-28T22:24:10.843Z","330":"2015-09-28T22:40:31.363Z","331":"2015-09-28T22:42:14.215Z","332":"2015-09-28T22:42:27.965Z","333":"2015-09-28T22:47:17.067Z","334":"2015-09-28T22:47:47.401Z","335":"2015-09-28T22:49:06.647Z","336":"2015-09-28T22:49:16.514Z","337":"2015-09-28T22:50:37.472Z","338":"2015-09-28T22:50:48.615Z","339":"2015-09-28T22:50:56.022Z","340":"2015-09-28T22:51:33.153Z","341":"2015-09-28T23:08:52.173Z","342":"2015-09-29T00:46:11.469Z","343":"2015-09-29T01:09:38.194Z","344":"2015-09-29T04:49:55.988Z","345":"2015-09-29T07:22:29.091Z","346":"2015-09-29T17:38:30.627Z","347":"2015-09-29T17:48:29.603Z","348":"2015-09-29T17:55:55.062Z","349":"2015-09-29T19:57:40.084Z","350":"2015-09-29T19:58:32.359Z","351":"2015-09-29T20:24:27.770Z","352":"2015-09-29T20:25:48.851Z","353":"2015-09-29T22:14:54.702Z","354":"2015-09-30T01:20:39.899Z","355":"2015-09-30T16:46:43.643Z","356":"2015-09-30T18:07:26.306Z","357":"2015-09-30T19:02:29.534Z","358":"2015-09-30T19:10:47.476Z","359":"2015-09-30T19:14:47.357Z","360":"2015-10-01T23:52:04.688Z","361":"2015-10-02T01:06:12.737Z","362":"2015-10-02T01:33:38.122Z","363":"2015-10-02T01:40:06.185Z","364":"2015-10-02T01:45:11.048Z","365":"2015-10-02T17:20:53.898Z","366":"2015-10-05T05:31:51.853Z","367":"2015-10-05T10:48:02.325Z","368":"2015-10-05T16:57:00.173Z","369":"2015-10-05T17:29:14.016Z","370":"2015-10-05T18:57:52.463Z","371":"2015-10-05T21:54:41.339Z","372":"2015-10-05T22:05:33.427Z","373":"2015-10-05T22:45:18.793Z","374":"2015-10-08T22:43:37.658Z","375":"2015-10-08T22:44:05.508Z","376":"2015-10-08T22:57:09.701Z","377":"2015-10-08T23:56:14.280Z","378":"2015-10-09T10:36:02.494Z","379":"2015-10-09T10:37:01.696Z","380":"2015-10-09T23:59:53.143Z","381":"2015-10-10T00:22:04.323Z","382":"2015-10-10T00:22:16.948Z","383":"2015-10-10T00:31:28.497Z","384":"2015-10-10T00:38:31.364Z","385":"2015-10-10T01:26:12.478Z","386":"2015-10-10T01:26:19.102Z","387":"2015-10-10T06:31:14.518Z","388":"2015-10-10T11:44:49.663Z","389":"2015-10-10T11:45:45.829Z","390":"2015-10-10T13:54:26.658Z","391":"2015-10-10T13:57:57.830Z","392":"2015-10-10T18:02:58.246Z","393":"2015-10-10T18:03:06.007Z","394":"2015-10-11T05:58:55.575Z","395":"2015-10-11T05:59:07.884Z","396":"2015-10-11T06:00:05.878Z","397":"2015-10-11T06:09:02.353Z","398":"2015-10-11T06:31:43.918Z","399":"2015-10-11T07:13:35.766Z"},"user":{"0":"vchollati","1":"lo5","2":"vchollati","3":"vchollati","4":"vchollati","5":"mmalohlava","6":"vchollati","7":"vchollati","8":"mmalohlava","9":"vchollati","10":"mmalohlava","11":"vchollati","12":"vchollati","13":"vchollati","14":"mmalohlava","15":"chrinide","16":"mmalohlava","17":"andrewcstewart","18":"andrewcstewart","19":"andrewcstewart","20":"andrewcstewart","21":"amywang718","22":"amywang718","23":"amywang718","24":"andrewcstewart","25":"andrewcstewart","26":"andrewcstewart","27":"amywang718","28":"andrewcstewart","29":"andrewcstewart","30":"andrewcstewart","31":"lo5","32":"lo5","33":"andrewcstewart","34":"andrewcstewart","35":"lo5","36":"andrewcstewart","37":"andrewcstewart","38":"andrewcstewart","39":"lo5","40":"lo5","41":"lo5","42":"andrewcstewart","43":"andrewcstewart","44":"andrewcstewart","45":"andrewcstewart","46":"lo5","47":"lo5","48":"andrewcstewart","49":"lo5","50":"andrewcstewart","51":"lo5","52":"andrewcstewart","53":"lo5","54":"andrewcstewart","55":"andrewcstewart","56":"mihaisecasiu","57":"amywang718","58":"lo5","59":"andrewcstewart","60":"mmalohlava","61":"chrinide","62":"chrinide","63":"chrinide","64":"petro-rudenko","65":"mmalohlava","66":"mmalohlava","67":"petro-rudenko","68":"bghill","69":"zgmming","70":"zgmming","71":"zgmming","72":"mmalohlava","73":"chrinide","74":"bghill","75":"vchollati","76":"chrinide","77":"binga","78":"hvanhovell","79":"mmalohlava","80":"mmalohlava","81":"hvanhovell","82":"mmalohlava","83":"geponce","84":"lo5","85":"mmalohlava","86":"maxschloemer0xdata","87":"rpeck","88":"pcchong","89":"rpeck","90":"geponce","91":"bghill","92":"geponce","93":"bghill","94":"geponce","95":"geponce","96":"geponce","97":"geponce","98":"geponce","99":"bghill","100":"geponce","101":"rpeck","102":"rpeck","103":"geponce","104":"rpeck","105":"rpeck","106":"rpeck","107":"geponce","108":"geponce","109":"geponce","110":"mihaisecasiu","111":"geponce","112":"geponce","113":"geponce","114":"geponce","115":"geponce","116":"geponce","117":"geponce","118":"andrewcstewart","119":"andrewcstewart","120":"mihaisecasiu","121":"mihaisecasiu","122":"andrewcstewart","123":"rpeck","124":"rpeck","125":"rpeck","126":"rpeck","127":"geponce","128":"bghill","129":"geponce","130":"bghill","131":"bghill","132":"bghill","133":"geponce","134":"bghill","135":"andrewcstewart","136":"rpeck","137":"geponce","138":"bghill","139":"bghill","140":"geponce","141":"andrewcstewart","142":"bghill","143":"geponce","144":"geponce","145":"bghill","146":"bghill","147":"geponce","148":"Sam7","149":"petro-rudenko","150":"bghill","151":"bghill","152":"bghill","153":"petro-rudenko","154":"bghill","155":"Sam7","156":"Sam7","157":"mmalohlava","158":"petro-rudenko","159":"mmalohlava","160":"aglagla","161":"aglagla","162":"petro-rudenko","163":"petro-rudenko","164":"bghill","165":"bghill","166":"bghill","167":"bghill","168":"petro-rudenko","169":"geponce","170":"bghill","171":"bghill","172":"kryton","173":"bghill","174":"bghill","175":"bghill","176":"petro-rudenko","177":"petro-rudenko","178":"bghill","179":"bghill","180":"geponce","181":"amywang718","182":"geponce","183":"petro-rudenko","184":"petro-rudenko","185":"geponce","186":"bghill","187":"bghill","188":"bghill","189":"bghill","190":"geponce","191":"bghill","192":"petro-rudenko","193":"petro-rudenko","194":"mmalohlava","195":"mmalohlava","196":"davemssavage","197":"davemssavage","198":"rpeck","199":"rpeck","200":"rpeck","201":"rpeck","202":"davemssavage","203":"rpeck","204":"Sam7","205":"rpeck","206":"Sam7","207":"rpeck","208":"Sam7","209":"jvullo","210":"jvullo","211":"rpeck","212":"rpeck","213":"mmalohlava","214":"Sam7","215":"mmalohlava","216":"mmalohlava","217":"christophergutierrez","218":"christophergutierrez","219":"mmalohlava","220":"christophergutierrez","221":"christophergutierrez","222":"geponce","223":"geponce","224":"geponce","225":"geponce","226":"bghill","227":"geponce","228":"bghill","229":"geponce","230":"davidljung","231":"bghill","232":"davidljung","233":"davidljung","234":"bghill","235":"bghill","236":"davidljung","237":"davidljung","238":"davidljung","239":"davidljung","240":"davidljung","241":"bghill","242":"bghill","243":"davidljung","244":"davidljung","245":"bghill","246":"davidljung","247":"davidljung","248":"bghill","249":"bghill","250":"davidljung","251":"davidljung","252":"bghill","253":"bghill","254":"bghill","255":"davidljung","256":"bghill","257":"geponce","258":"bghill","259":"geponce","260":"geponce","261":"geponce","262":"geponce","263":"geponce","264":"geponce","265":"bghill","266":"bghill","267":"geponce","268":"bghill","269":"geponce","270":"bghill","271":"geponce","272":"r3tex","273":"rpeck","274":"bghill","275":"bghill","276":"bghill","277":"bghill","278":"geponce","279":"geponce","280":"geponce","281":"geponce","282":"bghill","283":"geponce","284":"bghill","285":"geponce","286":"geponce","287":"geponce","288":"geponce","289":"geponce","290":"geponce","291":"bghill","292":"geponce","293":"geponce","294":"bghill","295":"bghill","296":"geponce","297":"geponce","298":"geponce","299":"bghill","300":"geponce","301":"bghill","302":"bghill","303":"geponce","304":"bghill","305":"geponce","306":"bghill","307":"geponce","308":"bghill","309":"bghill","310":"bghill","311":"geponce","312":"geponce","313":"geponce","314":"bghill","315":"mongoose54","316":"ledell","317":"gdequeiroz","318":"bghill","319":"gdequeiroz","320":"lo5","321":"gdequeiroz","322":"gdequeiroz","323":"gdequeiroz","324":"bghill","325":"gdequeiroz","326":"bghill","327":"gdequeiroz","328":"gdequeiroz","329":"bghill","330":"jagatsingh","331":"jagatsingh","332":"jagatsingh","333":"bghill","334":"jagatsingh","335":"bghill","336":"bghill","337":"bghill","338":"jagatsingh","339":"jagatsingh","340":"bghill","341":"bghill","342":"haty1","343":"bghill","344":"haty1","345":"jagatsingh","346":"mmalohlava","347":"mmalohlava","348":"mmalohlava","349":"geponce","350":"geponce","351":"bghill","352":"bghill","353":"mmalohlava","354":"geponce","355":"geponce","356":"mmalohlava","357":"geponce","358":"geponce","359":"geponce","360":"jagatsingh","361":"mmalohlava","362":"toddniven","363":"mmalohlava","364":"jagatsingh","365":"mmalohlava","366":"jagatsingh","367":"jagatsingh","368":"mmalohlava","369":"mmalohlava","370":"mmalohlava","371":"jagatsingh","372":"mmalohlava","373":"haty1","374":"gdequeiroz","375":"gdequeiroz","376":"gdequeiroz","377":"mmalohlava","378":"toddniven","379":"toddniven","380":"jagatsingh","381":"mmalohlava","382":"mmalohlava","383":"jagatsingh","384":"toddniven","385":"mmalohlava","386":"mmalohlava","387":"bawongfai","388":"bawongfai","389":"jagatsingh","390":"bawongfai","391":"bawongfai","392":"jagatsingh","393":"jagatsingh","394":"ledell","395":"ledell","396":"ledell","397":"jagatsingh","398":"bawongfai","399":"ledell"},"message.1":{"0":"Anyone using h2o from Python?","1":"Sure. Have you tried the python bindings that ship with h2o? ","2":"I actually installed h2o with pip, using the link mentioned on h2o website. I'm having issues starting h2o from inside python. Works okay if I start the cluster from commandline and then connect from python but h2o.init() fails to start the cluster. Tried both h2o.init() and h2o.init(start_h2o=True).","3":"No instance found at ip and port: localhost:54321. Trying to start local jar...\\n\\n\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-2-e7cfdc50af66> in <module>()\\n----> 1 h2o.init()\\n\\nC:\\\\Anaconda\\\\lib\\\\site-packages\\\\h2o\\\\h2o.pyc in init(ip, port, size, start_h2o, enable_assertions, license, max_mem_size_GB, min_mem_size_GB, ice_root, strict_version_check)\\n    449   :return: None\\n    450   \\\\\\\\n--> 451   H2OConnection(ip=ip, port=port,start_h2o=start_h2o,enable_assertions=enable_assertions,license=license,max_mem_size_GB=max_mem_size_GB,min_mem_size_GB=min_mem_size_GB,ice_root=ice_root,strict_version_check=strict_version_check)\\n    452   return None\\n    453 \\n\\nC:\\\\Anaconda\\\\lib\\\\site-packages\\\\h2o\\\\connection.pyc in __init__(self, ip, port, size, start_h2o, enable_assertions, license, max_mem_size_GB, min_mem_size_GB, ice_root, strict_version_check)\\n     76           if not ice_root:\\n     77             ice_root = tempfile.mkdtemp()\\n---> 78           cld = self._start_local_h2o_jar(max_mem_size_GB, min_mem_size_GB, enable_assertions, license, ice_root)\\n     79         else:\\n     80           print \\No jar file found. Could not start local instance.\\\\n\\nC:\\\\Anaconda\\\\lib\\\\site-packages\\\\h2o\\\\connection.pyc in _start_local_h2o_jar(self, mmax, mmin, ea, license, ice)\\n    164       raise ValueError(\\`ice_root` must be specified\\)\\n    165 \\n--> 166     stdout = open(H2OConnection._tmp_file(\\stdout\\), 'w')\\n    167     stderr = open(H2OConnection._tmp_file(\\stderr\\), 'w')\\n    168 \\n\\nC:\\\\Anaconda\\\\lib\\\\site-packages\\\\h2o\\\\connection.pyc in _tmp_file(type)\\n    272   def _tmp_file(type):\\n    273     if sys.platform == \\win32\\:\\n--> 274       usr = re.sub(\\[^A-Za-z0-9]\\ \\_\\ os.getenv(\\USERNMAME\\))\\n    275     else:\\n    276       usr = re.sub(\\[^A-Za-z0-9]\\ \\_\\ os.getenv(\\USER\\))\\n\\nC:\\\\Anaconda\\\\lib\\\\re.pyc in sub(pattern, repl, string, count, flags)\\n    149     a callable, it's passed the match object and must return\\n    150     a replacement string to be used.\\\\\\\\n--> 151     return _compile(pattern, flags).sub(repl, string, count)\\n    152 \\n    153 def subn(pattern, repl, string, count=0, flags=0):\\n\\nTypeError: expected string or buffer","4":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/3Yqv\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/3Yqv\/blob)","5":"Hi, can you please try to type \\n```\\nimport os\\nos.getenv(\\USERNAME\\). \\n```\\n\\nIt is bug in our code, i just would like to verify the behavior on your site.","6":"Returns 'User'","7":"I fixed it by changing the return statement","8":"Oki, thank you, it is stupid typo on our side, sorry for that. I push fix to the master, but release will be ready later during weekend","9":"not a problem, fixed it at my end as well :)","10":"perfect! thank you for trying h2o! :-)","11":"fixed another small bug, sent PR.","12":"Thanks for making it opensource ","13":":D","14":"cool! thanks a lot for PR","15":"very perfect project, I really love it","16":"Thank you! ","17":"Hi there.  Im wondering if there's a mismatch between some example code im following and the version of h2o Im using with R...","18":"Im trying to do\\n```\\ntrain <- as.h2o(localH2O, object = df)\\n```","19":"h2o appears to be assigning its own col names to the resulting object","20":"whereas the example code im following along with seems to be subsetting the object with the assumption that the column names match the original dataframe","21":"Hey Andrew, as.h2o would save the df object temporarily and then upload it into H2O. Sounds like when during the upload we detected no headers.","22":"Which example were you following?","23":"Or what's the df object?","24":"@amywang718 Adding the headers=TRUE option did the trick.","25":"Just strange because it didn't seem necessary in a prior example I had seen.","26":"Thanks.","27":"Hey Andrew, if it's not specified H2O's parser will try to detect whether the first column is part of the data or not. Looks like this version and this dataset it detected the first column is part of the data.","28":"Yeah I got the hang of it now :)  Awesome software!","29":"Hm.. so I realize this question is surely covered in the documentation, but I'm kinda curious for a more interactive answer: what's the experience like for implementing new algorithms into h2o?  ","30":"Is h2o limited to java based implementations?","31":"Sure. Take a look at Cliff\\u2019s blog posts here: http:\/\/h2o.ai\/blog\/2014\/16\/Hacking\/Algos\/","32":"If you want to dig deeper into some of the performance characteristics to be aware of while implementing new algorithms, look at http:\/\/h2o.ai\/blog\/2014\/02\/kv-store-memory-analytics\/ and http:\/\/h2o.ai\/blog\/2014\/05\/kv-store-memory-analytics-part-2-2\/","33":"Thanks","34":"Pretty good overview, but I'm wondering if its imaginable to implement an algo in another language... perhaps by wrapping with the necessary Java dressings making a call to some kind of process container?","35":"Well, ultimately it has to run in the JVM. So are you talking about another JVM language, or something else?","36":"something else","37":"but, even sticking within the JVM could open possibilities it seems..","38":"so perhaps something like Clojure or Scala?","39":"Sure, that should be possible. There\\u2019s Sparkling Water, which is built in Scala, but that\\u2019s more of a H2O-Spark integration project. ","40":"Which language would you want to write the algos in, if I may ask?","41":"Or are you looking at some kind of DSL, or surface-syntax-to-Java transformation?","42":"More curious about what options exist than anything","43":"Python is my usual go to","44":"But even jumping into something like Groovy would be nice","45":"as for DSLs.. what about a DSL that lives in the JVM?","46":"Currently all of the algos are implemented in Java. This is because the implementors know what the Java translates to during runtime, and consequently have better guarantees about how the algo will perform. A DSL or high level surface-syntax would be elegant, but we haven\\u2019t built any yet. @mmalohlava has more ideas on this. ","47":"He\\u2019s the DSL expert :-)","48":"I don't know a ton about h2o yet but it seems that one of the main ingredients here is use of this kv-store ?","49":"Yes","50":"and I assume that would then bound any algo implementations?","51":"Yes","52":"(ie, any implementations must use that kv-store)","53":"Correct. It\\u2019s a distributed kv-store. The algos work on that. ","54":"Hm.. I wonder if you could potentially use something like docker to encapsulate the algorithm implementation","55":"allowing the implementation in any language so long as it interfaces with the kv-store","56":"Hello everyone, I'm using the python api. Is there a way to save the models using the api? I can see that I can download a model's POJO file but how can I recreate the model from the POJO file in python without retraining it? Thank you!","57":"@mihaisecasiu At the moment we haven't yet implemented save\/restore model in H2O-3.0 yet, it's a pending jira: https:\/\/0xdata.atlassian.net\/browse\/PUBDEV-1164?jql=text%20~%20%22save%20model%22 . Once it has been implemented we'll put in the R and Python bindings. ","58":"@mihaisecasiu Save and restore is not implemented yet. There\\u2019s a ticket for that: https:\/\/0xdata.atlassian.net\/browse\/PUBDEV-787\\n@andrewcstewart I would suppose the api calls across that boundary would be prohibitively expensive for this to be a practical strategy.","59":"@lo5 Yeah, I suspect you're right.","60":"@andrewcstewart regarding DSL - right now at JVM level we expose Java API, and simple Scala API from data frames.  For example of using Java API i would point you to github tests (for example, https:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-algos\/src\/test\/java\/hex\/tree\/gbm\/GBMTest.java), for Scala API you can visit Sparkling Water project using H2O API (look here: https:\/\/github.com\/h2oai\/sparkling-water\/blob\/master\/examples\/scripts\/chicagoCrimeSmallShell.script.scala)","61":"Hi H2Oer,","62":"I have found that in H2O Classic, when one column is mixed int or real numbers with string, than H2O can not identify the string, only get blank values. But H2O 3.0 can identify mixed type columns.","63":"if one set the column to ENUM, then H2) 3 can identify the mixed type","64":"Hi everyone. I have a question regarding sparkling water. Any reason why it collects column domain on a spark side, not at h2o - where it should be faster:\\nhttp:\/\/i.imgur.com\/8ngv2Sl.png","65":"Hey Petro, right now if Spark DataFrame\/RDD contains a string field, it does not contain information about arity of column, so i collect this information in advance in Spark and then decide if i should create H2O String-column or Enum-column.\\n\\nHowever, you are right - perhaps better strategy would be to transfer column directly to H2O and do post-processing (switching column type to enum) in H2O. \\n\\n","66":"Let me create H2O ticket for this and solve it later. \\n\\nIs it blocking you?","67":"Not so much, but it neglects performance winning for small models compared to spark's mllib implementation. ","68":"@chrinide Yes, in H2O Classic, the parser reads all the data and tries to guess the column type.  In H2O-3, the parser reads a subset and makes a type guess for each column.  In Flow, you will see these guesses and can change the column types as you like and the parser should obey the new column types.  Similar effects can be done in R and Python.","69":"hi guys, it seems the H2O 3.0 not work well with R","70":"prostate.dl = h2o.deeplearning(x = 3:9, training_frame = prostate.hex, autoencoder = TRUE,\\n                               hidden = c(10, 10), epochs = 5)\\n","71":"ERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http:\/\/localhost:54321\/3\/Rapids)\\n\\nwater.exceptions.H2OIllegalArgumentException\\n [1] \\water.api.RapidsHandler.exec(RapidsHandler.java:132)\\           \\n [2] \\sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\    \\n [3] \\sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\\    \\n [4] \\sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\\\\n [5] \\java.lang.reflect.Method.invoke(Unknown Source)\\                \\n [6] \\water.api.Handler.handle(Handler.java:56)\\                      \\n [7] \\water.api.RequestServer.handle(RequestServer.java:673)\\         \\n [8] \\water.api.RequestServer.serve(RequestServer.java:610)\\          \\n [9] \\water.NanoHTTPD$HTTPSession.run(NanoHTTPD.java:438)\\            \\n[10] \\java.lang.Thread.run(Unknown Source)\\                           \\n\\nError in .h2o.doSafeREST(conn = conn, urlSuffix = page, parms = .params,  : \\n  factor requires a single column\\n","72":"Hello @zgmming, can you specify your H2O version? Was it 3.0.0.12?","73":"Hi H2Oer, it is now GLRM model Available under Python API & WebUI? I saw that it is available under R API? ","74":"@chrinide I believe GLRM is still under testing.  As such we haven't ported it to the other interfaces yet.","75":"Is it possible to run sparkling water on windows (on pyspark)?","76":"@bghill Thanks for your information.","77":"Hello, I am interested to use h2o algorithms (like RF\/GBM) to perform a classification task on a dataset loaded in Spark. Scala is on my \\to-learn\\ list but at this point of time, I would like to use R. Is it possible to write R code that calls h2o algorithms on data in spark?","78":"Hello, is it possible to filter a H2OFrame  using the scala\/sparkling water? Or do I have to do this in Spark?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   ","79":"@binga yes, you can connect from R to running H2O\/Sparkling water cluster and run algos, analyze data, or do feature munging. See docs.h2o.ai \\n\\nAlso you can look at some example in Sparkling Water:\\n\\n  - Prepare data\/models in Spark\/Sparkling Water and use them from R https:\/\/github.com\/h2oai\/sparkling-water\/blob\/master\/examples\/meetups\/Meetup20150326.md\\n  - Analysis on airline data, making a regression model in Sparkling Water, and producing residuals plot in R. See code https:\/\/github.com\/h2oai\/sparkling-water\/blob\/master\/examples\/meetups\/Meetup20150203.md\\n","80":"@hvanhovell yes:\\n  - column filtering is easy, just remove columns which you do not need, or create a new H2OFrame from selected columns (probably there is no H2OFrame accessor right now, but you can create a new Frame - Frame(String[] names, Vec[] vec) and then make H2OFrame wrapper around it (new H2OFrame(frame))\\n\\n  - row filtering is little bit harder. There are two ways: \\n    - create an additional binary vector holding 1\/0 for in\/out the sample, and during your computation you have to take into account the created vector. This solution is quite cheap, since you do not duplicate data, just create one simple vector in one data walk.\\n    -  create a new frame with filtered rows - this is harder tasks, since you have to copy data - look at #deepSlice call on Frame (H2OFrame)","81":"@mmalohlava  It concerns row filtering. I have added a binary Vec to my dataframe, now how do i make deeplearning or some other algo ignore the rows with a '1' value for the binary vec? I suppose I need to use the deepslice approach?","82":"@vchollati yes, but there is no explicit Windows script right now. \\n\\nHowever, you can go Spark way (`spark-shell`, `spark-submit`) and specify cmd line parameter `--jars` to point to sparkling water fat jar (should be in directory `assembly\/build\/libs\/`)","83":"Regarding H20-Flow-3, Any idea on why do I get the message \\Exception in thread \\NanoHTTPD Session\\ java.lang.OutOfMemoryError: Requested array size exceeds VM limit\\n        at java.util.Arrays.copyOf(Arrays.java:2367)\\ when I try to \\Download POJO\\ ?\\n","84":"@geponce: @paragsanghavi replied to you on google groups","85":"@srisatish for Spark packages: $SPARK_HOME\/bin\/spark-shell --packages ai.h2o:sparkling-water-core_2.10:1.3.3","86":" - Spark 1.3.1 Hadoop 2.4 from https:\/\/spark.apache.org\/downloads.html- direct link: http:\/\/d3kbcqa49mib13.cloudfront.net\/spark-1.3.1-bin-hadoop2.4.tgz\\n - Sparkling Water 1.3.4 (can probably change during weekend) http:\/\/h2o-release.s3.amazonaws.com\/sparkling-water\/rel-1.3\/4\/index.html (the backup version is Sparkling Water 1.3.1 from http:\/\/h2o-release.s3.amazonaws.com\/sparkling-water\/rel-1.3\/1\/index.html)\\n\\n - Demo scripts are located in the downloaded Sparkling Water zip file under examples\/scripts folder or here: https:\/\/github.com\/h2oai\/sparkling-water\/tree\/rel-1.3\/examples\/scripts\\n  \\n We have several applications which can be run from Sparkling shell:\\n   -  Craigslist application ( craigslistJobTitles.scala)- based on job description it predicts job category (e.g., \\labor\\ \\eduction\\ ...)\\n   -  Chicago crime ( chicagoCrimeSmallShell.script.scala) - predicts probability of arrest for given crime  some nice graphs in flow explaining crimes in Chicago\\n   - Ham or Spam ( mlconf_2015_hamSpam.script.scala) - for a given message predicts if it is spam or normal message (ham)\\n   - CitiBike demo ( strata2015_demo.scala) - predicts number of bikes at given stations and hour\\n\\nTo run them, please launch Sparkling Shell: bin\/sparkling-shell -i examples\/scripts\/craigslistJobTitles.scala\\n\\nThere are also several standalone applications (Spark guys call them self-contained applications) corresponding to scripts listed above,\\nHOWEVER one app is fresh new: Craigslist Streaming Application (it builds a model, and then open a Spark stream and classifies incoming events).\\n\\nTo run it:\\n   1. window: bin\/run-example.sh CraigslistJobTitlesStreamingApp\\n   2. window: nc -lk 9999\\n\\nwait for 1. window build a model and then send job descriptions from the 2nd window. Window #1 will show you predicted job category for events you send via 2nd window\\n","87":"I'm planning on adding a minimalist, fully transparent, opt-out telemetry system to H2O 3, to help us  find and fix problems in the software.  I've begin a new thread in the Google Group here: https:\/\/groups.google.com\/forum\/#!topic\/h2ostream\/C8B6bHP2Q4A\\n\\nOf course, you're welcome to discuss it here as well.  :-)","88":"I want to use this project in Eclipse.Who have done this?","89":"@pcchong : sorry for the delayed response.  We've had a few internal developers do H2O development in Eclipse in the past, but they've all moved to IntelliJ.  @bghill was looking at Eclipse yesterday, and the Eclipse syntax checker  was giving errors on some fairly advanced type parameter usage, which works fine in both javac and IntelliJ.  Looks like an Eclipse bug.  We'll be tracking that down and will get back to you.","90":"@rpeck @paragsanghavi  Hi,  So,  is the size of a H2O model an issue regarding the process of downloading POJO and the resulting predict vector from H2O Flow server?   Is there anyway to get rid of this limitation within the JVM set up?","91":"@pcchong I've just sent an email with screenshots of each step.  I'll put them together as a blog post, but for now, the directions I emailed you should help you get it running in the latest Eclipse.","92":"@paragsanghavi  The water meter only works in Linux, right?  Mac OS ?","93":"@geponce Correct.  The water meter will only show you the performance of the H2O server if it is running on Linux.","94":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/qNcF\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/qNcF\/blob)","95":"@paragsanghavi @bghill @rpeck  ","96":"@bghill @paragsanghavi @rpeck  ~ 13 hrs (DRF, 66M rows training data and 11M rows for validation data, 400 Trees, Depth 20, 12 predictors (most of the columns are real, I  reduced the size of my  tables by reducing the precision to 3 using \\format(myDataTable, digits=3)\\   ","97":"@bghill @paragsanghavi @rpeck Server specs:  Dell R710, 2 Xeon processors with 16 Cores and 290 GB Ram  ","98":"@paragsanghavi @bghill @rpeck  How to run h2o.saveModel in R ? Documentation in R (June 14, 2015) has a difference in the parameters between the example and the specification of h2o.saveModel(...) ? Should I still use ... save_cv = TRUE, force = TRUE... ?","99":"@geponce You shouldn't have to use those two parameters.  force= TRUE only overwrites the file if you try to save the file twice to the same name.  I think save_cv isn't in the current version, and I don't think it would apply to your model anyways.","100":"@bghill @paragsanghavi @rpeck Is the newest version of h2o.randomForest only intended for use on classification ?","101":"No, random forest works for binary and multinomial classification and for regression.  In H2O 3 the type of the response column determines whether you get classification or regression.  If your response column is a categorical (aka an enum) you can convert it to numerical before training to get regression.  This is true of all the algos.","102":"Folks got confused sometimes by the automatic conversion in H2O 2, so we made it more explicit \/ user controlled in H2O 3.","103":"@rpeck  Thanks... I just got confused while reading current documentation where it says: \\H2O supports a number of standard statistical models, such as GLM, K-means, and Random Forest\\nclassification. \\","104":"Ah, I'll see that that gets fixed.","105":"Thank you!","106":"@geponce, that seems like an unexpectedly long training time for that size of data.  We really need to dig into the details with you.  Is it possible to send us the dataset to profile in-house?  Also, if your number of categorical levels you will likely get better speed and less overfitting by reducing nbins_cats.","107":"@rpeck sure... I'll zip the data... ","108":"@rpeck any email address I can use to send a link ","109":"@rpeck I only have three ENUM variables with 2 to 3 levels ","110":"Hello, when I do a prediction using the python api for any model ( binomial  or bernoulli distribution) I get a frame with 3 columns , the first is NaN and the second ones seem to be probabity for the two classes I'm trying to predict. Shouldn't I be getting the actual class in the first column? I thought it would be fine to just assume that if the number in the column 2 is higher than 0.5 then the predicted class would be 1 but if I do this and I compute a confusion matrix using something like sklearn confusion_matrix I get slightly different numbers than what I get using the model's build in confusion matrix. Then I find ( for the glm ) the threshold parameter which the documentation says it's the \\decision threshold for label-generation\\ but I don't know how to read it from the model so that I can get the right classes from the prediction.  ","111":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/GCp0\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/GCp0\/blob)","112":"@rpeck @bghill @paragsanghavi  it took ~ 8 hrs to run predict over ~ 600 M rows","113":"@rpeck @bghill @paragsanghavi  I reduced precision of real columns to 3 digits for all my data tables.","114":"I got the following error when I tried to convert the predictions vector into a data.frame @paragsanghavi @bghill  ","115":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/tXDM\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/tXDM\/blob)","116":"@paragsanghavi @bghill I prefer to use data.table instead of dataframe, but I also got this: ","117":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/qCu0\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/qCu0\/blob)","118":"Anyone ever see this? (not sure if should report a bug)","119":"```\\n> localH2O = h2o.init()\\nSuccessfully connected to http:\/\/127.0.0.1:54321\/ \\n\\n\\nERROR: Unexpected HTTP Status code: 404 Not Found (url = http:\/\/127.0.0.1:54321\/3\/Cloud?skip_ticks=true)\\n\\nError in fromJSON(rv$payload) : unexpected character '<'\\n```","120":"Ok, back with a bit more information, it seems i had some issues converting from my h2o frame to numpy but now that fixed that I can see that the prediction column in the prediction frame is always 1 and I can see why this happened because I looked in the POJO at the score0 method and it seems that the threshold for converting the probabilityies to classes is 0 so of course the class is always 1  ","121":"ah btw this seems to only happen in glm ","122":"(btw, with above error, I can reach the h2o web service just fine in the browser)","123":"@mihaisecasiu I'll ping Tomas the GLM guy to take a look at the issue.  Feel free to file JIRA tickets for these things.  I go through every day or two and triage the tickets.","124":"@andrewcstewart That sure looks like a bug to me.  The binding is trying to get cloud status to see if it's ok.  In your case (and in most cases) it is, so you're good to go, but it's probably not detecting cloud failures correctly.","125":"@mihaisecasiu https:\/\/0xdata.atlassian.net\/browse\/PUBDEV-1459","126":"@andrewcstewart https:\/\/0xdata.atlassian.net\/browse\/PUBDEV-1460","127":"@bghill Did you get a chance to take a look to the data? ","128":"@geponce I'm playing with the data currently.  I'll let you know what I find.","129":"@bghill Thanks... Is there anyway to visualize  tree-diagrams out of DRF-h2o models,  just an excerpt?","130":"Not currently.  There is a desire to do it.  We have had that as a request by others.  We are adding visualizations as a part of the H2O Flow (the web UI) development.","131":"@geponce For giggles I'm also running Deep Learning on the data (I already ran GBM and GLM with defaults just to see where they land). From the logs you sent, I see that your 400 tree model gave a MSE of 0.028679 on the validation data.  So that is the score to beat.  Of course, with rerunning the 400 tree RF model, I am mostly trying to get a sense of whether the generated model could be trimmed down in size.  But if I can stumble upon a faster more accurate model, that could be fun.","132":"@geponce Still waiting on the RF tree construction.  While playing with different model types, I did find I can build a GBM model that gets a validation MSE of 0.0259 in less than an hour.  This was on 32 cores shared with other jobs and 60G of memory (I probably didn't need it all, but I was trying lots of different model types).  I went with the defaults except for 150 tress, and a max_depth of 15.  The resulting model is also smaller (31MB) and so should be faster for scoring.  ","133":"@bghill Do you think that one of the variables (ENUM) with a lot of 0's can be causing the timing and object size ? I don't remember where I read something related to this in the h2o documentation... ","134":"@geponce Not in this case.  Your categorical columns only have two values in each (zeros aside).  The slow down you are referring to can happen for large numbers of categories and category bins.  In your case, the nbins_cats variable may say 1024, but it actually shrinks to fit your data.","135":"Can h2o's deep learning library be used in a semisupervised context","136":"@andrewcstewart: @arnocandel is the person to ask.  He's overseas right now, so it might be a bit before he can get back to you.","137":"@bghill Regarding the data set I sent you,  would you suggest to use GBM instead of DRF?  However, GBM it is more prone to overfit  than RF, right? And also can have trouble with noisy data?  ","138":"@geponce While that can happen with GBM in some cases, your current data looks pretty good as far as this is concerned.  66M training with 11M validation is lots of data.   For smaller data sets you would need cross validation to prevent over fit (as well as playing with learning rates and other parameters).  When I made the GBM model, it seemed to get even slightly better results on the validation data.  You should see the sign of overfitting in the MSE for the validation being poorer than for the validation. ","139":"@geponce Are you concerned with noise in the data or the training classification?  The criticisms I have read of boosting methods and noise have only been in regard to classification noise.  ","140":"@bghill I also got slightly better results on the val data with DRF.   I'm just trying to get more familiarized with the GBM algo before I give it a try and just was reading something on the drawbacks for GBM and saw the issue related to noisy data, I might missed the part where it makes reference to classification noise. ","141":"I keep getting `Unknown parameter: n_folds` while trying to run h2o.deeplearning (h2o v3.0)  Is the documentation not in sync with the function definitions ?","142":"@andrewcstewart Hi Andrew, cross-validation didn't make it into the current release of H2Ov3.0.  It is coming in the next couple of weeks.  Your are correct that it seems to have slipped through into the documentation.","143":"@bghill  In one of your last comments regarding the overfitting issuess you said: \\...You should see the sign of overfitting in the MSE for the validation being poorer than for the validation.\\  You meant the sign in the MSE for the validation being poorer than for the \\training\\ right? ","144":"@bghill  Any clue on why it takes so long to run the DRF on the data that I sent you? ","145":"@geponce Correct.  It should show up in the difference between the validation and training MSEs.","146":"@geponce Random Forests are inherently slow, and you've built a really big forest.  I'm still looking to see if we can optimize this model, but I am waiting on fixes being done by others first before I can get an answer.","147":"@bghill @rpeck Is ROC only used to evaluate quality and correctness of classification models or can also be used for Regression? Let's say DRF, which has the dual purpose(Regression\/Classification). ","148":"I'd like to build a .net (c#) library to connect to the h2o REST API, but I can't seem to find the JSON schema definition. Can anyone help?","149":"Hi, started local h20 with -Xmx8g and uploaded 2Gb file. Getframesummary fails with OOM:\\n```\\nfrom \/192.168.1.115:54321; by class water.fvec.RollupStats$ComputeRollupsTask; class java.lang.OutOfMemoryError: Java heap space (water.DException.DistributedException)\\n  water.fvec.RollupStats$Histo.map(RollupStats.java:314)\\n  water.MRTask.compute2(MRTask.java:638)\\n  water.MRTask.compute2(MRTask.java:599)\\n  water.H2O$H2OCountedCompleter.compute(H2O.java:698)\\n  jsr166y.CountedCompleter.exec(CountedCompleter.java:429)\\n  jsr166y.ForkJoinTask.doExec(ForkJoinTask.java:263)\\n  jsr166y.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:914)\\n  jsr166y.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:979)\\n  jsr166y.ForkJoinPool.runWorker(ForkJoinPool.java:1477)\\n  jsr166y.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:104)\\n```","150":"@geponce Sorry for the delay.  ROC curves can also be used to evaluate regressions.","151":"@Sam7 I believe this is what you are looking for: http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-shannon\/26\/docs-website\/h2o-docs\/index.html#schema-reference  correct?","152":"@petro-rudenko That shouldn't happen.  Does it happen reliably, or did garbage collection get delayed too long in single run?  I'm working on reducing the memory footprint during parsing.  Does this data set have an categorical columns with extremely large category counts?  That is another item that currently uses more memory than it needs.  I've written a fix for that, but it is still in testing.  ","153":"Yep it's criteo dataset that has high cardinality for categorical columns","154":"@petro-rudenko  Ok, I'll try to play with that a bit later today.","155":"@bghill  thanks for responding :) I was actually looking for the json schema. I actually found it by looking at the python  scripts.","156":"http:\/\/localhost:54321\/3\/Metadata\/schemas","157":"@petro-rudenko can you specify your java version? Java7 or Java8?","158":"@mmalohlava ```java version \\1.8.0_45\\\\nJava(TM) SE Runtime Environment (build 1.8.0_45-b14)\\nJava HotSpot(TM) 64-Bit Server VM (build 25.45-b02, mixed mode)\\n```","159":"oki, thanks a lot!","160":"Sorry to plug me here with something that isn't probably relevant to the current h2o-3 trunk. I have a local test spark 1.4, scala 2.11 env. When I run sparkling water sparkling-shell, the --jars with the assembly are added : \\n````\\n15\/07\/12 00:52:18 INFO SparkContext: Added JAR file:\/sparkling-water\/assembly\/build\/libs\/sparkling-water-assembly-0.2.17-SNAPSHOT-all.jar at http:\/\/192.168.1.6:50586\/jars\/sparkling-water-assembly-0.2.17-SNAPSHOT-all.jar with timestamp 1436655138712\\n```\\nBut ...\\n```\\nscala> import org.apache.spark.h2o._\\n<console>:20: error: object h2o is not a member of package org.apache.spark\\n       import org.apache.spark.h2o._\\n```\\nAny ideas ?  Sorry if too obvious. Thanks.","161":"It seems an issue with spark 1.4 spark-shell not adding the jars passed in --jars. Must use SparkContext.addJar() instead.","162":"Hi is there a way to convert a string frame to enum? E.g. i have a categorical columns in spark that are strings (e.g. \\cat1\\ \\cat2\\ etc.) - i've transformed to h2oframe and trying to call:\\n```scala\\ndataFrame.colToEnum(Array(colName))\\n```\\nAnd get Operation not allowed on string vector.","163":"Hi trying to play with 11gb criteo dataset on h2o cluster (launched on yarn 50 nodes cluster). It fails at parse phase: http:\/\/pix.toile-libre.org\/upload\/original\/1436976089.png\\n\\nIn logs i see only:\\n```\\n07-15 15:57:19.017 10.50.225.155:54321   1218   FJ-0-15   INFO: Canceled job $03010a32e19b32d4ffffffff$_b9a7dc85176438f5283f1f71ca3243fa(Parse) was cancelled again\\n```\\n\\nWhere can i get more information why it fails?","164":"@petro-rudenko  I think we may need to add that.  In native H2O you only get a string column if you ask for it or if your column is >= 95% unique values it is guessed to be strings.  In either case, the need to switch back to enum is unlikely.  I hadn't considered data coming in from Spark.  I'll open a JIRA ticket for that.","165":"@petro-rudenko Only 50 nodes, why so few?  ;)  I can't think of any failure mode that should cause a cancel, but I'll go through the code to see if I am forgetting something.  That is really weird.  It looks like ingestion of the file went quickly (yay!).  After ingestion (which hits 100%) there are 4 actions that happen after that.  Categories are collected across nodes and unified, the data is compressed, the unified category domains are distributed to each node, and finally basic summary stats are calculated over the data.  Each of these actions gets reported in the spot that says \\CANCELLED\\ in your screen view.  Any idea which was the last one to occur?  If you couldn't see you can also get it in the logs.  In the image above, your \\Select Log File Type:\\ is set to \\info\\. Try \\trace\\.  You'll get more details.","166":"@petro-rudenko Where is the 11G Criteo dataset?  I see larger ones and smaller ones.  I'd like to play with it.","167":"A note on parsing.  Several improvements are on their way.  Faster categorical unification is almost done.  This will vastly improve the memory consumption during the parse of datasets with extremely large categorical sets.  The current method is fine for most datasets, but I've come up with hostile test cases where I'd like to see better performance.  All parsing should benefit from it.  Next up is another memory\/speed improvement that will do compression as parsing occurs instead of as a post processing step.  After that I've got a few new formats that I will work on supporting.","168":"@bghill here's a dataset from original kaggle challenge: https:\/\/s3-eu-west-1.amazonaws.com\/criteo-labs\/dac.tar.gz - mine dataset is almost similar, but with few other columns (not important ones)","169":"@bghill Getting back to my issue with DRF and the size of the resulting object model and the time is taking to run the DRF model and the prediction task... You suggested to run GBM as an alternative, but if I'm more interested on reducing variance, DRF is my best option, right? Since GBM is more on the bias reduction... Also, I'm wondering how GBM can be faster than DRF, if GBM, intrinsically, has to run one tree at the time... right? while DRF can take advantage of the parallelization of trees... ","170":"@petro-rudenko Thanks, downloading it now.","171":"@geponce Let me see if I can get the person who implemented our GBM and DRF to answer","172":"i was watching Venkatesh Ramanathan's video, and I was wondering if he released the code\/data to the public","173":"@geponce The guy who implemented them doesn't have a Gitter account yet.  In short, yes, DRF would likely be better at reducing variance.  As far as speed, there are a number of ways to parallelize tree building.  Our DRF is currently designed for situations where all the data doesn't fit into the memory of a single node.  In the past we have had other RF implementations to use for smaller datasets.  We are looking at how we want to move forward for improving the performance.","174":"@petro-rudenko It seems that the 17th column has over 10 million unique values.  That is above H2O's current limit on categories for a single column.  If you set that column type to string, the parse should work.  I've got three things to do here.  One is to figure out why your job reported as cancelled and not failed.  The other is to see why the error message didn't propagate.  Finally, I already have a JIRA ticket for this, but I need to report which column it is that blew the categorical limit.  ","175":"@petro-rudenko Looking at the data, the 18th column has 2.2M categories, the 26th has 8.4M categories, the 30th has 5.5M, and the 36th has 7M.  Any algorithm that does categorical expansion will have some serious problems with these columns.","176":"Hey guys. Is 120GB data generated or it's original data? from here: \\nhttps:\/\/github.com\/h2oai\/h2o-2\/wiki\/Hacking-Airline-DataSet-with-H2O\\n","177":"@bghill we're using spark's freqItem (count-sketch), to get most frequent items, all other encode as a special category. Winning solution for criteo used similar approach:\\n![](http:\/\/pix.toile-libre.org\/upload\/original\/1437132967.png)","178":"@petro-rudenko The 120GB data is the base dataset repeated 10 times over.  It is mainly for looking at scaling and speed.","179":"@petro-rudenko Good to hear.  Just wanted to make sure.  With such a wide variety of skill levels around, it is sometimes important to warn folks not to try the impossible.","180":"Is there anyway to get the **range** of values of a prediction object (H2OFrame) in R ?  I got this:  **dt.predictions <- h2o.predict(md, dt.main.hex)** and then **dt.predictions2 <- data.table(dt.predictions)**. However, **class(dt.predictions2$V1)** is still a H2OFrame and not a vector, I guess the only option is to save it as CSV and then reload it, right?","181":"@geponce At the moment we can convert a H2OFrame to a data.frame which you can in turn make into a data.table. To convert a H2OFrame to data.frame just run ```dt.predictions2 <- as.data.frame(dt.predictions)```. However if you just want the range of the frame you can run ```min(dt.predictions$predict)``` and ```max(dt.predictions$predict)```.","182":"@amywang718 Thanks Amy...  Interesting, first time that I see a data.table inheriting data frame that doesn't work","183":"Hi i have several questions:\\n1) How many columns does h2o supports? What's the overhead per column? The problem is when i transfer from spark dataframe of sparse vector of size 200000 it puts down all h2o cluster.\\n2) How does word2vec in h2o works? Does it accepts a single column of type string or multiple columns? Here's a script i'm running:\\n```scala\\nval fr = h2oContext.asH2OFrame(data)\\n    val word2VecParams = new Word2VecParameters()\\n    word2VecParams._minWordFreq = 10\\n    word2VecParams._train = fr\\n    val word2VecModel = new Word2Vec(word2VecParams).trainModel().get()\\n    val vec = word2VecModel.score(fr, $(labelCol))\\n```\\n\\nand get ``` No Model Metrics for Word2Vec.```. I want to transform categorical string to a single vector and run GLM on it. Thanks","184":"get cluster stucked, even though getCloud shows all instances are green. In log see next:\\n```07-23 16:38:48.911 10.50.225.6:54321     12887  FJ-123-1  ERRR: Possibly broken network, can not send ack through, got 600 resends.```\\nCluster is under vpn on yarn though sparkling-water.  Yarn and spark show all nodes healthy also. What could cause it?","185":"@bghill Should I try the new H2O release (DRF) for the data set I sent you before? ","186":"@petro-rudenko 1) We don\\u2019t have a set limit on columns. We\\u2019ve had customers successfully use column counts above 10K.  While our current design is set to scale on row count,  we are constantly pushing to make things more efficient to handle higher column counts.  For 100K sparse columns we still need to implement internal storage that takes greater advantage of the sparsity.","187":"2) Word2Vec is still underdevelopment, and not ready for production use.  We don\\u2019t expose it in the REST API.  I forgot that it can be seen through Sparkling Water.","188":"@petro-rudenko Our current underlying network infrastructure relies mostly on UDP with an additional reliability layer.  In the past, it has given us speed advantages.  We have now twice systems whose routers are repeatedly blocking specific UDP packets (for unknown reasons).  This effectively killed any resend from getting through.   We\\u2019ve are testing moving completely to TCP currently.","189":"@geponce The current release has the underlying infrastructure needed to stream out extremely large models, but the actual streaming has not been added.  I hope to add it in the next couple of weeks.","190":"@bghill Do you think precision of the float variables could be the main issue related to the size of the H2O object? I guess we discussed this, just wondering how Java is handling the numeric precision... if you change the values from float to integer in the data set I sent do you think that would significantly reduce the size of the H2O object model ?","191":"@geponce I don\\u2019t think that it would.  There are likely to be just as many if statements.","192":"Hi, DRFModel for regression makes a predict column of type integer, not double:\\n```scala\\nval drf = new DRF(rfParams)\\nval rfModel = drf.trainModel.get\\nrfModel._output.nclasses() == 1 \/\/regression\\nrfModel.score(frame).vec(\\predict\\).isInt == true \/\/Should be double\\n```","193":"Hi, how does categorical variables encode in H2O DL model? One hot encoding or something different?","194":"Hi Petro, one-hot encoding","195":"@petro-rudenko regarding DRF `isInt()` issue - can you give us more details? i created simple regression DRF model, call `modelFinal.score(f).vec(\\predict\\).isInt()` and it returns `false`.","196":"I'm investigating h2o and interested in extending some of the build in models, I found this article: http:\/\/h2o.ai\/blog\/2014\/11\/Hacking\/Algo\/KMeans\/ and I'm just wondering if this is still the way to go in the 3.0 release branch? If not any pointers to differences would be appreciated","197":"Specifically I've been digging in the code and it seems like the Schema class is loading models via Reflections, wondering if there are any downsides to building the new models in a separate project and including in the classpath vs building a custom version of h2o? I noticed some comments around hadoop classloading issues, anyone tried this approach and have any feedback?","198":"@davemssavage: The blog post was written for H2O 3, but it's somewhat out of date.  It's being updated, but probably won't be in time for what you want to do.  I'm happy to help here or on h2ostream.\\n\\nIf you want to extend an existing algo I'd do just that, on a fork.  If you want to contribute back your changes you can send us a pull request.\\n\\nYou can create brand new algos and register them via the mechanism that's in H2O.registerExtensions().  Currently your extension will need to be in either the water or hex package to be discovered.  We will loosen this soon (sooner if it's very important to you).\\n\\nH2O.registerExtensions() discovers all subclaasses of AbstractRegister and calls them.  The built-in ones are hex.api.Register and water.api.Register.  You'll see that these register algos and resource roots, respectively.  Use the former as a model of how to register your new algo.","199":"If you're enhancing an existing algo and have new parameters you'll need to add them to the appropriate ModelParametersSchema class.  If you're creating a new algo it's best to begin with an existing one: copy and rename all the classes as appropriate.  You'll find a bunch: the ModelBuilder, the Model.Parameters, the Model (containing the model state), and the ModelOutput.  For each of these there's a Schema which provides stability for the REST API.  Normally all you need in the Schemas are the fields; the rest will happen through reflection magic.","200":"Doing just these should make your algo Just Work (tm) via Flow.  If you want it accessible through R and\/or Python you will need to tweak the bindings.  We don't currently have a nice extension mechanism for those, so if you don't push your changes back to master you'll need to track and merge changes yourself.","201":"Let me know if you have any questions!","202":"@rpeck thanks for getting back to me, at the moment I'm experimenting, I'll try out your suggestions and get back if I have any problems","203":"Excellent!","204":"Does anyone know of a h2o REST API wrapper for .net?","205":"Currently I have Java classes for all the payloads, generated as part of the build process.  I'm working on and off on generated endpoint proxies and hope to have that done in the next few weeks.\\n\\nIt should be trivial to generate .NET classes for the payloads if you send me some examples of what the syntax would be in .NET.  Have you had a look at the Java classes which are in the bindings jar file?","206":"Thanks @rpeck for the response. I've had a look at those and easily modified the py script to spit out a c# syntax. It doesn't seem to be generating the endpoint classes though. Just the models.","207":"Yes, I haven't finished the endpoint proxy generation yet, sorry.\\n\\nIf you would like to contribute back your C# class generation we'd love to incorporate it into the build.  Just send us a pull request for your branch.\\n\\nI'll let you know when I have proxy generation complete for Java Retrofit.  It might be a week or two, since other priorities are more pressing.","208":"@rpeck  done: https:\/\/github.com\/h2oai\/h2o-3\/pull\/25","209":"With h20 flow when building a model (say a DRF), is there an option to randomly do a 90\/10 split for the training_frame & validation_frame  ? ","210":"Ah - SplitFrame achieves this. ","211":"@Sam7: THANKS!","212":"@mmalohlava: can you pull Sam's C# bindings once we've cut simons-5?  Thanks!","213":"@rpeck sure, i am just curious - @Sam7 is there any repository for C# artifacts similar to Maven central?","214":"@mmalohlava yes. it's https:\/\/www.nuget.org\/ once @rpeck get's around to creating a script for the endpoints I'm happy to create a nuget package.","215":"@Sam7 cool! thanks for that - when we are ready i will plug it into our release pipeline","216":"One more note: https:\/\/0xdata.atlassian.net\/browse\/PUBDEV-1851 for tracking C# building and publishing","217":"Is there a way we can score a file and pass an ID through? e.g. record_id, var1, var2... -> record_id, predicted 1","218":"right now we just assume same order","219":"@christophergutierrez  can you explain your usecase in more details? you can do something like this:\\n```java\\nVec recordId = validFrame.vec(\\record_id\\);\\nFrame prediction = model.score(validFrame)\\nprediction.add(\\recordId\\ recordId)\\n```\\n\\nNow prediction frame contains prediction itself and additional column with input \\record_id\\.\\n\\nWill this work for you?\\n\\n","220":"I think so. Basically, when we score a file using the compiled pojo, we'd want to know which score goes with which record. e.g. Suppose the record_id was a social security number and we  scored a file with many people (using social security number as a key). Without depending on the file order, we'd like to know what person (social security number) got what score.","221":"if you're passing the record_id through that should work ","222":"@bghill  I'm trying again the DRF model with the newest H2O version... Any idea why I got this error while I was running h2o.randomForest? \\n|========   4%\\nError in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  :\\nUnexpected CURL error: couldn't connect to host\\nTiming stopped at: 15.101 0.614 1032.699\\nAlso, if I check the web app and click on \\getJobs\\ is says Running in the status column...\\n\\n\\nThanks,\\nGuillermo ","223":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/4ZMa\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/4ZMa\/blob)","224":"@bghill The job continues working: \\n\\n","225":"@bghill Should I leave it running?  I was working within R-session","226":"@geponce Yes.  If the web app reports things are still running, then I would.  Sometimes the R just doesn\\u2019t get a response fast enough and it believes the H2O server may have died.","227":"@bghill Thanks Brandon... Yesterday I got a 6 hours run time with a DRF training with the latest H2O version for a very similar dataset as the one I sent you before... 66M rows with 11 predictors...  ","228":"@geponce That\\u2019s a nice improvement.","229":"@bghill  Still need to test the prediction part... I guess that's where it took a while before...","230":"Hey all.  Just new and trying out the first h2o demo - the prostate.ipynb, but it isn't able to load the CSV file even when the file path is correct.  Anyone come across that?  I am using the nightly build 3.1.0.3143 - maybe I should have used stable?","231":"@davidljung Do you mean prostate_gbm.ipynb?","232":"Yes","233":"I just get an exception: ValueError: ImportFiles of \/home\/jungd\/h2odev\/prostate.csv failed on [u'\/home\/jungd\/h2odev\/prostate.csv']  (which is the correct path. I've tried various combinations of paths, relative and abs, with and without locate(), but nothing seems to work.  I've just downgraded to 3.0 stable and same issue)","234":"That is part of our nightly test system, so it runs regularly.  Any chance it is a permissions issue?","235":"\\u201cFailed\\ isn\\u2019t helpful. I need to add a better set of messages to ImportFiles.","236":"Unsure.  The JVM and ipython notebook are both running as the same user..  file perms are read for everyone..  (Redhat EL)","237":"Server terminal just prints: 08-27 15:23:59.237 10.130.11.251:54321   6953   #00356-16 INFO: Method: GET   , URI: \/3\/ImportFiles, route: \/3\/ImportFiles, parms: {path=\/home\/jungd\/h2odev\/prostate.csv}","238":"(the jar is in and run from \/home\/jungd\/h2o, which is a sym link to a dir at the same level with the version in the name)","239":"I have a python stack-trace - unsure if you want that pasted in here..","240":"The last part is: \\n```\\n\/home\/jungd\/Enthought\/Canopy_64bit\/User\/lib\/python2.7\/site-packages\/h2o\/h2o.pyc in lazy_import(path)\\n     31   if isinstance(path,(list,tuple)): return [_import(p)[0] for p in path]\\n     32   elif os.path.isdir(path):         return _import(path)\\n---> 33   else:                             return [_import(path)[0]]\\n     34 \\n     35 def _import(path):\\n\\n\/home\/jungd\/Enthought\/Canopy_64bit\/User\/lib\/python2.7\/site-packages\/h2o\/h2o.pyc in _import(path)\\n     35 def _import(path):\\n     36   j = H2OConnection.get_json(url_suffix=\\ImportFiles\\ path=path)\\n---> 37   if j['fails']: raise ValueError(\\ImportFiles of \\  path  \\ failed on \\  str(j['fails']))\\n     38   return j['destination_frames']\\n     39 \\n```\\n","241":"@davidljung I\\u2019ve got our python packaging person looking into this.","242":"Is this from a pip install?","243":"@bghill  thanks - appreciate it.  From the same notebook, I can use pandas.read_csv() on the file just fine.  Yes, pip install.","244":"(into my local user site-packages, not system-wide one)","245":"Is your H2O instance only running locally, or is there more than one node?","246":"There are two nodes, but the ipython notebook is running on the node I started first.  (unsure if any node is special?  Does the order in flatfile.txt matter?)","247":"(I can certainly try it with one node, if that would help eliminate variables)","248":"Is the second node running on your notebook?","249":"The parse is parallel, so all nodes need to be able to see the file.","250":"(have to run to a meeting... back soon).   The init() response shows 2 nodes in the cloud, if that is what you mean.  There is no ipython notebook on the 2nd node.","251":"Ah - I didn't install python h2o on the 2nd node - I thought it was just a 'client' side part and the file would be read by JVM.","252":"If you have a file that only one H2O node can see then you can use UploadFile()","253":"the python H2O doesn\\u2019t need to be on both nodes.","254":"It is just a client talking to the H2O server cloud.  It is the server cloud that reads the file (unless you upload a file).","255":"ok, that did the trick: scaling back to 1 node where the file is locally.  I didn't understand that the 2 nodes needed the same file in their local filesystems at the same place (a case where reading a remote from S3 would be easier..).  Thanks for your help.","256":"@davidljung I\\u2019ve got it on my ToDo list to add an error message that clarifies this. I\\u2019m also considering letting one node parse the file, and distribute the data with a warning that the user is reading in the data the \\u201cslow\\u201d way. For playing around this is just fine and users with \\u201cBig Data\\u201d are suitably warned.","257":"@bghill Do you know if the resulting model will be available through my R-session once it finishes ?","258":"@geponce If your R-session has just lost connection or the connection has timed-out, then yes.  You should be able to connect and re-connect an R session to H2O at any point.","259":"@bghill Well, I didn't close my R-Session and the same h2o.init server is still running ","260":"h2oServer\\nIP Address: 127.0.0.1\\nPort      : 54321\\nSession ID: _sid_9c453a6c1971d9ece7c9cfab67c840ad\\nKey Count : 4","261":"I just don't see the object ","262":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/CeiX\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/CeiX\/blob)","263":"@bghill ","264":"@bghill How can I retrieve  model_DRF_66M_EEMT.hex  within my R-session?","265":"I belive h2o.getModel(model_id, conn = h2o.getConnection(), linkToGC = FALSE) should return a link to your model.","266":"myModel <- h2o.getModel(h2oServer, \\u201cmodel_DRF_66M_EEMT.hex\\u201d)","267":"@bghill Do you know if I can benefit from using Sparkling Water, in terms of speed? Not sure if I can access DRF from R using Sparkling Water","268":"@geponce Sparkling Water runs H2O inside of Spark.  There is no speed advantage.  The purpose is to give Spark users acess to the H2O algorithms.","269":"@bghill So, when I convert my data table into h2o object( using as.h2o), is the file converted into HDFS internally?  Spark is supposed to be a faster approach due to the 'run in memory' capabilities along with the RDD features for data from HDFS or any other file,  right?","270":"@geponce H2O runs in memory.  When you read in a file it is converted into our own internal in memory data structure where each chunk of data is compressed according to its type and properties.  This saves memory and gives you speed (due to cache efficiencies).  Spark is moving away from RDDs in favor of an approach similar to ours.","271":"@bghill Hi Brandon, do you have any case or example where you guys have implemented a solution using DRF?   I read some stuff that H2O is doing with paypal for fraud detection using deep learning algorithm... any other good example for other algorithms?    ","272":"Hi! When I train a model or parse data I'm not able to follow the progress. It stays at 0% and the \\view\\ button doesn't do anything. I can see in the job tracker that the tasks actually do complete without issue though. Is this an issue in the Edge browser on Windows 10 (which seems a bit strange) or have I not configured something correctly?","273":"That sounds like an Edge bug.  @lo5, have you seen this?","274":"@geponce Here are three sets of examples:https:\/\/github.com\/h2oai\/h2o-3\/tree\/master\/h2o-docs\/src\/product\/flow\/packs\/examples","275":"https:\/\/github.com\/h2oai\/h2o-3\/tree\/master\/h2o-r\/demos","276":"https:\/\/github.com\/h2oai\/h2o-3\/tree\/master\/h2o-py\/demos","277":"Sorry they weren\\u2019t easy to find. We\\u2019ve got that on someone\\u2019s ToDo list.","278":"@bghill \\n```\\ndt.model.prediction <- h2o.predict(model, dt.main.table)\\ndt.pred.column <- data.table(pred = dt.model.prediction$predict)\\nhead(dt.pred.column[, list(pred)])\\nError in `[.data.table`(x, i) :\\n  invalid type\/length (S4\/6) in vector allocation\\n```","279":"@bghill I don't remember if I already mentioned this to Matt D. \\n","280":"@bghill  Is he in this chat? ","281":"there's still no way to convert the resulting 'vector' of predictions into something that can be attached to another data.table right? ","282":"@geponce I\\u2019ve sent an invite to Matt to get him to join.","283":"I just recalled that what I ended up doing is saving the predict vector as csv and then reload it back as a data.table or data.frame...  I don't see how can I do an straight column-append, e.g. dt.table[,predictions:=dt.model.prediction$predict]","284":"@geponce That is probably simplest.  I don\\u2019t think Matt has added any means to directly import from H2O.","285":"I just went back to this chat log and found that Amy mentioned that as.data.frame works... dt.pred <- as.data.frame(dt.md.predictions)","286":"@bghill  I  keep getting this error everytime I try to access the prediction vector from H2OFrame","287":"dt.pred<-data.frame(predictions=dt.md.predictions$predict$predict)\\n\\nERROR: Unexpected HTTP Status code: 400 Bad Request (url = http:\/\/127.0.0.1:54321\/3\/DownloadDataset?frame_id=subset_40&hex_string=1)\\n","288":"already used as.data.frame, as.data.table,  data.frame and data.table... What would be the right way to get the predict vector out of a H2OFrame ?","289":"I guess h2o.downloadCSV should work... testing now... ","290":"It didn't work... I get the same ERROR 400: bad request","291":"How big is that frame?","292":"object.size(dt.md.predictions)  = 4120 bytes","293":"138M values (rows)","294":"The 4120 bytes is the size of the local prediction object, not the data itself (which is still held in H2O).","295":"Since H2O can run on huge datasets, many of the local H2O objects in R are pointers to the actual data on the cluster. Otherwise we\\u2019d fill your local laptop memory.","296":"How can I save that predic vector into disk ?","297":"I need to generate some raster\/image data with that... ","298":"@bghill  I need to append that predict vector to another table that has a reference to pixels (pixel_id)","299":"dt.pred <- as.data.frame(dt.model.predictions) didn\\u2019t work?","300":"No...  dt.pred <- as.data.frame(dt.md.predictions)\\n\\nERROR: Unexpected HTTP Status code: 400 Bad Request (url = http:\/\/127.0.0.1:54321\/3\/DownloadDataset?frame_id=predictions_a7eb_model_DRF_66M_cover_SRAD.hex_on_file6f862bada8\\n3b_csv_5.hex_6&hex_string=1)","301":"I really thought our limit on downloading was 2G.  Your prediction results should only be 1104M (138M *8bytes).  ","302":"It could be that the limit is lower than I realize (any limit is a bug). I know the limit issue was fixed for h2o.exportFile() recently. That should have no limit size.","303":"exportFile is working... ","304":"Ok, it may be a little slow since that is a fair bit of data to transfer.","305":"I tried h2o.downloadCSV  but didn't work... That method uses 'wget' and for some reason it didn't go through... ","306":"Once the data gets above a certain size, it needs to be streamed to the client. It seems exportFile is the only method that has had the streaming capability added so far.  All the rest are going to happen, but clearly haven\\u2019t yet.  Sorry for the hassle.","307":"@bghill thanks Brandon...  exportFile works :\ufffd:  ","308":"I\\u2019m glad to hear it.  We need to put the limits into the documentation until the fixes are in.","309":"Even if it is to shame us into fixing it faster. Makes no sense to crunch big data but not be able to save the results because they are \\u201ctoo big\\u201d.","310":"@geponce I\\u2019m told that the fix has been put in over the weekend for other means of downloading the predictions frame.  There are some methods that aren\\u2019t going to be updated, and their documentation notes that they aren\\u2019t intended to pull big data frames. I need to check what the latest error messages are in this case, so users can understand what the issue and solutions are.","311":"@bghill :\ufffd: ","312":"@bghill do you know how DRF deals with -Inf values within a data set ? ","313":"I'm working in R ","314":"Off the top of my head, I don\\u2019t know. I\\u2019d have to try on a small model to see. ","315":"Hi guys, I am new to H2O and before diving in I would like to know what NNs H2O supports. More specifically I am interested in ConvNets, Deep Belief Nets, Recurrent NNs, Long-Short Term Memory units.  I cannot seem to find this information on the website. Thanks in advance and I am sorry for any duplications.","316":"@mongoose54 we have feedforward NNs.  i am in the process of updating our deep learning booklet, but you can check out the work in progress here: http:\/\/h2o-release.s3.amazonaws.com\/h2o\/master\/3157\/docs-website\/h2o-docs\/booklets\/DeepLearning_Vignette.pdf","317":"I have a question regarding H2O flow. In my model output what is the unit of the run_time? I got this `run_time\\t160156`","318":"milliseconds","319":"@bghill Thanks!","320":"@r3tex I don\\u2019t have access to an Edge instance. Is the console reporting any errors? F12 to bring up dev tools, then Ctrl\ufffd for the console.","321":"I'm getting this error:\\n```\\nERROR: Unexpected HTTP Status code: 412 Precondition Failed (url = http:\/\/127.0.0.1:54321\/99\/Rapids)\\n\\nError in .h2o.doSafeREST(conn = conn, h2oRestApiVersion = h2oRestApiVersion,  : \\n  *Unimplemented* failed lookup on token: ``. Contact support@h2o.ai for more information.\\n````","322":"I have no ideia what is going on.","323":"I was able to run and not to get this error anymore. Not sure what happened but now it is working fine.","324":"Curious.","325":"Yeah ... I got the error when I ran:\\n`rand_vec <- h2o.runif(subset_sample) `\\n","326":"I\\u2019ll let the Rapids expert know.  What version are you using?","327":"@bghill \\u2018h2o\\u2019 version 3.2.0.3","328":"I couldn't find any example on how to use `weights_column` in  `h2o.glm`. Can someone point me to a place where I can find it? ","329":"@gdequeiroz Does this help: https:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-r\/tests\/testdir_algos\/glm\/runit_GLM_weights2.R","330":"Hi , Are there any best practices or documention related to production implementation of work done in H2o R ","331":"The https:\/\/h2o.gitbooks.io\/h2o-world-2015-training\/content\/ says its being made , the older version says do not use this. ","332":"Is there any updated version available","333":"There are some drafts for other tutorials. Our docs person is checking with the different authors to see if they can be published.","334":"Thanks @bghill  , Can you please tell Are there any best practices or documention related to production implementation of work done in H2o R","335":"For going into production, you can either use H2O as a scoring server that you use via REST calls, or you can get your models as a Plain Old Java Object (POJO). You can then compile this code and put it into your own server environment without H2O.","336":"The later is discribed a bit here: http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-slater\/5\/docs-website\/h2o-docs\/index.html#POJO%20Quick%20Start","337":"The JavaDoc for the POJO form of the models is here: http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-slater\/3\/docs-website\/h2o-genmodel\/javadoc\/index.html ","338":"What can be unified approach for R , python , scala based work","339":"We are running Sparking water","340":"Let me get our Sparkling water expert to respond to that.","341":"Oh @mmalohlava where art thou?","342":"@bghill we are keen to get Enterprise Support for H2O at Telstra. Are you able to provide cost and best practice  guides for Spark deployments?\\ncc:@jagatsingh ","343":"@haty1 As far as costs, someone will contact you on that. As an engineer, I try to stay out of such things. We\\u2019ve got your contact info through the form you filled out. We can have an SE contact you to answer your questions on Spark deployment.","344":"@bghill it would be great to speak to a SE contact. Thanks","345":"We are running random forest on sparking water using r . if we specify more that one executors it takes long to run than when we specify 1. Do does it means we cannot run in parallel?","346":"@jagatsingh there is major difference between R\/Python and Scala. R\/Python clients communicating with H2O\/Sparkling Wate via REST API, on the other hand Scala code is running directly in JVM. That means with Scala you can access lower level internals of H2O and also Spark, on the other hand REST API provides well defined stable interface. Normally i would recommend to do main hard-work on big data in Scala and process results in R\/Python or Flow UI - for example, there is a demo which train a regression model on big data in Scala, but make predictions and residuals plot from R (see https:\/\/github.com\/h2oai\/sparkling-water\/tree\/master\/examples)","347":"@haty1 regarding Spark deployments - Sparkling Water is developed as an application on top of Spark, so it deployment is strictly driven by deployment of Spark applications - see Spark documentation (http:\/\/spark.apache.org\/docs\/latest\/cluster-overview.html). From our experience, the best way to deploy Spark and run Sparkling water is YARN environment since it is independent on Spark version and does not need any special configuration like standalone clusters.","348":"@jagatsingh  you can run in parallel, but RF by design is network dependent since it needs to collect information about data spread on different nodes. So RF right now scales better on fat machine (lot of cpus, lot of memory) than on many skinny machines. \\n\\nHowever, we are still working on improvements.","349":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/ddzw\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/ddzw\/blob)","350":"@bghill Testing DRF with an Dell R930 144 cores ","351":"Wow, pretty cool","352":"@geponce Progress is now being made on downloading and compiling POJOs for extremely large models like the ones you were making.","353":"@geponce wow! nice water meter screenshot!","354":"@bghill @mmalohlava should I expect a significant reduction in running time for h2o.randomForest compared with our previous server that has 16 cores?","355":"@bghill I'm kind of surprised that running time in one server with 16 cores took around 6.5 hrs to run DRF and with a 144 cores ~ 5.5 hrs... Should we expect a linear behavior on #cores  vs running time ? ","356":"@geponce you should see speedup, but not linear, since drf  parallelizes data histogram  preparation not actual tree building (which is done layer-per-layer, tree-after-tree) - we have several ideas to improve this process and working on them","357":"@mmalohlava Thanks Michal... Thus, #cores vs running time for DRF is more like an exponential to rise maximum relationship, where at some point (#cores), the running time rate will plateau, right? ","358":"@mmalohlava *Correction... Exp. decay of #Cores vs Elapsed seconds (DRF)","359":"[![blob](https:\/\/files.gitter.im\/h2oai\/h2o-3\/y1JX\/thumb\/blob.png)](https:\/\/files.gitter.im\/h2oai\/h2o-3\/y1JX\/blob)","360":"Model file produced by h2o is of 250mb. What are best practices to version control this. Do we need to ?","361":"@jagatsingh are you referring to the model saved to disk in binary form or generated model \\pojo\\ (model represented by java code)?","362":"I am working with @jagatsingh and he is referring to binary form. We had errors trying to save as polo. ","363":"@toddniven right now we do not have any versioning support for binary models (still on roadmap). However, you can version them using git or github LFS.\\n\\nCan you specify error getting from saving model as POJO? We updated the feature in the latest release 3.2.0.5 Slater (http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-slater\/5\/index.html)","364":"Thanks we are not on github at this moment using gitlab free version.  I guess error we should be able to report on monday. ","365":"@jagatsingh perfect! thank you very much and please let us know the pojo-download error ","366":"Any plans to release sparking water with spark 1.5.1","367":"@mmalohlava @bghill We did not get email from your team. @haty1  sent request to find price of support. ","368":"@jagatsingh sorry for that, i am going to ping our sales team ","369":"@jagatsingh Sparkling Water for Spark 1.5 is already released - Spark 1.5.2 - http:\/\/h2o.ai\/download\/","370":"@haty1 can you contact me in private channel ? i will connect you directly with our sales machine","371":"@mmalohlava Cannot see on http:\/\/mvnrepository.com\/artifact\/ai.h2o\/sparkling-water-core_2.10","372":"@jagatsingh probably some synchronization issue between maven central and mvnrepository.com:\\n\\nIf i search for package at maven central: http:\/\/search.maven.org\/#artifactdetails%7Cai.h2o%7Csparkling-water-examples_2.10%7C1.5.2%7Cjar","373":"Thanks, have sent email to Max this morning","374":"What is the best way to solve this:  ` Version mismatch! H2O is running version 3.0.1.2 but R package is version 3.2.0.3`","375":"I'm running h2o on ec2.","376":"and R version 3.1.1","377":"hi @gdequeiroz , you have to use the right R-package with same h2o version. Please follow instructions on h2o download page: http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-slater\/5\/index.html","378":"Hi @mmalohlava. We are running 1000 tree drf in sparkling water on our cluster using 2 executors, 40g ram per executor, and 15 cores per executor. The training set is 250k rows by 100 cols. The model build is taking roughly 1 hr 45 mins. Does","379":"this sound right? We are trying to manage our resources to get the right balance. ","380":"Thanks to awesome support by @mmalohlava  and @bghill . I have written short note on how we implemented Machine learning production pipelines in Telstra cc @toddniven  @haty1   https:\/\/www.linkedin.com\/pulse\/article\/production-implementation-machine-learning-models-jagat-singh","381":"@toddniven for this size of dataset i would expect long runtime even if you use only 2 executors (minimize network traffic) - what is depth of trees? 20?\\n\\nIf you are interested you can try top of master which contains several patches especially to speed up computation in DRF\/GBM","382":"@jagatsingh that's great! thank you for nice article!","383":"We are using all defaults except.\\n ```    \\nrfParams._ntrees = 50\\n rfParams._binomial_double_trees = true\\n``` \\nMaster we can try on Monday\\n","384":"thanks @mmalohlava. We will keep you updated with our progress and thanks so much for all the help you and everyone at h2o are providing.","385":"@jagatsingh you do not need ` rfParams._binomial_double_trees = true`","386":"i expect","387":"Hello everyone. I am new to h2o","388":"Is there any good book or article that I can start with","389":"Start with h2o docs on there website","390":"There is free ebook,right? Officially providedin the website","391":"Ar.. That's from world event last year","392":"Seems its for old version. ","393":"Did you check version ","394":"@bawongfai here are the latest versions of the booklets: https:\/\/github.com\/h2oai\/h2o-3\/tree\/master\/h2o-docs\/src\/booklets\/v2_2015\/PDFs\/online","395":"@bawongfai are you a R or Python user? (or something else)","396":"Also, the docs (for latest stable release) are here: http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-slater\/5\/docs-website\/h2o-docs\/index.html","397":"@ledell  is it upgraded version of pdf ebook on gitbook","398":"@ledell Python ","399":"@jagatsingh the h2o-2 ebooks were on leanpub and i think the new h2o-3 books will be on gitbook.  right now, there is nothing on our gitbook: https:\/\/www.gitbook.com\/book\/h2o\/h2o-world-2015-training\/details"},"predicted topic":{"0":"H2O installation and usage in Python","1":"H2O installation and usage in Python","2":"H2O installation and usage in Python","3":"H2O installation and usage in Python","4":"H2O installation and usage in Python","5":"Bug fix in H2O code","6":"Bug fix in H2O code","7":"UNDEFINED","8":"UNDEFINED","9":"Bug fix in H2O code","10":"UNDEFINED","11":"Bug fix in H2O code","12":"Bug fix in H2O code","13":"Bug fix in H2O code","14":"Bug fix in H2O code","15":"Bug fix in H2O code","16":"Bug fix in H2O code","17":"Question about H2O implementation in R","18":"Question about H2O implementation in R","19":"Bug fix in H2O code","20":"Question about H2O implementation in R","21":"UNDEFINED","22":"Question about H2O implementation in R","23":"Question about H2O implementation in R","24":"Question about H2O implementation in R","25":"UNDEFINED","26":"Question about H2O implementation in R","27":"Question about H2O implementation in R","28":"Bug fix in H2O code","29":"UNDEFINED","30":"Question about H2O implementation in R","31":"Question about H2O implementation in R","32":"Question about H2O implementation in R","33":"Question about H2O implementation in R","34":"Bug fix in H2O code","35":"Bug fix in H2O code","36":"Bug fix in H2O code","37":"Bug fix in H2O code","38":"Bug fix in H2O code","39":"Bug fix in H2O code","40":"Language and DSL discussion","41":"Language and DSL discussion","42":"Language and DSL discussion","43":"Language and DSL discussion","44":"UNDEFINED","45":"Language and DSL discussion","46":"Language and DSL discussion","47":"Language and DSL discussion","48":"H2O Specific Questions","49":"H2O Specific Questions","50":"H2O Specific Questions","51":"H2O Specific Questions","52":"H2O Specific Questions","53":"H2O Specific Questions","54":"UNDEFINED","55":"Spark and H2O Integration","56":"Python API Questions","57":"Python API Questions","58":"Python API Questions","59":"Language and DSL discussion","60":"UNDEFINED","61":"UNDEFINED","62":"UNDEFINED","63":"UNDEFINED","64":"UNDEFINED","65":"Spark and H2O Integration","66":"Spark and H2O Integration","67":"Spark and H2O Integration","68":"Spark and H2O Integration","69":"H2O Specific Questions","70":"R API Questions","71":"UNDEFINED","72":"H2O Specific Questions","73":"H2O Specific Questions","74":"UNDEFINED","75":"H2O Specific Questions","76":"UNDEFINED","77":"UNDEFINED","78":"Spark and H2O Integration","79":"Spark and H2O Integration","80":"H2O Framework and Packages","81":"UNDEFINED","82":"H2O Framework and Packages","83":"Row and Column Filtering","84":"UNDEFINED","85":"H2O Framework and Packages","86":"H2O Framework and Packages","87":"H2O Framework and Packages","88":"UNDEFINED","89":"Row and Column Filtering","90":"UNDEFINED","91":"UNDEFINED","92":"UNDEFINED","93":"UNDEFINED","94":"UNDEFINED","95":"UNDEFINED","96":"UNDEFINED","97":"UNDEFINED","98":"H2O Framework and Packages","99":"R and H2O Integration","100":"UNDEFINED","101":"H2O Framework and Packages","102":"H2O Framework and Packages","103":"H2O Framework and Packages","104":"H2O Framework and Packages","105":"H2O Framework and Packages","106":"H2O Framework and Packages","107":"Email Communication","108":"Email Communication","109":"ENUM Variables","110":"H2O Framework and Packages","111":"H2O Framework and Packages","112":"H2O Framework and Packages","113":"Data Precision","114":"Data Precision","115":"R and H2O Integration","116":"UNDEFINED","117":"UNDEFINED","118":"H2O Framework and Packages","119":"H2O Framework and Packages","120":"H2O Frame and Numpy Conversion Issues","121":"H2O Frame and Numpy Conversion Issues","122":"H2O Frame and Numpy Conversion Issues","123":"H2O Frame and Numpy Conversion Issues","124":"H2O Frame and Numpy Conversion Issues","125":"JIRA Tickets and GLM Issues","126":"JIRA Tickets and GLM Issues","127":"Data Analysis and Visualization","128":"UNDEFINED","129":"Data Analysis and Visualization","130":"Data Analysis and Visualization","131":"Deep Learning with H2O","132":"JIRA Tickets and GLM Issues","133":"Data Analysis and Visualization","134":"Data Analysis and Visualization","135":"Deep Learning with H2O","136":"Deep Learning with H2O","137":"UNDEFINED","138":"Deep Learning with H2O","139":"Deep Learning with H2O","140":"Deep Learning with H2O","141":"UNDEFINED","142":"UNDEFINED","143":"UNDEFINED","144":"UNDEFINED","145":"UNDEFINED","146":"UNDEFINED","147":"UNDEFINED","148":"C# Library for H2O REST API","149":"Out of Memory Error","150":"Out of Memory Error","151":"UNDEFINED","152":"UNDEFINED","153":"Out of Memory Error","154":"Out of Memory Error","155":"UNDEFINED","156":"C# Library for H2O REST API","157":"Java Version and Memory Footprint","158":"Java Version and Memory Footprint","159":"Java Version and Memory Footprint","160":"H2O with Spark and Scala Environment","161":"H2O with Spark and Scala Environment","162":"Converting String Frame to Enum","163":"H2O and Spark","164":"Criteo Dataset","165":"UNDEFINED","166":"H2O and Spark","167":"Criteo Dataset","168":"Converting String Frame to Enum","169":"H2O Data and Datasets","170":"H2O and Spark","171":"H2O and Spark","172":"H2O and Spark","173":"Criteo Dataset","174":"Criteo Dataset","175":"Criteo Dataset","176":"H2O Data and Datasets","177":"Converting String Frame to Enum","178":"Criteo Dataset","179":"Criteo Dataset","180":"H2O Frame Predictions in R","181":"H2O Frame Predictions in R","182":"H2O Frame Predictions in R","183":"Number of Columns Supported by H2O","184":"Criteo Dataset","185":"H2O Modeling and Predictions","186":"H2O Modeling and Predictions","187":"UNDEFINED","188":"Miscellaneous","189":"Miscellaneous","190":"H2O Modeling and Predictions","191":"H2O Modeling and Predictions","192":"DRFModel Predictions","193":"DRFModel Predictions","194":"H2O Development and Extensions","195":"H2O Development and Extensions","196":"H2O Model Extensibility","197":"H2O Model Extensibility","198":"H2O Model Extensibility","199":"H2O Model Extensibility","200":"H2O Algorithm Discussion","201":"H2O Algorithm Discussion","202":"UNDEFINED","203":"UNDEFINED","204":"C# Bindings","205":"H2O Algorithm Discussion","206":"C# Bindings","207":"H2O Algorithm Discussion","208":"C# Bindings","209":"H2O Algorithm Discussion","210":"H2O Algorithm Discussion","211":"H2O Algorithm Discussion","212":"H2O Algorithm Discussion","213":"H2O Algorithm Discussion","214":"C# Bindings","215":"C# Bindings","216":"C# Bindings","217":"C# Bindings","218":"H2O Algorithm Discussion","219":"H2O Algorithm Discussion","220":"H2O Algorithm Discussion","221":"H2O Algorithm Discussion","222":"H2O Algorithm Discussion","223":"Error Reports","224":"Error Reports","225":"H2O Algorithm Discussion","226":"H2O Algorithm Discussion","227":"UNDEFINED","228":"UNDEFINED","229":"UNDEFINED","230":"H2O Algorithm Discussion","231":"CSV File Loading","232":"CSV File Loading","233":"CSV File Loading","234":"Error Reports","235":"H2O Algorithm Discussion","236":"H2O Algorithm Discussion","237":"H2O Algorithm Discussion","238":"CSV File Loading","239":"CSV File Loading","240":"UNDEFINED","241":"UNDEFINED","242":"UNDEFINED","243":"UNDEFINED","244":"UNDEFINED","245":"UNDEFINED","246":"UNDEFINED","247":"UNDEFINED","248":"UNDEFINED","249":"UNDEFINED","250":"UNDEFINED","251":"UNDEFINED","252":"UNDEFINED","253":"UNDEFINED","254":"UNDEFINED","255":"UNDEFINED","256":"UNDEFINED","257":"UNDEFINED","258":"UNDEFINED","259":"UNDEFINED","260":"UNDEFINED","261":"UNDEFINED","262":"UNDEFINED","263":"UNDEFINED","264":"UNDEFINED","265":"UNDEFINED","266":"UNDEFINED","267":"UNDEFINED","268":"UNDEFINED","269":"UNDEFINED","270":"UNDEFINED","271":"UNDEFINED","272":"UNDEFINED","273":"UNDEFINED","274":"UNDEFINED","275":"UNDEFINED","276":"UNDEFINED","277":"UNDEFINED","278":"UNDEFINED","279":"UNDEFINED","280":"H2O Frame Predictions","281":"H2O Frame Predictions","282":"UNDEFINED","283":"H2O Frame Predictions","284":"H2O Frame Predictions","285":"H2O Frame Predictions","286":"Discussion about H2O's limitation on downloading predictions","287":"UNDEFINED","288":"H2O Frame Predictions","289":"H2O Frame Predictions","290":"H2O Frame Predictions","291":"H2O Frame Predictions","292":"H2O Frame Predictions","293":"UNDEFINED","294":"H2O Frame Predictions","295":"H2O Frame Predictions","296":"H2O Frame Predictions","297":"H2O Frame Predictions","298":"H2O Frame Predictions","299":"H2O Frame Predictions","300":"UNDEFINED","301":"H2O Frame Predictions","302":"H2O Frame Predictions","303":"H2O Frame Predictions","304":"UNDEFINED","305":"H2O Frame Predictions","306":"H2O Frame Predictions","307":"H2O Frame Predictions","308":"H2O Frame Predictions","309":"H2O Frame Predictions","310":"H2O Frame Predictions","311":"H2O Frame Predictions","312":"Question about handling -Inf values in DRF","313":"Question about handling -Inf values in DRF","314":"H2O Frame Predictions","315":"H2O Frame Predictions","316":"H2O Frame Predictions","317":"H2O Frame Predictions","318":"H2O Frame Predictions","319":"H2O Frame Predictions","320":"UNDEFINED","321":"Error with H2O on local instance","322":"Error with H2O on local instance","323":"Error with H2O on local instance","324":"Error with H2O on local instance","325":"Error with H2O on local instance","326":"Discussion about H2O R version and documentation","327":"Discussion about H2O R version and documentation","328":"Discussion about H2O R version and documentation","329":"Discussion about H2O R version and documentation","330":"UNDEFINED","331":"Discussion about H2O R version and documentation","332":"Discussion about H2O R version and documentation","333":"Discussion about H2O R version and documentation","334":"Discussion about Spark deployments and H2O Enterprise Support","335":"Discussion about H2O R version and documentation","336":"Discussion about H2O R version and documentation","337":"Discussion about H2O R version and documentation","338":"Discussion about Spark deployments and H2O Enterprise Support","339":"Discussion about Spark deployments and H2O Enterprise Support","340":"Discussion about Spark deployments and H2O Enterprise Support","341":"Discussion about Spark deployments and H2O Enterprise Support","342":"Discussion about Spark deployments and H2O Enterprise Support","343":"Discussion about Spark deployments and H2O Enterprise Support","344":"Discussion about Spark deployments and H2O Enterprise Support","345":"Discussion about Spark deployments and H2O Enterprise Support","346":"Error with H2O on local instance","347":"Discussion about Spark deployments and H2O Enterprise Support","348":"Discussion about Spark deployments and H2O Enterprise Support","349":"Error with H2O on local instance","350":"Error with H2O on local instance","351":"UNDEFINED","352":"UNDEFINED","353":"UNDEFINED","354":"Discussion about Spark deployments and H2O Enterprise Support","355":"Discussion about Spark deployments and H2O Enterprise Support","356":"UNDEFINED","357":"Error with H2O on local instance","358":"Error with H2O on local instance","359":"Error with H2O on local instance","360":"Model version control","361":"Model version control","362":"Model version control","363":"UNDEFINED","364":"Model version control","365":"Sparkling Water and Spark version","366":"Model version control","367":"Model version control","368":"UNDEFINED","369":"Sparkling Water and Spark version","370":"UNDEFINED","371":"Sparkling Water and Spark version","372":"Sparkling Water and Spark version","373":"UNDEFINED","374":"H2O version mismatch","375":"H2O version mismatch","376":"H2O version mismatch","377":"H2O version mismatch","378":"Implementing Machine learning production pipelines","379":"Implementing Machine learning production pipelines","380":"Implementing Machine learning production pipelines","381":"Implementing Machine learning production pipelines","382":"UNDEFINED","383":"Implementing Machine learning production pipelines","384":"UNDEFINED","385":"UNDEFINED","386":"UNDEFINED","387":"Onboarding new users","388":"Onboarding new users","389":"Onboarding new users","390":"Onboarding new users","391":"Implementing Machine learning production pipelines","392":"Implementing Machine learning production pipelines","393":"Implementing Machine learning production pipelines","394":"Onboarding new users","395":"UNDEFINED","396":"UNDEFINED","397":"UNDEFINED","398":"UNDEFINED","399":"UNDEFINED"}}