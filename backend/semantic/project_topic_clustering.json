{
    "_": {
        "0": "3637bdb2-5bfc-4be6-bdc2-afe415be8040",
        "1": "cb5e54a7-8c1d-4820-a3da-1bff9cfa3708",
        "2": "8d32f2f6-c52b-4ae9-894d-6b6ec8e13234",
        "3": "0f09c5e3-516e-46e0-92a2-79a23d5fbf78",
        "4": "c12b34e6-0bf2-4729-95cb-37f2b236032e",
        "5": "93456f47-5425-4d4e-82c1-4b0aa0f66ad1",
        "6": "d61e9780-e5ea-4913-92ef-d8024f530c9b",
        "7": "e7500d52-b5df-49c5-afcb-c30575b3903a",
        "8": "fe5b5b89-476e-4f8c-9b72-a45718c77099",
        "9": "1a4db4f2-2415-4dfa-be70-29a543df371c",
        "10": "d99bb734-14d4-4418-8d25-8588b9c2c985",
        "11": "c4ae2443-5226-421b-a324-173af7d4573f",
        "12": "7fb9a369-5c93-4b6e-9c04-0b6388e5bcff",
        "13": "8873ef18-c157-4588-8533-df01965685c1",
        "14": "3461788f-c008-471e-9c31-d7acfcad9e67",
        "15": "6ad7df3f-d116-4cc6-a0b1-10ecc00597ec",
        "16": "6e82bcc7-6f0d-4b11-8826-bc71f1b3d5df",
        "17": "ecab7560-8fc8-463f-929f-a8cb146b2a20",
        "18": "77661053-8ea8-4e4c-97d7-26452d56305c",
        "19": "1a0b7115-0aae-479f-bff3-1f285ecb08ef",
        "20": "1319a9ef-970b-43c0-80f0-e15b710fa03c",
        "21": "6c9e40d6-2798-4a74-8447-92362b259ef7",
        "22": "d63cd949-bd5d-421f-90fa-0e8fff3de5a2",
        "23": "4ac18369-8e4d-48e2-9fd6-a7b73797cc29",
        "24": "7c8e97c9-192a-453c-812e-bedf7bf45329",
        "25": "a05c8d5b-9679-418e-bd76-71ff763b6c07",
        "26": "2f5630ff-fa00-4950-a97c-d477c326b9b9",
        "27": "c75225a5-24a2-4342-9be6-696534ecb546",
        "28": "7c94a770-4133-49a5-a8c1-440be1d3d0bd",
        "29": "7f979910-f45f-4772-8884-47c2f42b2670",
        "30": "d4a85dba-1879-438d-82db-7f1e6f25bf4b",
        "31": "ab40fae3-1e90-436c-96a1-6bee4b294794",
        "32": "8089d32c-f4d1-457d-ac23-3cf5bf387321",
        "33": "acbc6f51-7ec1-4a00-a890-5a42e0ff4f9e",
        "34": "f053055e-38af-4ff1-8512-adf5d499cf6f",
        "35": "7d7ac6ab-93bd-49c5-a85f-cd1cd8eb3a90",
        "36": "2a260b33-8520-4a12-84a5-6ac8fa555858",
        "37": "714cf0f3-0375-4504-b245-e96e115505f0",
        "38": "ce504bef-9595-4fa5-8d26-5a1dab73e50a",
        "39": "c0e99400-326f-41dd-97de-08f791727a6d",
        "40": "6e885266-1097-4636-b55f-a787f0aea2d3",
        "41": "458dd86b-1c4a-43e9-9cea-5523d684187d",
        "42": "8213890b-df3f-4c22-80a9-d29dbe744cf6",
        "43": "09495fe9-4393-4cba-b035-f2c0549078fe",
        "44": "0ee74199-5653-4419-a3a2-a539d72418ab",
        "45": "69bc9a22-3980-4627-8e64-ccec71cf61cb",
        "46": "d41db613-ec01-4196-89fb-f5a2468fb5ea",
        "47": "b1e878e4-e990-4a10-bebd-228637ca1a2c",
        "48": "ff4c1c15-9aab-4006-abea-2b3299e93587",
        "49": "78004275-bf27-487b-a00f-b2104985b4a3",
        "50": "1ecd21e2-c046-4ce4-b3bd-031ba43e84db",
        "51": "c772eeea-1a3f-4be4-ad6d-a0ab8f1c8d08",
        "52": "904cdd9d-2e83-4242-b2de-bdfd7eae531f",
        "53": "697f7dd8-b1b2-47f8-b40f-b62a0ccdda51",
        "54": "bcda2c7a-684f-440e-80e2-9ca0f8bd3783",
        "55": "c92f8d25-172f-4e1d-bcef-86518aed5bf7",
        "56": "2a2ace12-4f2f-400f-9bb6-667cbaa757a3",
        "57": "580A9F49-1A58-4C83-9B67-BE641FDB17E2",
        "58": "31b9cd8e-207c-4e7d-a0e4-1acf7f2c28fa",
        "59": "EC7FAD59-4C19-4961-9716-A2D58262F7BA",
        "60": "69bb3500-f6b1-4bf4-861a-c3d762a71c3e",
        "61": "62fc79a7-dd25-487b-9b91-92636a926816",
        "62": "fe0401c4-27c5-4881-91f2-ff696de3c45d",
        "63": "e99f70c3-64b1-4a3e-93f6-659501822949",
        "64": "8289d44b-0958-43d8-8474-3ef7fecd903c",
        "65": "a03c8762-bfe6-41fb-8229-ce35ed3ff554",
        "66": "f30ac643-34b1-4e32-b7af-bb40c5359203",
        "67": "eb7c0e0b-95c7-480c-a260-b972a02b3692",
        "68": "b0a7e30a-14b6-443d-85b9-806e98a7de78",
        "69": "cbcba9db-63f0-4e30-bee0-7e8a7d99d451",
        "70": "208afa95-53ec-4b3d-b5be-d45b70fa6308",
        "71": "3457bea2-165e-4eee-a628-942ce7720d9b",
        "72": "a78422ea-20fc-4271-9534-fb1b679799c0",
        "73": "e98f065d-32fa-4fb1-b0eb-0db434c96953",
        "74": "96970689-3bbc-4548-b7ab-6467cfb6ca17",
        "75": "8b47fb01-75e8-41c0-ba3d-fac72e6db282",
        "76": "bb9376ac-84d6-407d-83f5-b399c7af7aca",
        "77": "d7317db5-3d86-4ad5-b8dd-c3a651a8226f",
        "78": "c228c28a-f6be-48d9-b69b-fe39b8ca3641",
        "79": "bbd8481a-7785-4124-a2fb-774ecf17c808",
        "80": "0783cf6f-7dc2-48b4-b6e7-18ae660bb455",
        "81": "84dc1ad1-21c0-4337-b841-090ee7c20221",
        "82": "70988c08-fd4d-4b3c-8375-a1d7a98fe533",
        "83": "76f1d60e-f9cd-4bc4-8704-1d5658ebb97e",
        "84": "ecf57f18-33f0-47a6-b0a3-b4361f60d5f8",
        "85": "6689f0fc-b764-433b-9e50-06bd8b8b453b",
        "86": "d08b3658-8fb5-4406-b6ac-50d2f334dc1c",
        "87": "2b1ef370-3ef6-4735-81f9-5b0294abf292",
        "88": "e32b9e18-004e-4ed8-9569-d375eddcd581",
        "89": "70a3d621-75eb-4bd6-aa9c-2207c4f88400",
        "90": "f10f6045-5698-4f09-a37e-c65ac4d58eb2",
        "91": "5190560f-024b-47b0-9384-ad3b8df362d8",
        "92": "a1129b86-197e-4a4b-acf8-be14c0e00cf4",
        "93": "8f1e8e2b-b912-49e3-afed-d7b9b252af8f",
        "94": "18da01a2-f6b9-40dc-8a8e-e6269435bced",
        "95": "857fe5c8-812a-40a4-8567-efa7ad7ff38a",
        "96": "6880492e-2d2d-4138-b86c-3600b52ae473",
        "97": "711d771e-5ace-4aec-b98a-0eda99322dc6",
        "98": "a1555d14-a6f1-4cc3-8acd-cb12d0512665",
        "99": "9439e19e-5866-4e1b-a9be-2e34e0a1a703",
        "100": "23dc7326-767d-4f0f-999f-dea82ad7a915",
        "101": "1c5dc758-ee13-41ad-aa75-40062470ed1d",
        "102": "68da3718-ae27-4ff9-98c5-e1bbdc14f895",
        "103": "c5950c96-f3d7-4ca8-a773-8b5948c83e3c",
        "104": "49ffd139-a175-459d-b90f-7b41c65adb94",
        "105": "1e7d39c0-87b1-4452-ba66-717f8ff45176",
        "106": "d4a8a6ca-a0be-4156-9142-1d22e4c1267f",
        "107": "e2bd454f-2afb-410f-b70b-efd348b1767c",
        "108": "4b870384-6d30-472d-b808-181956fc48f1",
        "109": "f0a43e8b-595b-4a4f-a5e8-aa7cbd4882b3",
        "110": "73d2ec38-9798-4dec-b522-2e026c1433d0",
        "111": "47acfa12-113b-4ccc-bde3-184de122e1ff",
        "112": "ab64b532-c95d-45dc-8705-5d7a699da17b",
        "113": "52846551-e30a-4199-9220-dc1446b67245",
        "114": "42731b08-ab51-4e4c-aa15-e0bed2c33910",
        "115": "6b9d521e-46ca-43ef-9514-3a31f8416dc8",
        "116": "74a58930-b378-43d9-afdb-b8522cd8b333",
        "117": "481a01c0-4925-427e-9e53-3b9c3242f6ae",
        "118": "b5913452-075c-414a-883a-34e96094a9f6",
        "119": "a95c8ebb-2f60-4d64-aa94-ccf917a2c155",
        "120": "fe031888-f603-4bb5-9cd5-0e76c1f16485",
        "121": "e6ce0691-f82c-4ff3-b9ed-e4a779ece7c9",
        "122": "307caee8-8e06-48cb-9671-d0ce016f7ee6",
        "123": "7dc5c32e-aff2-4c3f-8599-fbbc6fa94319",
        "124": "54083b93-167b-4ef0-946c-7893625bf675",
        "125": "eae0e2c8-59ce-497b-b834-7b2deac05f92",
        "126": "b6a09726-1351-464e-9414-2db1628f12b1",
        "127": "d6fe835b-27e9-4b71-b7ee-2b93433b945f",
        "128": "bbc643cc-23f8-4a7e-9078-1b09fece6b74",
        "129": "e4cc6c72-2eff-4d41-9ba1-5799334e54f5",
        "130": "eea4623a-f89b-443a-8a1a-d6347c465c83",
        "131": "bcdf6cd2-4339-4cd2-a383-ca3de75bb214",
        "132": "94e90830-c594-4cbb-b61d-40cec72fab2c",
        "133": "5ed3e87e-a2d6-4328-a427-b1ca48ce4486",
        "134": "64e0d496-de66-4275-ac5f-36ebba94644c",
        "135": "a2f0d269-9034-4c98-9915-ec7efd07d24b",
        "136": "ee37245a-2055-4cd2-b22c-1a6b5c24463d",
        "137": "d9d2a27d-8db1-4766-8039-830447aeb96c",
        "138": "21fd4145-2634-419a-a36b-c7378d7e6d8b",
        "139": "e1ef9548-aff4-4838-a9f0-604fc2535d4d",
        "140": "380a5e6a-12ad-46bc-bb1f-aad074d3e27b",
        "141": "8a0aea5b-33bb-4581-8ded-4f803efd033d",
        "142": "db190a57-036b-4077-b6f2-2de028741729",
        "143": "93b33d60-d527-4da3-8695-5fcdb803897e",
        "144": "229ad36d-aca5-40c1-85ae-8116ed0ad74c",
        "145": "61c10a48-eb07-49fc-a582-74712a4f80e6",
        "146": "24ba7ebb-d8ac-4603-adc0-5a7ca9782738",
        "147": "fe84ce7f-40a5-4d48-95dc-c138db1f592e",
        "148": "ebe72820-36d1-477c-ad1c-4b4d22e45163",
        "149": "e649baa0-b6b2-4c09-8bd7-f8fa0f8fb08f",
        "150": "acbfa94a-f119-4bec-bc33-b10dcf2714c0",
        "151": "629a872e-8cff-43a5-a98d-9eab222432fb",
        "152": "6c3512e3-71bd-451e-b9c4-cb13ca38c2de",
        "153": "2b0e8a42-e918-4dac-9b61-7d19514af89d",
        "154": "a5ef5542-f477-4631-9462-5702cd72d168",
        "155": "f04b425b-7d5c-4cf0-b44d-7558b24df78b",
        "156": "32b22adb-b753-44b8-8105-b0d6e2492f43",
        "157": "5630b2ed-f0b1-4559-a413-f92ca07759b3",
        "158": "d3f52829-2890-40f6-b69e-c05f4a1d31fd",
        "159": "45b4f151-d767-4437-adca-9d1ffd68a6c2",
        "160": "19bae55d-d643-4753-b3a4-bee4add221de",
        "161": "594e88c1-41c1-4aae-993d-08a8593c7a74",
        "162": "1700a674-d8f4-4620-b9a6-2d11fb14f8b0",
        "163": "d15b1d61-80ae-4a38-9eef-5f0a070bdfbb",
        "164": "3653a8b8-7b5d-4fab-826d-630408863fc5",
        "165": "b13eed81-1888-4e39-8091-3b66a84a031b",
        "166": "e90acded-813f-4335-a377-3eff41dcad87",
        "167": "820f9c2d-abfc-4db0-a83a-a92de3820321",
        "168": "43f4e438-9904-463e-bb66-494754684b6b",
        "169": "30bf96aa-5f71-4727-9d50-6959c0415f47",
        "170": "7e40dbfe-b86c-4a32-a3fd-213b5b85aa0e",
        "171": "80b42503-fdb2-4fcd-b265-c31c53d537bc",
        "172": "d250e4ce-2942-4a7e-9295-4c5ff4db7cec",
        "173": "ecf57704-4d99-4a6d-943c-07ad86a04041",
        "174": "284929ee-909f-46f5-b7fb-65f37e2dc813",
        "175": "5d049d18-3914-4477-bc0d-4d605c8204be",
        "176": "b6fe4030-4f87-492e-9e37-334495b40796",
        "177": "f43e1606-5366-46a4-bb17-1ee50ae405fa",
        "178": "318e5d02-7614-4878-93cc-02e5f630e6d9",
        "179": "9083e1a9-464f-4359-81b3-48b04b02c252",
        "180": "bb41b849-36f0-4441-bdac-5590cd2db2be",
        "181": "CFCEA52E-822C-4083-ABBD-9E1084F220F2",
        "182": "dbd2b30d-37cd-42bd-ae46-a2c8242e78c5",
        "183": "8b05def7-a2a0-4a9a-835f-bfc9e2ba4708",
        "184": "a7372412-60fe-4b42-9edc-b30329586de7",
        "185": "be7eb3cb-6501-4cc0-92a0-7b8d0b789f01",
        "186": "a87f9e83-3da8-4736-9ed1-2aa05c547b74",
        "187": "99ca9317-29ba-429a-9164-de623d1b0e03",
        "188": "2b9928e6-3317-4683-9a09-6de5305c8dc0",
        "189": "9c63929a-0e5f-460a-9bbc-3025e6365085",
        "190": "f202223d-57b6-44e4-846b-1ebd1e150f52",
        "191": "9d4df88e-605c-47f0-b7c8-ea4169e018fc",
        "192": "ec01086e-b25b-4cf1-803e-c9a2afb52a14",
        "193": "4fed9c12-b49c-43da-9b7b-d591cf4b5dd0",
        "194": "ca06a579-99cf-4f4d-b7a0-4504b05c9b69",
        "195": "7f6575bd-8885-4e23-91d1-a6587617edd1",
        "196": "5b5d2a29-47b1-418d-9c42-0c9ec08964f7",
        "197": "14eb8272-6cae-4be8-9668-fa89ca1c85e4",
        "198": "cd112947-3a9a-4fe1-9c30-02c0f4768353",
        "199": "affb4d1e-8e64-46ad-ad29-b2aca5f64aa5",
        "200": "8a6b412f-3892-4800-a8eb-edc01c3c235f",
        "201": "e950b798-3bf2-4f0d-9855-7908c5bb279d",
        "202": "7bf2b171-6724-43c5-8513-208f28434e94",
        "203": "b79a4371-790f-49aa-8311-ff1e4d65d85b",
        "204": "bdc90a51-b394-4300-8751-a814560f557b",
        "205": "3aca2fe5-57fa-4192-ad3b-c4e4bf5d2be1",
        "206": "8ad51d19-f486-46b6-b067-55a64427f8da",
        "207": "c26fb573-0aa8-4e06-9896-1cbcb231eba6",
        "208": "11db4ead-6931-4156-a35b-3ec284252008",
        "209": "67f66bb3-f21c-43df-a217-56705af47e3e",
        "210": "f0ad57e3-33cf-4a18-a3cc-0b0d62f92d89",
        "211": "26820db8-1396-439a-b347-39fa9cb8fef5",
        "212": "a735c1e8-998a-4c53-a6af-0a1b6c2211d5",
        "213": "9546f60b-05d1-477e-b6db-b5415470972b",
        "214": "9d1176c3-7a88-48f9-a8fa-059e01ff19f6",
        "215": "33468512-7d85-483c-b37e-e790cc25f7b5",
        "216": "d663666c-5571-4dc1-a6b3-127892091d38",
        "217": "744e7eb6-a07a-4058-80a7-83fd9b2410d1",
        "218": "2985D7EC-D6E5-483C-861E-EEB8A4E95493",
        "219": "1a31bf04-d3a9-47a6-bd13-ac2c555b26d5",
        "220": "ce630459-afff-41f7-bac8-c371bbfb52d2",
        "221": "4eefae50-82a7-4870-a256-a1fe72039901",
        "222": "c4b2066a-cb33-40fd-b13d-ab19a01553dd",
        "223": "0a659275-7f91-4630-8a41-aeffd61e6b1f",
        "224": "119452f4-b0ef-42eb-bc47-e5d79a6e2c2b",
        "225": "cf79d5aa-fbd4-4d0a-84e8-c0c226484730",
        "226": "3fdcdc34-147d-41ec-b1a3-81e5dc8548e1",
        "227": "5bf6d13f-d004-4511-ab3a-e37c4e637092",
        "228": "aa907a82-8c64-480d-9a53-60603b31ccd4",
        "229": "56cf3cbd-4ad5-422e-8c96-35918a302606",
        "230": "35e86338-da43-457e-a6e6-8b79aa8d6983",
        "231": "fa028d98-f0fb-476e-92de-7bfe3a48a023",
        "232": "48a432db-d13b-4d2e-86b2-de51adfc5438",
        "233": "266d9017-c862-4c34-b565-bf1033021167",
        "234": "f4f9b620-f97a-4a62-bb41-8b6b605487db",
        "235": "9c6dd30b-6622-48bb-8a91-c58483279166",
        "236": "3B4B6BB7-7EBC-47E0-8E65-A5EB9991632B",
        "237": "5f9eac12-6d14-4b4f-82bd-a8057c2ed493",
        "238": "85428e31-a192-4fac-b370-45feadaf1e22",
        "239": "a7e8fe58-c4df-4e21-8285-330ebfe5b177",
        "240": "4b69ff91-7446-412d-9612-c1556cbec8f1",
        "241": "7e1c01ab-b171-4147-96f5-eabfdbccd47d",
        "242": "a264294d-3bf3-4c22-b135-6a6bcba9ee97",
        "243": "893bc0f3-410b-43e7-af20-ecd287d4e4a0",
        "244": "c7b5784c-9614-4f4f-b02a-1921eb576777",
        "245": "8ffed639-2ea6-441a-9e56-53e2bc8fa972",
        "246": "3615eebe-fd32-43c3-b572-a16ce7f5e159",
        "247": "0db19911-51c2-430e-9b1f-4430fc8ac2eb",
        "248": "2bbc307e-171a-4314-9969-b39ec65a3df5",
        "249": "24343dea-9749-4aa4-956c-0056b2d07cbd"
    },
    "created_at": {
        "0": "2024-04-11T07:49:45.714149",
        "1": "2024-04-11T08:22:09.374109",
        "2": "2024-04-11T08:59:32.370529",
        "3": "2024-04-11T09:02:28.000959",
        "4": "2024-04-12T08:04:20.532329",
        "5": "2024-04-12T08:10:01.657379",
        "6": "2024-04-12T08:12:39.405869",
        "7": "2024-04-12T14:38:46.243249",
        "8": "2024-04-12T14:40:26.467979",
        "9": "2024-04-12T14:41:18.957279",
        "10": "2024-04-12T14:46:06.414419",
        "11": "2024-04-12T14:59:36.708369",
        "12": "2024-04-12T15:32:48.240509",
        "13": "2024-04-13T12:09:07.884739",
        "14": "2024-04-13T12:11:20.192959",
        "15": "2024-04-15T07:01:29.339909",
        "16": "2024-04-15T09:26:05.217729",
        "17": "2024-04-15T09:27:40.447249",
        "18": "2024-04-15T13:12:48.641699",
        "19": "2024-04-15T13:35:59.968019",
        "20": "2024-04-16T08:02:54.157819",
        "21": "2024-04-16T09:11:26.191759",
        "22": "2024-04-16T09:41:31.111939",
        "23": "2024-04-16T09:47:18.636679",
        "24": "2024-04-16T10:09:10.877429",
        "25": "2024-04-16T10:19:34.713109",
        "26": "2024-04-16T10:20:25.675849",
        "27": "2024-04-16T10:40:57.626029",
        "28": "2024-04-16T10:43:29.648159",
        "29": "2024-04-16T10:47:45.387509",
        "30": "2024-04-16T10:48:55.201849",
        "31": "2024-04-16T10:52:05.015509",
        "32": "2024-04-16T10:54:12.118249",
        "33": "2024-04-16T10:56:25.440069",
        "34": "2024-04-16T11:01:03.760249",
        "35": "2024-04-16T11:01:47.534799",
        "36": "2024-04-16T11:02:18.019999",
        "37": "2024-04-16T11:03:58.125319",
        "38": "2024-04-16T11:06:30.380979",
        "39": "2024-04-16T11:11:54.911299",
        "40": "2024-04-16T11:13:21.184049",
        "41": "2024-04-17T00:30:45.630059",
        "42": "2024-04-17T07:08:59.424749",
        "43": "2024-04-17T08:33:16.978639",
        "44": "2024-04-16T11:21:30.409439",
        "45": "2024-04-16T11:22:55.995339",
        "46": "2024-04-16T23:47:44.281029",
        "47": "2024-04-17T09:19:24.738599",
        "48": "2024-04-17T11:04:09.138889",
        "49": "2024-04-17T11:09:15.296539",
        "50": "2024-04-17T15:23:31.402539",
        "51": "2024-04-17T11:06:58.317489",
        "52": "2024-04-18T09:52:18.987509",
        "53": "2024-04-18T09:53:02.605509",
        "54": "2024-04-18T09:55:13.016279",
        "55": "2024-04-18T09:56:56.290409",
        "56": "2024-04-19T09:08:52.248739",
        "57": "2024-04-19T10:14:33.341259",
        "58": "2024-04-19T10:28:50.735599",
        "59": "2024-04-19T10:32:20.360249",
        "60": "2024-04-19T10:38:38.162269",
        "61": "2024-04-22T06:25:03.537959",
        "62": "2024-04-22T09:06:23.825079",
        "63": "2024-04-22T09:27:36.799779",
        "64": "2024-04-22T10:02:39.963909",
        "65": "2024-04-22T11:35:35.997299",
        "66": "2024-04-22T23:13:35.710259",
        "67": "2024-04-23T06:33:04.000889",
        "68": "2024-04-23T06:37:56.901869",
        "69": "2024-04-23T06:38:41.687849",
        "70": "2024-04-23T06:39:32.411959",
        "71": "2024-04-23T06:39:36.022959",
        "72": "2024-04-23T15:42:26.237879",
        "73": "2024-04-23T15:53:16.577059",
        "74": "2024-04-23T15:54:38.516049",
        "75": "2024-04-23T22:13:46.641809",
        "76": "2024-04-24T00:30:36.357919",
        "77": "2024-04-24T02:22:36.340649",
        "78": "2024-04-24T06:36:59.963899",
        "79": "2024-04-24T09:49:27.695939",
        "80": "2024-04-24T10:03:08.031849",
        "81": "2024-04-24T10:11:23.955809",
        "82": "2024-04-24T10:13:26.701209",
        "83": "2024-04-24T10:34:43.181839",
        "84": "2024-04-25T05:50:08.479849",
        "85": "2024-04-25T06:02:49.175689",
        "86": "2024-04-25T07:50:33.416349",
        "87": "2024-04-25T07:55:58.551269",
        "88": "2024-04-25T08:05:22.159059",
        "89": "2024-04-25T08:12:49.880479",
        "90": "2024-04-25T08:14:26.658559",
        "91": "2024-04-25T08:21:47.643379",
        "92": "2024-04-25T08:23:55.012369",
        "93": "2024-04-25T08:24:13.875399",
        "94": "2024-04-25T08:24:31.350569",
        "95": "2024-04-25T08:25:36.666619",
        "96": "2024-04-25T08:26:36.633589",
        "97": "2024-04-25T08:30:33.798969",
        "98": "2024-04-26T06:04:19.924139",
        "99": "2024-04-26T06:32:41.262449",
        "100": "2024-04-26T06:33:18.803729",
        "101": "2024-04-26T07:05:32.364109",
        "102": "2024-04-26T07:07:15.235459",
        "103": "2024-04-26T07:10:49.142579",
        "104": "2024-04-26T07:10:55.963789",
        "105": "2024-04-26T07:12:02.652669",
        "106": "2024-04-26T07:47:18.724549",
        "107": "2024-04-26T08:26:52.669769",
        "108": "2024-04-26T08:43:37.674389",
        "109": "2024-04-26T08:47:30.393729",
        "110": "2024-04-26T09:48:56.153429",
        "111": "2024-04-29T07:06:29.547429",
        "112": "2024-04-29T07:14:00.666989",
        "113": "2024-04-29T07:15:03.441909",
        "114": "2024-04-29T07:17:31.733599",
        "115": "2024-04-29T07:18:50.181249",
        "116": "2024-04-29T07:20:23.246399",
        "117": "2024-04-29T08:04:55.740559",
        "118": "2024-04-29T08:08:54.358089",
        "119": "2024-04-29T08:25:14.105749",
        "120": "2024-04-29T09:07:36.906549",
        "121": "2024-04-29T10:40:45.641519",
        "122": "2024-04-30T00:37:02.137289",
        "123": "2024-04-30T07:06:12.799789",
        "124": "2024-04-30T07:07:37.584569",
        "125": "2024-04-30T07:08:12.106119",
        "126": "2024-04-26T13:02:49.445479",
        "127": "2024-04-29T08:20:29.796729",
        "128": "2024-04-29T21:22:52.785409",
        "129": "2024-04-29T23:24:53.283749",
        "130": "2024-04-30T05:39:50.661929",
        "131": "2024-04-30T07:14:56.035879",
        "132": "2024-04-30T09:41:32.624869",
        "133": "2024-04-30T09:48:12.281509",
        "134": "2024-05-01T08:13:35.851159",
        "135": "2024-05-01T08:30:17.603829",
        "136": "2024-05-01T08:48:54.763169",
        "137": "2024-05-01T08:59:32.587779",
        "138": "2024-05-01T09:10:54.233649",
        "139": "2024-05-01T09:27:00.036189",
        "140": "2024-05-01T09:39:31.401199",
        "141": "2024-05-01T09:55:14.490259",
        "142": "2024-05-01T10:20:25.627029",
        "143": "2024-05-01T10:23:44.879729",
        "144": "2024-05-01T10:27:16.631849",
        "145": "2024-05-01T10:28:12.080919",
        "146": "2024-05-01T09:39:58.806459",
        "147": "2024-05-02T05:51:00.630679",
        "148": "2024-05-02T06:12:29.664209",
        "149": "2024-05-02T07:47:33.742409",
        "150": "2024-05-02T08:33:22.351429",
        "151": "2024-05-02T08:38:40.651229",
        "152": "2024-05-02T10:30:18.252329",
        "153": "2024-05-02T11:24:54.496919",
        "154": "2024-05-02T11:58:36.192009",
        "155": "2024-05-02T12:23:06.459059",
        "156": "2024-05-02T12:25:33.030559",
        "157": "2024-05-03T03:24:09.331659",
        "158": "2024-05-03T07:12:27.041879",
        "159": "2024-05-03T07:22:14.253869",
        "160": "2024-05-03T07:29:47.827779",
        "161": "2024-05-03T08:15:23.383889",
        "162": "2024-05-03T08:26:56.348219",
        "163": "2024-05-05T20:09:14.135919",
        "164": "2024-05-06T02:16:13.741809",
        "165": "2024-05-06T07:41:37.207179",
        "166": "2024-05-06T07:48:00.359529",
        "167": "2024-05-06T08:04:11.830199",
        "168": "2024-05-06T08:30:03.145719",
        "169": "2024-05-06T08:46:51.006869",
        "170": "2024-05-06T08:59:01.243629",
        "171": "2024-05-06T10:58:34.610559",
        "172": "2024-05-06T15:36:32.586469",
        "173": "2024-05-02T08:35:51.204919",
        "174": "2024-05-02T11:39:33.206719",
        "175": "2024-05-06T07:15:18.229139",
        "176": "2024-05-06T15:05:41.365449",
        "177": "2024-05-06T15:06:02.817439",
        "178": "2024-05-06T07:30:50.971839",
        "179": "2024-05-06T07:43:54.904189",
        "180": "2024-05-06T23:09:22.989419",
        "181": "2024-05-06T23:13:13.156389",
        "182": "2024-05-07T09:18:55.578589",
        "183": "2024-05-07T16:34:18.827799",
        "184": "2024-05-07T16:37:50.611059",
        "185": "2024-05-07T16:38:54.650189",
        "186": "2024-05-07T16:40:09.106469",
        "187": "2024-05-07T16:41:12.108819",
        "188": "2024-05-07T16:45:19.598249",
        "189": "2024-05-08T09:56:24.586699",
        "190": "2024-05-08T10:01:05.297359",
        "191": "2024-05-08T10:01:39.202749",
        "192": "2024-05-08T10:02:59.948029",
        "193": "2024-05-07T18:48:27.747449",
        "194": "2024-05-08T02:49:06.457599",
        "195": "2024-05-08T07:06:15.779359",
        "196": "2024-05-08T07:44:45.688329",
        "197": "2024-05-08T07:03:57.371389",
        "198": "2024-05-08T07:09:41.486189",
        "199": "2024-05-08T07:19:05.155009",
        "200": "2024-05-08T07:23:15.498439",
        "201": "2024-05-08T07:35:27.670999",
        "202": "2024-05-08T07:39:00.502559",
        "203": "2024-05-08T08:00:41.041189",
        "204": "2024-05-08T08:02:04.796669",
        "205": "2024-05-08T15:12:58.446579",
        "206": "2024-05-08T17:45:41.086369",
        "207": "2024-05-09T01:06:22.770869",
        "208": "2024-05-09T04:03:28.758539",
        "209": "2024-05-09T22:09:22.825309",
        "210": "2024-05-09T23:48:32.017299",
        "211": "2024-05-10T00:14:43.252289",
        "212": "2024-05-10T07:03:49.833029",
        "213": "2024-05-10T07:04:09.046529",
        "214": "2024-05-10T07:07:29.677529",
        "215": "2024-05-10T07:12:27.456819",
        "216": "2024-05-10T09:41:15.393119",
        "217": "2024-05-12T22:59:48.954979",
        "218": "2024-05-13T00:41:28.091729",
        "219": "2024-05-13T10:08:11.422419",
        "220": "2024-05-14T07:00:54.688469",
        "221": "2024-05-14T08:31:04.677789",
        "222": "2024-05-14T09:05:08.365039",
        "223": "2024-05-14T09:23:27.165989",
        "224": "2024-05-14T14:17:31.149069",
        "225": "2024-05-15T07:00:16.824089",
        "226": "2024-05-15T07:34:30.328739",
        "227": "2024-05-15T08:29:27.317489",
        "228": "2024-05-15T08:40:16.218219",
        "229": "2024-05-15T08:40:51.995059",
        "230": "2024-05-15T08:41:11.246639",
        "231": "2024-05-15T08:45:31.590919",
        "232": "2024-05-15T08:51:40.522889",
        "233": "2024-05-15T10:29:32.707049",
        "234": "2024-05-15T10:51:46.677609",
        "235": "2024-05-15T08:08:07.717999",
        "236": "2024-05-15T08:09:53.932229",
        "237": "2024-05-15T08:08:50.259319",
        "238": "2024-05-15T08:13:17.720589",
        "239": "2024-05-15T13:23:54.213349",
        "240": "2024-05-15T13:36:51.625709",
        "241": "2024-05-16T07:09:12.380969",
        "242": "2024-05-16T07:00:12.860229",
        "243": "2024-05-16T07:09:19.402009",
        "244": "2024-05-16T07:30:11.782829",
        "245": "2024-05-16T07:33:00.645309",
        "246": "2024-05-16T11:24:58.207359",
        "247": "2024-05-16T11:27:03.332109",
        "248": "2024-05-16T11:30:49.711009",
        "249": "2024-05-16T12:26:44.536339"
    },
    "creator_id": {
        "0": "UU43NJY8K",
        "1": "U066Q9JAU3B",
        "2": "UU43NJY8K",
        "3": "U066Q9JAU3B",
        "4": "UU43NJY8K",
        "5": "U047WSYBE5T",
        "6": "U066Q9JAU3B",
        "7": "U066Q9JAU3B",
        "8": "U066Q9JAU3B",
        "9": "U066Q9JAU3B",
        "10": "U066Q9JAU3B",
        "11": "U066Q9JAU3B",
        "12": "U066Q9JAU3B",
        "13": "U066Q9JAU3B",
        "14": "U05B5FXLP62",
        "15": "U047WSYBE5T",
        "16": "U066Q9JAU3B",
        "17": "U066Q9JAU3B",
        "18": "U047WSYBE5T",
        "19": "U066Q9JAU3B",
        "20": "U06RU2JKMDG",
        "21": "U066Q9JAU3B",
        "22": "U06RU2JKMDG",
        "23": "U05B5FXLP62",
        "24": "U066Q9JAU3B",
        "25": "U066Q9JAU3B",
        "26": "U066Q9JAU3B",
        "27": "U05B5FXLP62",
        "28": "U066Q9JAU3B",
        "29": "U05B5FXLP62",
        "30": "U05B5FXLP62",
        "31": "U066Q9JAU3B",
        "32": "U066Q9JAU3B",
        "33": "U066Q9JAU3B",
        "34": "U05B5FXLP62",
        "35": "U05B5FXLP62",
        "36": "U05B5FXLP62",
        "37": "U066Q9JAU3B",
        "38": "U05B5FXLP62",
        "39": "U066Q9JAU3B",
        "40": "U05B5FXLP62",
        "41": "U06RU2JKMDG",
        "42": "U066Q9JAU3B",
        "43": "U06RU2JKMDG",
        "44": "U047WSYBE5T",
        "45": "U066Q9JAU3B",
        "46": "U06855K24SE",
        "47": "U066Q9JAU3B",
        "48": "U066Q9JAU3B",
        "49": "U05B5FXLP62",
        "50": "U066Q9JAU3B",
        "51": "U066Q9JAU3B",
        "52": "U066Q9JAU3B",
        "53": "U05B5FXLP62",
        "54": "U066Q9JAU3B",
        "55": "U05B5FXLP62",
        "56": "U066Q9JAU3B",
        "57": "U047WSYBE5T",
        "58": "U066Q9JAU3B",
        "59": "U047WSYBE5T",
        "60": "U066Q9JAU3B",
        "61": "U047WSYBE5T",
        "62": "U066Q9JAU3B",
        "63": "U066Q9JAU3B",
        "64": "U066Q9JAU3B",
        "65": "U047WSYBE5T",
        "66": "U06855K24SE",
        "67": "U06RU2JKMDG",
        "68": "U06855K24SE",
        "69": "U047WSYBE5T",
        "70": "U047WSYBE5T",
        "71": "U06855K24SE",
        "72": "U066Q9JAU3B",
        "73": "U066Q9JAU3B",
        "74": "U066Q9JAU3B",
        "75": "U047WSYBE5T",
        "76": "U066Q9JAU3B",
        "77": "U047WSYBE5T",
        "78": "U047WSYBE5T",
        "79": "U047WSYBE5T",
        "80": "U066Q9JAU3B",
        "81": "U066Q9JAU3B",
        "82": "U066Q9JAU3B",
        "83": "U066Q9JAU3B",
        "84": "U06RU2JKMDG",
        "85": "U047WSYBE5T",
        "86": "U06RU2JKMDG",
        "87": "U066Q9JAU3B",
        "88": "U06RU2JKMDG",
        "89": "U06RU2JKMDG",
        "90": "U047WSYBE5T",
        "91": "U066Q9JAU3B",
        "92": "U047WSYBE5T",
        "93": "U066Q9JAU3B",
        "94": "U066Q9JAU3B",
        "95": "U047WSYBE5T",
        "96": "U06RU2JKMDG",
        "97": "U066Q9JAU3B",
        "98": "U047WSYBE5T",
        "99": "U047WSYBE5T",
        "100": "U047WSYBE5T",
        "101": "U05B5FXLP62",
        "102": "U066Q9JAU3B",
        "103": "U05B5FXLP62",
        "104": "U05B5FXLP62",
        "105": "U05B5FXLP62",
        "106": "U06RU2JKMDG",
        "107": "U066Q9JAU3B",
        "108": "U06RU2JKMDG",
        "109": "U066Q9JAU3B",
        "110": "U06RU2JKMDG",
        "111": "U066Q9JAU3B",
        "112": "U06RU2JKMDG",
        "113": "U06RU2JKMDG",
        "114": "U066Q9JAU3B",
        "115": "U06RU2JKMDG",
        "116": "U066Q9JAU3B",
        "117": "U06RU2JKMDG",
        "118": "U06RU2JKMDG",
        "119": "U066Q9JAU3B",
        "120": "U06RU2JKMDG",
        "121": "U066Q9JAU3B",
        "122": "U06RU2JKMDG",
        "123": "U066Q9JAU3B",
        "124": "U05B5FXLP62",
        "125": "U066Q9JAU3B",
        "126": "U047WSYBE5T",
        "127": "U047WSYBE5T",
        "128": "U06855K24SE",
        "129": "U06RU2JKMDG",
        "130": "U047WSYBE5T",
        "131": "U066Q9JAU3B",
        "132": "U066Q9JAU3B",
        "133": "U047WSYBE5T",
        "134": "U06RU2JKMDG",
        "135": "U066Q9JAU3B",
        "136": "U066Q9JAU3B",
        "137": "U066Q9JAU3B",
        "138": "U066Q9JAU3B",
        "139": "U06RU2JKMDG",
        "140": "U066Q9JAU3B",
        "141": "U06RU2JKMDG",
        "142": "U066Q9JAU3B",
        "143": "U066Q9JAU3B",
        "144": "U06RU2JKMDG",
        "145": "U066Q9JAU3B",
        "146": "U066Q9JAU3B",
        "147": "U06RU2JKMDG",
        "148": "U066Q9JAU3B",
        "149": "U06855K24SE",
        "150": "U066Q9JAU3B",
        "151": "U047WSYBE5T",
        "152": "U066Q9JAU3B",
        "153": "U066Q9JAU3B",
        "154": "U066Q9JAU3B",
        "155": "U06RU2JKMDG",
        "156": "U066Q9JAU3B",
        "157": "U06RU2JKMDG",
        "158": "U066Q9JAU3B",
        "159": "U06RU2JKMDG",
        "160": "U066Q9JAU3B",
        "161": "U06RU2JKMDG",
        "162": "U066Q9JAU3B",
        "163": "U066Q9JAU3B",
        "164": "U06RU2JKMDG",
        "165": "U066Q9JAU3B",
        "166": "U066Q9JAU3B",
        "167": "U06RU2JKMDG",
        "168": "U066Q9JAU3B",
        "169": "U06RU2JKMDG",
        "170": "U066Q9JAU3B",
        "171": "U06RU2JKMDG",
        "172": "U066Q9JAU3B",
        "173": "UU43NJY8K",
        "174": "U047WSYBE5T",
        "175": "UU43NJY8K",
        "176": "U066Q9JAU3B",
        "177": "U066Q9JAU3B",
        "178": "U05R0KHPU8N",
        "179": "U066Q9JAU3B",
        "180": "U066Q9JAU3B",
        "181": "U05R0KHPU8N",
        "182": "U047WSYBE5T",
        "183": "U066Q9JAU3B",
        "184": "U066Q9JAU3B",
        "185": "U066Q9JAU3B",
        "186": "U066Q9JAU3B",
        "187": "U066Q9JAU3B",
        "188": "U066Q9JAU3B",
        "189": "U05R0KHPU8N",
        "190": "U05R0KHPU8N",
        "191": "U066Q9JAU3B",
        "192": "U05R0KHPU8N",
        "193": "U066Q9JAU3B",
        "194": "U05R0KHPU8N",
        "195": "U066Q9JAU3B",
        "196": "U05R0KHPU8N",
        "197": "U047WSYBE5T",
        "198": "U066Q9JAU3B",
        "199": "U06RU2JKMDG",
        "200": "U066Q9JAU3B",
        "201": "U06RU2JKMDG",
        "202": "U05R0KHPU8N",
        "203": "U066Q9JAU3B",
        "204": "U06RU2JKMDG",
        "205": "U066Q9JAU3B",
        "206": "U066Q9JAU3B",
        "207": "U05R0KHPU8N",
        "208": "U047WSYBE5T",
        "209": "U066Q9JAU3B",
        "210": "U047WSYBE5T",
        "211": "U05R0KHPU8N",
        "212": "U066Q9JAU3B",
        "213": "U05R0KHPU8N",
        "214": "U066Q9JAU3B",
        "215": "U066Q9JAU3B",
        "216": "U066Q9JAU3B",
        "217": "U066Q9JAU3B",
        "218": "U05R0KHPU8N",
        "219": "U066Q9JAU3B",
        "220": "U05R0KHPU8N",
        "221": "U047WSYBE5T",
        "222": "U066Q9JAU3B",
        "223": "U047WSYBE5T",
        "224": "U066Q9JAU3B",
        "225": "USLACKBOT",
        "226": "U05R0KHPU8N",
        "227": "U066Q9JAU3B",
        "228": "U066Q9JAU3B",
        "229": "U05R0KHPU8N",
        "230": "U05R0KHPU8N",
        "231": "U066Q9JAU3B",
        "232": "U05R0KHPU8N",
        "233": "U066Q9JAU3B",
        "234": "U05R0KHPU8N",
        "235": "U066Q9JAU3B",
        "236": "U05R0KHPU8N",
        "237": "U066Q9JAU3B",
        "238": "U05R0KHPU8N",
        "239": "U05R0KHPU8N",
        "240": "U066Q9JAU3B",
        "241": "U05R0KHPU8N",
        "242": "USLACKBOT",
        "243": "U05R0KHPU8N",
        "244": "U066Q9JAU3B",
        "245": "U05R0KHPU8N",
        "246": "U066Q9JAU3B",
        "247": "U05R0KHPU8N",
        "248": "U066Q9JAU3B",
        "249": "U05R0KHPU8N"
    },
    "content": {
        "0": "Hi <@U066Q9JAU3B>\nNice to see you at our project channel :tada:\nThe other members of our team will be added asap after our kick-off meeting.",
        "1": "Hi <@UU43NJY8K>,\nThanks for inviting us.",
        "2": "Hi <@U066Q9JAU3B> <@U06855K24SE>\nDon't you mind if we shift our kick-off meeting 30 minutes tomorrow?\nFor 7:30 am your time please?",
        "3": "Works for me.",
        "4": "Hi Yogesh,\nPlease do not hesitate to share all the information here.\nI am adding all the team's members to the channel -\n<@U05B5FXLP62> <@U06RU2JKMDG> <@U047WSYBE5T>",
        "5": "<!here> Hi team :spock-hand: I'm thrilled to be joining you all! Looking forward to launching work together.",
        "6": "Thank you for your flexibility to get on a call on a Friday evening. I lived many years in Europe and I know that this is a big deal!",
        "7": "<@U05B5FXLP62> <@U06RU2JKMDG> Here is the dataset for the project",
        "8": "the zip folder contains several csv files of different length (some contain few hundred messages, while the others contain more than hundred thousand messages.). To begin with we can start with smaller files and then move to bigger files.",
        "9": "Here are few files from the folder that I think is a good mix. I just gave them a cursory look. Please feel free to choose your own csv files (and let me know why you chose them).",
        "10": "here is the link where I got the data from - <https:\/\/figshare.com\/s\/3fd5af0b869b8fd010bb> . We are not 'training' a new model using this data, so we should have theoretically no licensing issues.",
        "11": "this dataset is an export from conversations on gitter, a chat application similar to Slack.\n<https:\/\/gitter.im\/>",
        "12": "Here are the project requirement and Q&amp;A docs for reference. I believe you already have it, but it's good to keep them here.",
        "13": "<@U05B5FXLP62> I was referring to this article. <https:\/\/freedium.cfd\/https:\/\/generativeai.pub\/advanced-rag-retrieval-strategies-auto-merging-retrieval-dc3f869654c4>\nImagine a query like \"Give me the status of project xyz\". Now there can be multiple 'topics\/conversations'  'related' to project xyz. If we provide those topics as context, then the query response can be quite accurate (Vs providing all messages in all channels as context)",
        "14": "I'll check",
        "15": "Hi <@U066Q9JAU3B>, I'm reaching out to request access to Cloud Sys. It would be ideal if you could choose RunPod service as the Cloud Sys for our project. If possible, please create an account there and add <@U06RU2JKMDG> (<mailto:maksym.lysyi@litslink.com|maksym.lysyi@litslink.com>) to the team, granting him admin permissions.\nPlease let me know if you have any questions.",
        "16": "<@U047WSYBE5T> <@U06RU2JKMDG> How are we going to fund the GPUs?",
        "17": "<@U06RU2JKMDG> you can join my team at RunPod using this link\n```<https:\/\/www.runpod.io\/console\/user\/invite\/clv163u5c0000mi08a61ecuf1>```",
        "18": "<@U066Q9JAU3B>, we recommend registering on <http:\/\/runpod.io|runpod.io>, making a deposit (we suggest $30 should be sufficient), and adding Maksym to the team.",
        "19": "<@U047WSYBE5T> Done. Added $30 and invited <@U06RU2JKMDG>",
        "20": "Hello <@U066Q9JAU3B>, I want to share with you the current results of an ongoing project and receive your feedback.\n\nAs an example, I attempted to hierarchically cluster messages from the cdnjs.csv file.\n\nBelow, I've attached all the demonstration files:\n\n-tree_hierarchy.txt: This file contains the hierarchy tree derived from messages contained in cdnjs.csv.\n-Screenshots: These show examples of messages found in the leaves of that hierarchy tree (topics).\n-message2topic.csv with messages and coresponding brief topics.\n\nDo you have any suggestions regarding the quality of these results?\nIs there anything you'd like to adjust?",
        "21": "<@U06RU2JKMDG> I am not sure if we are on the same page regarding the project. I see that every single message has a different topic. That is not the expected output at all. We are trying to identify which messages are part of a conversation and group them together in topic.\nHave you review this <@U05B5FXLP62>?",
        "22": "Hello <@U066Q9JAU3B>, those topics for each message are just an intermediate form for computing actual topics.\nIt would be more appropriate to call them 'message descriptors'. You can see the actual topics in the screenshots.\nEach topic has its own unique ID (for example, ID=113 on the first screenshot corresponds to topic #113).\nTopic #113 has a branch of messages with 'message descriptors' (I named it 'context').",
        "23": "Hi there!\nYes, how Maksym said, that are intermediate results for demonstrating and getting feedbacks",
        "24": "hmm.. I am trying to wrap my head around this.\nOk, so what you are saying is\n\u2022 First you generated message descriptors\n\u2022 Fed these message descriptors as context\n\u2022 Generated topic Ids.\nFair enough.\n\nCould you please share a screenshot that has multiple topic ids (input=messages, output=multiple topics). Otherwise, it is hard for me to evaluate how well this approach is working.\n\nFor example, in one of your below screenshots, I'd imagine message # 3789 should belong to a different topic id.\n\nWhat do you think?",
        "25": "I wonder if some context is lost in this approach :thinking_face:\nFor example, in the below screenshot, there is a question and a statement. These indicate that they are part of an ongoing conversation.\n\n If we extract 'performance issue' as intermediate message descriptor,  and feed only that descriptor to the model for the next step, will some signal be lost? Not if you feed both the descriptor and the original message, I guess.",
        "26": "Please let me know if I am unable to understand the approach or explain myself properly. May be getting on a quick call might help.",
        "27": "What do you mean about \"lost signal\"?",
        "28": "Whenever we compress something, we lose signal if it is a lossy compression.\n\nFor example:\n\n\u2022 Weather is nice today\n\u2022 Weathering the storm is key to persevarance.\nIf the message descriptor for above 2 statements is 'weather', we have lost the some signal. It is hard to tell just from the descriptor if these 2 statements should be a part of same conversation or different.",
        "29": "Gotcha.\nBut what topic did you assign messages upper? (where is performance issues)",
        "30": "And also do you mean that if we have dialog:\n\n\u2022 Hello! Who knows Python?\n\u2022 I am\nSo these messages needs to be in one topic?",
        "31": "yeah, if there is an ongoing dialog, they need to be in  one topic *if and only if* the dialog doesn't move away to some other topic.\n\nFor example, right now we are having a dialog about evaluation of the first results from the model.\n\nNow if I ask you about the new hollywood movie, obviously we have changed to new topic.",
        "32": "Can you do me a favor?\n\nPlease send me a csv file that has about 100-200 messages. I want to see how does the model performs and splits these messages into *different* topics.\n\nIn the attached screenshot, I see all the messages have same topic id. So I can't tell if the model is capable of clustering messages into different topics or not.",
        "33": "May be it will be easier if we get on a quick call?",
        "34": "Okay, <@U06RU2JKMDG> will prepare this file tomorrow",
        "35": "It'll be with current result.",
        "36": "If you wanna, we can do meeting tomorrow",
        "37": "ok. sounds good. Just to be clear:\n\ninput = 200 messages\noutput = messages clustered into multiple topics\n\nIf possible, give me a live demo.\n\nChange the input on the fly, and show me generated output.",
        "38": "So this is not a finished system. We're currently working on the topic assignment part and decided to drop what it looks like right now to keep you up to date.\nOK, we can prepare a file with 200 messages that will be labeled by the current implementation",
        "39": "Sure, I understand. Not expecting a perfect system at this stage.\nIt's only that I am unable to evaluate the current performance with multiple screenshots. I need a single input\/output mapping .",
        "40": "Alright, we'll prepare bigger file for comparing",
        "41": "Hello, I'm sharing with you a CSV file with 4000+ messages that is grouped into clusters by ID. The ID = -1 represents outlier messages.",
        "42": "Thanks. <@U06855K24SE> Can you please provide your feedback too on this CSV file?",
        "43": "",
        "44": "Hi <@U066Q9JAU3B>, I'd like to schedule a meeting for tomorrow to discuss our progress.\nCould you let me know if that works for you and what time would be convenient?",
        "45": "8:30 am PST would be perfect.",
        "46": "I will be unable to join at that time, but will catch up with Yogesh offline.",
        "47": "Good call today. <@U05B5FXLP62> <@U06RU2JKMDG> <@U047WSYBE5T> <@U06855K24SE> We discussed\n\u2022 possibly using user_id as a feature in clustering algorithm in future\n\u2022 possibly using relationship between messages as a feature in clustering algorithm\n\u2022 Possibly using 'next sentence prediction' in clustering algorithm\n\u2022 message_descriptor is a good idea, we should continue using this but probably combine with other approaches for more accurate results\n\u2022 downstream application: summarization (individual topic clusters and possibly combining multiple topics in one big topic)\n\u2022 downstream application: recommendation engine or personalization, filtering. (e.g. in a channel with people from multiple functions, sales people may not be interested in production issues in mongo db cluster, whereas software engineers may not be interested in product pricing topics.)",
        "48": "<@U05B5FXLP62> you also talked about 'mentions' which is an important feature for recommendation, though I don't fully understand how can it help with clustering. May be it could, if the mentioned user replies.",
        "49": "Nope, I mean only next scope with recommendation system",
        "50": "Hi <@U06RU2JKMDG> <@U05B5FXLP62> I did some experiments with ChatGPT3.5 turbo on OpenAI playground. Here I am trying to generate a message descriptor and confidence score of a message &lt;in relationship with a few preceding messages&gt;. This ensures that context is not lost (as opposed to processing a message in isolation.)\n\nSee the prompt and 2 different inputs.\nWe can imagine to have a sliding window of messages while calculating message_descriptor and confidence score if the message is related to previous messages at all.\n\n```\nSYSTEM\n\nyou are an expert linguist.\nuser will give you  a sequence of messages in the following format.\n- message 1\n- message 2\n- message 3\n- message 4\n- message 5\n- message 6\nYour task is to figure out how is the last message is related to all the preceding messages. It is possible that the last message is part of the conversation. It is also possible that it is completely unrelated.\n\nyou have to use all these messages as context  and create a confidence score  quantifying the relationship to the other messages.\n\nYou must also\ncreate a descriptor for the only last message in the sequence. The descriptor should describe what the message is about.\n\nPrint the output in the following JSON format.\n\n```\n{\n\"original message\": \"last message\",\n\"message_descriptor\": \"descriptor:,\n\"confidence_score\": score\n}\n\n\nUSER\n\n- Do you know how to make an enterprise SaaS app SoC 2 compliant?\n- No, I don't. \n- But perhaps, it is a series of steps we can take to ensure that customers data is protected at all times.\n- For example, make sure the access to backend is through https. This ensures data is encrypted in flight.\n- Also encrypt the data at rest in the database.\n- Protect the keys encryption keys by putting it in a vault and having well-define and secure access policies.\n- it's fine.\n\n\nASSISTANT\n\n\n{\n\"original message\": \"it's fine.\",\n\"message_descriptor\": \"Casual response\",\n\"confidence_score\": 0.2\n}\n\n\nUSER\n\n- Do you know how to make an enterprise SaaS app SoC 2 compliant?\n- No, I don't. \n- But perhaps, it is a series of steps we can take to ensure that customers data is protected at all times.\n- For example, make sure the access to backend is through https. This ensures data is encrypted in flight.\n- Also encrypt the data at rest in the database.\n- Protect the keys encryption keys by putting it in a vault and having well-define and secure access policies.\n\nASSISTANT \n\n{\n\"original message\": \"Protect the keys encryption keys by putting it in a vault and having well-define and secure access policies.\",\n\"message_descriptor\": \"Protecting encryption keys\",\n\"confidence_score\": 0.8\n}```",
        "51": "",
        "52": "Now llama 3 came out. Honestly, I can't keep up with these models anymore :grinning:",
        "53": "And you follow the news :slightly_smiling_face:",
        "54": "yes. and it gives me headaches :grinning:",
        "55": "I see",
        "56": "What's the end of week project status <@U047WSYBE5T>?",
        "57": "Hi <@U066Q9JAU3B>, I\u2019ll share the status report with you within an hour. ",
        "58": "ok. Olga told me that you are sick. You don't have to work when you are sick. Just keep me informed. that's it.",
        "59": "Thank you <@U066Q9JAU3B> :slightly_smiling_face:  May I send a status update on Monday instead? ",
        "60": "yes",
        "61": "Hi <@U066Q9JAU3B> I'm sharing a status update for the previous week and our current status:\n\nDuring the previous week, we tested several approaches for conversation disentanglement (using next-sentence predictions, and message comparison using a prompt to LLM), we also started work on an additional method to improve the prediction of related messages. As for predicting topics, the numbering problem has been fixed, and prompt for LLM was improved. As for now, we're working on discovering a BERTopic so today and tomorrow we\u2019re going to prepare examples of predicted topics.\nWe believe that it would be more beneficial to show those examples via meeting and as a result to choose the option that matches your expectations and covers business goals.\n\nWhat time would be convenient for you to meet tomorrow?\n\nIn addition, I'm going to share a budget report with you via email <mailto:yksoni@transcendtech.io|yksoni@transcendtech.io>. Let me know if you have any questions or suggestions.",
        "62": "thanks <@U047WSYBE5T> ~8:30 am would be perfect~. How about you <@U06855K24SE>? If this time does not work, we can do it earlier too from 7 am onwards.",
        "63": "let's meet at the time when <@U06855K24SE> can join <@U047WSYBE5T>",
        "64": "<@U047WSYBE5T> can we meet 6:30 am PST? <@U06855K24SE> needs to drop off by 7:30 a.m",
        "65": "<@U066Q9JAU3B> sounds perfect. I scheduled the meeting for tomorrow at 6:30AM PST.",
        "66": "Thank you for your flexibility in schedule. Looking forward to catching up tomorrow.",
        "67": "Examples for meeting",
        "68": "<!here> Where are we meeting?",
        "69": "<https:\/\/meet.google.com\/gvs-vmyb-ekk?authuser=0&amp;pli=1>",
        "70": "<@U06855K24SE> sorry, just saw your message",
        "71": "Thank you",
        "72": "Thread for manual clustering",
        "73": "<@U06855K24SE> <@U047WSYBE5T> <@U05B5FXLP62> <@U06RU2JKMDG> <@UU43NJY8K>\n\nAs discussed during our call earlier today, trying out different techniques for clustering is fruitless unless you have a simple mechanism to compare the results of your trials to a ground truth.\n\nBasic Data Science 101\n\u2022 Have a ground truth input\/output combo\n\u2022 Create model\n\u2022 Provide input to the model\n\u2022 Check output against the ground truth\nToday we couldn't understand the assertions like 'previous techniques didn't work'. You have to be able to demonstrate what did not work, what was the output of the models (either good or bad).\n\nThis way, when you are trying new approaches, we will know that you are making progress and converging towards a good solution. Right now, we are completely in the dark and your effort is a complete blackbox. You weren't able to demo anything or explain what didn't work and why. This work is not meeting our expectations (frankly we are asking for basic science experiment).\n\nPlease use the following files in all experiments going forward:\n```Please MANUALLY cluster the messages from following files into topics and show us the result.\n\ndata-8.csv\nh2oai.csv -- first 400 lines\nscikit-learn.csv -- first 400 lines```\nIf you cannot even manually cluster them, then we have no hope of AI model doing it. We are then just spending cycles and wasting time. We might as well terminate the project.\n\nPlease let me know if you have any questions.",
        "74": "here are the csv files (these were part of the dataset I provided earlier, but attaching it again for your convenience.)",
        "75": "Hi <@U066Q9JAU3B> and <@U06855K24SE>, please let me know what time would work best for you to schedule a meeting? Do you have any free time between 8 and 10 am your time?",
        "76": "6:30 am PST",
        "77": "<@U066Q9JAU3B> <@U06855K24SE> find the meeting link below.\n\nTopic Clustering: status update\nWednesday, April 24 \u00b7 4:30 \u2013 5:00pm\nTime zone: Europe\/Kiev\nGoogle Meet joining info\nVideo call link: <https:\/\/meet.google.com\/gvs-vmyb-ekk>\nOr dial: \u202a(GB) +44 20 3910 5722\u202c PIN: \u202a693 954 816\u202c#\nMore phone numbers: <https:\/\/tel.meet\/gvs-vmyb-ekk?pin=9227601054781>",
        "78": "<@U066Q9JAU3B> <@U06855K24SE> <https:\/\/docs.google.com\/document\/d\/1mLIgh8_28xanJfLrObb6Sbr70L2skOf97HbD-9RZtqk\/edit|r>eport link: <https:\/\/docs.google.com\/document\/d\/1mLIgh8_28xanJfLrObb6Sbr70L2skOf97HbD-9RZtqk\/edit>",
        "79": "Hi team, please find the meeting summary below:\n1. We agreed to develop a comparison sheet to assess various methods: manual clusterization, LLM, and BERTtopics + LLM.\n2. Identified the need to find an individual to assist with annotations.\n3. Planned to establish a scoring pipeline - out if initial roadmap scope, to be estimated\n4. Once the pipeline is ready, we'll share the code with Yogesh and Srikanth for testing - out if initial roadmap scope, to be estimated (required some time for preparation)\nIn summary, we have plans for creating a comparison sheet for different methods, establishing a scoring pipeline, and preparing a demo for the next meeting to share with the team.\nMeet recording: <https:\/\/drive.google.com\/file\/d\/109-LSZtXujz8-ZicSk80m3l_D8RsY5ey\/view?usp=sharing>",
        "80": "<@U06RU2JKMDG> <@U05B5FXLP62> I am reading the report. I thought manual labeling required 2 hours. How can LLM take 2 hours to process 500 messages? Is there some error in the report?\n\n```LLM Generation required approximately 2 hours to process the dataset.```\n",
        "81": "<@U05B5FXLP62> Also, I will reiterate my sentiments regarding BertTopic performance. I completely disagree your statement in the report that <BertTopic provides 'better performance in topic quality'.> What is this statement based upon?\n\nJust a cursory look at the generated output from BertTopic reaffirms this.\n\nsee the attached screenshot below. These 2 sentences are clearly part of a single conversation\/topic, but Bertopic assigns them different topics. This performance is so bad, it leads me to question why are we still evaluating BertTopic.\n\n<Unless>, you are able to DEMONSTRATE a MUCH BETTER performance, I would say we abandon the BertTopic approach. We are just wasting time and money, chasing a wild goose.\n\nI am open to changing my mind. Please convince me with very good reasons!\n\n``````",
        "82": "<@U047WSYBE5T> Let's have the next meeting on Friday morning, same time.\nPlease set up a recurring meeting on Tuesday and Friday mornings (same time). We would like to closely monitor our progress so that we are on the same page regarding the model performance and project expectations.",
        "83": "<@U05B5FXLP62> <@U06RU2JKMDG> <@U06855K24SE>\n<https:\/\/maartengr.github.io\/BERTopic\/getting_started\/clustering\/clustering.html>\nIt seems BerTopic clusters messages based on semantic similarity. Which to me does not make sense at all for our purposes.\nConsider the following. The output of the following code is:\nCosine similarity score: 0.3524\n\nThis is low score and BerTOpic will assign it in different topics. Even though the question and answer have low semantic similarity, they are CLEARLY part of a single conversation\/topic. To me, BerTopic is CONCEPTUALLY flawed.\nUnless you are able to replace the BERTopic clustering algorithm, I don't see how we will get good performance.\n\nI'd appreciate if you could respond in detail to my doubts here on the channel.\n\n```from sentence_transformers import SentenceTransformer\nimport numpy as np\n\n# Load the pre-trained model\nmodel = SentenceTransformer('all-mpnet-base-v2')\n\n# Encode the sentences\nsentence1 = \"What is your name?\"\nsentence2 = \"robert\"\n\nembedding1 = model.encode(sentence1)\nembedding2 = model.encode(sentence2)\n\n# Calculate the cosine similarity\ncosine_similarity = np.dot(embedding1, embedding2) \/ (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n\nprint(f\"Cosine similarity score: {cosine_similarity:.4f}\")```",
        "84": "Hi, there are no errors. The performance of the LLM model has improved now due to better prompt engineering. The model no longer duplicates all messages from the input in response, which previously led to very slow generation of large output. Now the model generates a response for 550 messages in 15 minutes.",
        "85": "<@U066Q9JAU3B> daily meeting are scheduled",
        "86": "Sorry for delay in response. Today, we conducted several experiments and decided to continue without the BERTopic approach. Current approach uses LLM for topic generation. You can familiarize yourself with the results in the attached CSV file. The first row represents manual labeling, and the second row represents topics predicted by LLM. The inference time for topic generation for 550 messages was reduced from 2 hours to 20 minutes by utilizing unique message identifiers.",
        "87": "<@U06RU2JKMDG>\n1. Why did you drop BERTopic. Just because I said so or because of some logical reasons?\n2. Why was the inference time of LLM 2 hours  earlier and 20 minutes now? Are you running the inference on your laptop? I don't see any reason why inference would take such long time only for 550 messages. It literally takes a few seconds on gpt3.5. You should tell me exactly what is your setup.",
        "88": "Regarding the first point, we decided to drop BERTopic because this tool takes into consideration only the semantic meaning of messages for clustering them into \"semantically similar groups\" instead of generating actual topics. Fine-tuning BERTopic with LLM only leads to choosing the most appropriate description for \"semantically similar groups\". Based on this, we decided to stop trying to integrate it into our solution.",
        "89": "The performance of the LLM model has improved now due to better prompt engineering. Inference was performed on GPU accelerators NVIDIA T4 and RTX 3050. Today, I plunged headlong into this task and found a solution for reducing inference time. The model no longer duplicates all messages from the input in the response in the output JSON file, which previously led to a very slow generation of a large output.",
        "90": "<@U066Q9JAU3B> feel free to reach out if you need more details. We're here to help and can add any additional information you require.",
        "91": "Reach out to who? using what medium? By asking questions in this Slack thread I am reaching out. :thinking_face:",
        "92": "<@U066Q9JAU3B> are you available for a call?",
        "93": "<@U06RU2JKMDG> please send me the code. I will run it myself and see if it takes 20 minutes. It should be much faster.",
        "94": "in 10 minutes yes.",
        "95": "Please use this link to jump on the call in 10 min <https:\/\/meet.google.com\/iyw-qkks-gsf?authuser=0>",
        "96": "<https:\/\/colab.research.google.com\/drive\/1lUEmVU-lcSu-XHltEpR3U8BdR7lLSXkH?usp=sharing>",
        "97": "<@U06RU2JKMDG> Thanks . FYI. My plan is to profile this code and see where exactly 20 minutes is spent. Profiling is usually  is the first step if we hope to increase the performance.",
        "98": "Hi <@U066Q9JAU3B> and <@U06855K24SE>, hope you both doing well.\nI'm pleased to share that the processing time has been significantly reduced to just 30 seconds. Feel free to test it out yourselves as well :slightly_smiling_face:\n<https:\/\/fvijgqwgnl36dk-8888.proxy.runpod.net\/lab\/tree\/workspace\/speed_up_the_model.ipynb>\ntoken: <http:\/\/localhost:8888\/?token=ngwajx5sfyujd8tiyzjl|ngwajx5sfyujd8tiyzjl>",
        "99": "Status report: <https:\/\/docs.google.com\/document\/d\/1NZdtXv5jwTfhmaatSdjP_1uFU1SATH6riGd8rpVbdcg\/edit>",
        "100": "<@U066Q9JAU3B> <@U06855K24SE> feel free to join <https:\/\/meet.google.com\/gvs-vmyb-ekk?authuser=0>",
        "101": "*Potential additional metrics for evaluating similarity of messages:*\n    Normalized mutual information (NMI);\n    One-to-One Overlap (1-1);\n    Loc3;\n    Shen F score (Shen-F).",
        "102": "<@U05B5FXLP62> could you please pose a link to Golden Link paper?",
        "103": "<https:\/\/aclanthology.org\/P19-1374.pdf>",
        "104": "<https:\/\/github.com\/jkkummerfeld\/irc-disentanglement\/tree\/master>",
        "105": "It's one of packages which we use for understanding metrics which are created for Conversation Disentanglement task",
        "106": "Hi <@U066Q9JAU3B>. Currently, the LLM model accelerated by the vLLM library is running on the RunPod, which leads to unnecessary spending. Would you prefer to take a look at it at some point in the future, or can I shut it down? If you wish, I could re-run that Pod when you prefer to explore that solution. Thanks.",
        "107": "<@U06RU2JKMDG> Please keep it running over the weekend as we may want to run some experiments. How much does it cost\/hour? I thought it was a serverless instance (so no requests, no spending :thinking_face:)",
        "108": "The cost of this service is $0.84 per hour. Would it be possible for us to continue our discussion about deployment on Monday? Please let me know if that works for you.",
        "109": "yeah. <@U06RU2JKMDG> We can discuss Monday. Just send me an invite. Also, you can shutdown the service. Just send me instructions on how to restart it. When I need to run experiements, I will restart it myself.",
        "110": "Hello, here are some instructions: <https:\/\/docs.google.com\/document\/d\/1Ism79VAbykVv6nx4crpUq-x0XUk_xLtsyc2uw3zuDFo\/edit?usp=sharing>",
        "111": "<@U06RU2JKMDG> I am assuming that model would be deployed as a python code, and not in the notebook format, correct? let me know if you want to get on a quick call to discuss it.",
        "112": "Hello <@U066Q9JAU3B> . Of course, the .ipynb format is well-suited for experiments, but production code would be only in .py.",
        "113": "We are considering using the Gradio interface for interacting with the model. You should be able to drop a CSV file in the form and receive the same CSV file with an additional column labeled \"topic prediction\". Please let me know if that works for you.",
        "114": "I am fine with Gradio. Will we still be able to interface with the model using REST API calls in case we don't want to use any frontend?",
        "115": "Yes, of course.",
        "116": "cool thanks. Feel free to define the REST API as you see fit. :+1:",
        "117": "Hi <@U066Q9JAU3B>, I would like to provide with some update on our progress.\n\nToday, we annotated topics for 400 messages from h2oai.csv and scikit-learn.csv each and received LLM models' topic predictions. Evaluation was performed using different metrics: golden links matching, Interconnected golden links matching, and mean cosine similarity of topic descriptors. You can familiarize yourself with the results in this report: <https:\/\/docs.google.com\/document\/d\/1NZdtXv5jwTfhmaatSdjP_1uFU1SATH6riGd8rpVbdcg\/edit#heading=h.yg2kcje8omg4>.\n\nThe metric based on cosine similarity struggles to correctly capture the models' performance, so we suppose not to use it in future evaluations. The quality of predictions can be further improved by employing better prompt engineering and LLM memory. Additionally, we can use an additional LLM to reduce the number of messages that were not distributed into topics by the main LLM.",
        "118": "Currently, we are preparing the model for deployment, and we will inform you of our progress with this task.",
        "119": "thanks <@U06RU2JKMDG>;\nQuick question: for the deployment:\n1. where will the python code (gradio app) run?\n2. where will the actual model (finetuned or prompted llm) run?\n3. Assuming these deployments will be private, protected by auth?",
        "120": "Thanks <@U066Q9JAU3B>. We are currently exploring the possibility of using RunPod to deploy both an LLM and code on the same server, primarily due to its cost-effectiveness. Regarding the last question, I believe it would be beneficial to consult with Vlad before proceeding with the integration of authentication mechanisms. I intend to have a discussion with him about this matter tomorrow.",
        "121": "<@U06RU2JKMDG> quick question about RunPod. Do we have to pay them per hour (and start\/stop server)?\nOr\ncan we deploy server less and pay by usage? <https:\/\/www.runpod.io\/serverless-gpu>",
        "122": "Hi. We will use a serverless GPU for vLLM and a RunPod CPU server for all code.\nRegarding authentication, we could employ an API token very similar to the OpenAI API token for authentication.",
        "123": "<@U05B5FXLP62>  <@U06RU2JKMDG> forgot to ask. Do you need me to provide access to GCP or Azure for GPU serverless services?",
        "124": "Let's we'll finish deployment on RunPod and after we'll migrate to other provider",
        "125": "sounds good. thanks\/ :+1:",
        "126": "Hi team, Here are the key points discussed during our meeting:\n\n1.  *Assignments for Monday:*\n    \u25e6 Labeling: The team aims to label the remaining 2 files\n    \u25e6 Testing: Using two evaluation methods we\u2019re planning to test using provided data.\n    \u25e6 Final Report: Add those results to the final report.\n    \u25e6 Deployment: Create an endpoint to communicate with the model and deploy the final solution \n2. *Weekend Plans for Yogesh and Srikanth:*\n    \u25e6 Exploration: Yogesh and Srikanth will explore the report and conduct testing over the weekend. Additionally, they will determine a plan for the next iteration \n3. *Next Steps:*\n    \u25e6 Discussion: The team agreed to discuss the proposed plan during the daily meeting on Tuesday for further refinement and action.\nMeet recording: <https:\/\/drive.google.com\/file\/d\/1ZP6iAK6HsSQ6FyyJ2HTej-cidct2vLXI\/view?usp=sharing>",
        "127": "<!here> Please find below an update on the current status",
        "128": "Thank you for the progress so far. Can you please share the exact definitions of \"*Matched Golden Links*\" and \"*Matched Interconnected Golden links*\"?",
        "129": "Hello,\n\nGolden links represent the number of message pairs connected consecutively for each topic in the labeled dataset.\nInterconnected Golden links represent all possible message pairs for each topic in the labeled dataset.\n\nThen we can calculate the same links and interconnected links for topics generated by LLM.\nSubsequently, we determine the number of correctly matched message pairs between the generated links and the golden links (links from labeled data).\nInterconnected Golden links are pruned to capture more complex relationships in topics.",
        "130": "Hi <@U066Q9JAU3B> and <@U06855K24SE> just wanted to give you an update on what we've been up to. Today we tackled deployment and now we're working on debugging to make it easier for you to test things out. Hopefully, we'll have it all sorted before our meeting.",
        "131": "thanks <@U047WSYBE5T>. Please feel free to organize next meeting on Thursday or Friday as you see fit - Depends on how comfortable Max is.",
        "132": "yeah, me and Srikanth met last night and we have a fair idea of the next steps. We will need Vlad\/Max inputs and can finalize the next steps together.",
        "133": "<!here> Hi team, thanks for the productive meeting today. We made progress on deployment. Tomorrow, I'll check debugging progress and aim to reschedule Friday's meeting to Thursday at 6:30 AM if we're ready. In our next meeting, we'll discuss upcoming tasks. Please share scope info by Thursday.\n<https:\/\/drive.google.com\/file\/d\/1Cl2oDIRe_J2Vtrix5YHZGYeFNWAAd5jR\/view?usp=sharing>",
        "134": "Hi <@U066Q9JAU3B>,\n\nI wanted to inform you about today's progress. Today, we mainly focused on integrating serverless GPU for our solution and creating a CPU server on RunPod.\n\nWe have developed an endpoint featuring two POST methods to enhance performance and avoid timeout errors when processing large CSV files.\n\nThe POST method \/process_csv\/ is designed to receive a CSV file for processing along with the chat's name. It requires the CSV file to have columns labeled \"time,\" \"user,\" and \"message.1,\" and it returns predictions under the \"predicted topic\" column. To append the CSV with new messages, ensure you send the entire CSV file rather than just the new messages.\n\nIf the chat's name matches a previously processed chat, \/process_csv\/ will append predictions to the existing data for new messages.\n\nThe second POST method, \/receive_csv\/, accepts the chat's name to retrieve the preprocessed file with predictions.\n\nThe POST method \/process_csv\/ activates a GPU serverless instance for CSV processing. The POST method \/receive_csv\/ communicates only with the CPU server on RunPod.\n\nYou can access this endpoint via the following link: <https:\/\/jptopw0uv35dxl-4000.proxy.runpod.net\/docs>\n\nI look forward to discussing the next steps tomorrow. If you have any questions, I would be happy to help.",
        "135": "thanks for the update <@U06RU2JKMDG>:+1: I will have a look. Talk tomorrow!",
        "136": "<@U06RU2JKMDG> When you get a chance, please also share the source code and a short README about the infra setup (how to run this code as serverless on runpod ).",
        "137": "<@U06RU2JKMDG> it would also help if you share a couple of csv files for me to test the REST endpoint. :pray:",
        "138": "<@U06RU2JKMDG> Also, if we wanted to modify the processing REST endpoint, <to accept the input for  as an message_array instead of csv>, would there be any limitations we should be wary of? is there currently an upper limit on this rest endpoint regarding the number of rows in the csv?",
        "139": "Hi <@U066Q9JAU3B>, A Topic prediction model leverages timestamps of messages and the names of message authors. For now, we can only use CSV files, but we could correct this in the future. Regarding the second question, there is no upper limit of rows in a CSV file. I'll upload the code and send you the link when I'm back near a PC again.",
        "140": "<@U06RU2JKMDG> so I tried process_csv endpoint with h2oai.csv you shared.\nAfter about 100 seconds, the api times out with 524 Error. The api seems to not be working as expected :disappointed:",
        "141": "Yes, the server runtime timeout occurs every time for this POST method. To overcome the 512 timeout error, we implemented the \/receive_csv\/ POST method, where you can access the file after processing. Tomorrow, we plan to integrate a solution for easier communication with this endpoint.",
        "142": "Even the receive_csv is timing out",
        "143": "\u2022 I look forward to have REST API is working state. Users should not *expect* the api to fail with timeout. A successful operation should always return a successful response with data.\n\u2022 While you are at it, please see if you ALSO add API to accept\/return data in json format. The reason being, most of the time, the data will live inside mongodb. It is an added cost to construct\/deconstruct csv file. We might as well just provide a json with message_array.",
        "144": "Sorry for the inconvenience. Tomorrow, we will correct these timeout-related problems and add JSON compatibility.",
        "145": "to be clear, both input and output should be JSON.",
        "146": "```<!DOCTYPE html>\n&lt;!--[if lt IE 7]&gt; &lt;html class=\"no-js ie6 oldie\" lang=\"en-US\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 7]&gt;    &lt;html class=\"no-js ie7 oldie\" lang=\"en-US\"&gt; &lt;![endif]--&gt;\n&lt;!--[if IE 8]&gt;    &lt;html class=\"no-js ie8 oldie\" lang=\"en-US\"&gt; &lt;![endif]--&gt;\n&lt;!--[if gt IE 8]&gt;&lt;!--&gt; &lt;html class=\"no-js\" lang=\"en-US\"&gt; &lt;!--&lt;![endif]--&gt;\n&lt;head&gt;\n\n\n&lt;title&gt;<http:\/\/jptopw0uv35dxl-4000.proxy.runpod.net|jptopw0uv35dxl-4000.proxy.runpod.net> | 524: A timeout occurred&lt;\/title&gt;\n&lt;meta charset=\"UTF-8\" \/&gt;\n&lt;meta http-equiv=\"Content-Type\" content=\"text\/html; charset=UTF-8\" \/&gt;\n&lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=Edge\" \/&gt;\n&lt;meta name=\"robots\" content=\"noindex, nofollow\" \/&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/&gt;\n&lt;link rel=\"stylesheet\" id=\"cf_styles-css\" href=\"\/cdn-cgi\/styles\/main.css\" \/&gt;\n\n\n&lt;\/head&gt;\n&lt;body&gt;\n&lt;div id=\"cf-wrapper\"&gt;\n    &lt;div id=\"cf-error-details\" class=\"p-0\"&gt;\n        &lt;header class=\"mx-auto pt-10 lg:pt-6 lg:px-8 w-240 lg:w-full mb-8\"&gt;\n            &lt;h1 class=\"inline-block sm:block sm:mb-2 font-light text-60 lg:text-4xl text-black-dark leading-tight mr-2\"&gt;\n              &lt;span class=\"inline-block\"&gt;A timeout occurred&lt;\/span&gt;\n              &lt;span class=\"code-label\"&gt;Error code 524&lt;\/span&gt;\n            &lt;\/h1&gt;\n            &lt;div&gt;\n               Visit &lt;a href=\"<https:\/\/www.cloudflare.com\/5xx-error-landing?utm_source=errorcode_524&amp;utm_campaign=jptopw0uv35dxl-4000.proxy.runpod.net>\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;<http:\/\/cloudflare.com|cloudflare.com>&lt;\/a&gt; for more information.\n            &lt;\/div&gt;\n            &lt;div class=\"mt-3\"&gt;2024-05-01 16:37:51 UTC&lt;\/div&gt;\n        &lt;\/header&gt;\n        &lt;div class=\"my-8 bg-gradient-gray\"&gt;\n            &lt;div class=\"w-240 lg:w-full mx-auto\"&gt;\n                &lt;div class=\"clearfix md:px-8\"&gt;\n                  \n&lt;div id=\"cf-browser-status\" class=\" relative w-1\/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\"&gt;\n  &lt;div class=\"relative mb-10 md:m-0\"&gt;\n    \n    &lt;span class=\"cf-icon-browser block md:hidden h-20 bg-center bg-no-repeat\"&gt;&lt;\/span&gt;\n    &lt;span class=\"cf-icon-ok w-12 h-12 absolute left-1\/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"&gt;&lt;\/span&gt;\n    \n  &lt;\/div&gt;\n  &lt;span class=\"md:block w-full truncate\"&gt;You&lt;\/span&gt;\n  &lt;h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\"&gt;\n    \n    Browser\n    \n  &lt;\/h3&gt;\n  &lt;span class=\"leading-1.3 text-2xl text-green-success\"&gt;Working&lt;\/span&gt;\n&lt;\/div&gt;\n\n&lt;div id=\"cf-cloudflare-status\" class=\" relative w-1\/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\"&gt;\n  &lt;div class=\"relative mb-10 md:m-0\"&gt;\n    &lt;a href=\"<https:\/\/www.cloudflare.com\/5xx-error-landing?utm_source=errorcode_524&amp;utm_campaign=jptopw0uv35dxl-4000.proxy.runpod.net>\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;\n    &lt;span class=\"cf-icon-cloud block md:hidden h-20 bg-center bg-no-repeat\"&gt;&lt;\/span&gt;\n    &lt;span class=\"cf-icon-ok w-12 h-12 absolute left-1\/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"&gt;&lt;\/span&gt;\n    &lt;\/a&gt;\n  &lt;\/div&gt;\n  &lt;span class=\"md:block w-full truncate\"&gt;San Jose&lt;\/span&gt;\n  &lt;h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\"&gt;\n    &lt;a href=\"<https:\/\/www.cloudflare.com\/5xx-error-landing?utm_source=errorcode_524&amp;utm_campaign=jptopw0uv35dxl-4000.proxy.runpod.net>\" target=\"_blank\" rel=\"noopener noreferrer\"&gt;\n    Cloudflare\n    &lt;\/a&gt;\n  &lt;\/h3&gt;\n  &lt;span class=\"leading-1.3 text-2xl text-green-success\"&gt;Working&lt;\/span&gt;\n&lt;\/div&gt;\n\n&lt;div id=\"cf-host-status\" class=\"cf-error-source relative w-1\/3 md:w-full py-15 md:p-0 md:py-8 md:text-left md:border-solid md:border-0 md:border-b md:border-gray-400 overflow-hidden float-left md:float-none text-center\"&gt;\n  &lt;div class=\"relative mb-10 md:m-0\"&gt;\n    \n    &lt;span class=\"cf-icon-server block md:hidden h-20 bg-center bg-no-repeat\"&gt;&lt;\/span&gt;\n    &lt;span class=\"cf-icon-error w-12 h-12 absolute left-1\/2 md:left-auto md:right-0 md:top-0 -ml-6 -bottom-4\"&gt;&lt;\/span&gt;\n    \n  &lt;\/div&gt;\n  &lt;span class=\"md:block w-full truncate\"&gt;<http:\/\/jptopw0uv35dxl-4000.proxy.runpod.net|jptopw0uv35dxl-4000.proxy.runpod.net>&lt;\/span&gt;\n  &lt;h3 class=\"md:inline-block mt-3 md:mt-0 text-2xl text-gray-600 font-light leading-1.3\"&gt;\n    \n    Host\n    \n  &lt;\/h3&gt;\n  &lt;span class=\"leading-1.3 text-2xl text-red-error\"&gt;Error&lt;\/span&gt;\n&lt;\/div&gt;\n\n                &lt;\/div&gt;\n            &lt;\/div&gt;\n        &lt;\/div&gt;\n\n        &lt;div class=\"w-240 lg:w-full mx-auto mb-8 lg:px-8\"&gt;\n            &lt;div class=\"clearfix\"&gt;\n                &lt;div class=\"w-1\/2 md:w-full float-left pr-6 md:pb-10 md:pr-0 leading-relaxed\"&gt;\n                    &lt;h2 class=\"text-3xl font-normal leading-1.3 mb-4\"&gt;What happened?&lt;\/h2&gt;\n                    &lt;p&gt;The origin web server timed out responding to this request.&lt;\/p&gt;\n                &lt;\/div&gt;\n                &lt;div class=\"w-1\/2 md:w-full float-left leading-relaxed\"&gt;\n                    &lt;h2 class=\"text-3xl font-normal leading-1.3 mb-4\"&gt;What can I do?&lt;\/h2&gt;\n                          &lt;h3 class=\"text-15 font-semibold mb-2\"&gt;If you're a visitor of this website:&lt;\/h3&gt;\n      &lt;p class=\"mb-6\"&gt;Please try again in a few minutes.&lt;\/p&gt;\n\n      &lt;h3 class=\"text-15 font-semibold mb-2\"&gt;If you're the owner of this website:&lt;\/h3&gt;\n      &lt;p&gt;&lt;span&gt;The connection to the origin web server was made, but the origin web server timed out before responding. The likely cause is an overloaded background task, database or application, stressing the resources on your web server. To resolve, please work with your hosting provider or web development team to free up resources for your database or overloaded application.&lt;\/span&gt; &lt;a rel=\"noopener noreferrer\" href=\"<https:\/\/support.cloudflare.com\/hc\/en-us\/articles\/200171926-Error-524>\"&gt;Additional troubleshooting information here.&lt;\/a&gt;&lt;\/p&gt;\n                &lt;\/div&gt;\n            &lt;\/div&gt;\n        &lt;\/div&gt;\n\n        &lt;div class=\"cf-error-footer cf-wrapper w-240 lg:w-full py-10 sm:py-4 sm:px-8 mx-auto text-center sm:text-left border-solid border-0 border-t border-gray-300\"&gt;\n  &lt;p class=\"text-13\"&gt;\n    &lt;span class=\"cf-footer-item sm:block sm:mb-1\"&gt;Cloudflare Ray ID: &lt;strong class=\"font-semibold\"&gt;87d1305b3dee238d&lt;\/strong&gt;&lt;\/span&gt;\n    &lt;span class=\"cf-footer-separator sm:hidden\"&gt;&amp;bull;&lt;\/span&gt;\n    &lt;span id=\"cf-footer-item-ip\" class=\"cf-footer-item hidden sm:block sm:mb-1\"&gt;\n      Your IP:\n      &lt;button type=\"button\" id=\"cf-footer-ip-reveal\" class=\"cf-footer-ip-reveal-btn\"&gt;Click to reveal&lt;\/button&gt;\n      &lt;span class=\"hidden\" id=\"cf-footer-ip\"&gt;2601:646:9e01:fb20:9051:3d26:4d77:9653&lt;\/span&gt;\n      &lt;span class=\"cf-footer-separator sm:hidden\"&gt;&amp;bull;&lt;\/span&gt;\n    &lt;\/span&gt;\n    &lt;span class=\"cf-footer-item sm:block sm:mb-1\"&gt;&lt;span&gt;Performance &amp; security by&lt;\/span&gt; &lt;a rel=\"noopener noreferrer\" href=\"<https:\/\/www.cloudflare.com\/5xx-error-landing?utm_source=errorcode_524&amp;utm_campaign=jptopw0uv35dxl-4000.proxy.runpod.net>\" id=\"brand_link\" target=\"_blank\"&gt;Cloudflare&lt;\/a&gt;&lt;\/span&gt;\n    \n  &lt;\/p&gt;\n  &lt;script&gt;(function(){function d(){var b=a.getElementById(\"cf-footer-item-ip\"),c=a.getElementById(\"cf-footer-ip-reveal\");b&amp;&amp;\"classList\"in b&amp;&amp;(b.classList.remove(\"hidden\"),c.addEventListener(\"click\",function(){c.classList.add(\"hidden\");a.getElementById(\"cf-footer-ip\").classList.remove(\"hidden\")}))}var a=document;document.addEventListener&amp;&amp;a.addEventListener(\"DOMContentLoaded\",d)})();&lt;\/script&gt;\n&lt;\/div&gt;&lt;!-- \/.error-footer --&gt;\n\n\n    &lt;\/div&gt;\n&lt;\/div&gt;\n&lt;\/body&gt;\n&lt;\/html&gt;```",
        "147": "Hi <@U066Q9JAU3B>, today we're redoing the REST API. Now, it only has one POST method.\nAdditionally, the endpoint now accepts and returns files in JSON format.\nWe've also added authentication using keys.\n\nThe input JSON should still include information with timestamps and usernames for better topic estimation.\n\nIf you have any questions regarding the REST API, feel free to ask.",
        "148": "The previous link to REST endpoint doesn't work anymore. Could you please share the new link? It would be nice if you could demo the api during our call today.",
        "149": "Srikanth's Meeting Notes:\n\u2022 REST API\n\u2022 Topics discussed during the meeting\n    \u25e6 Hierarchical clustering\n    \u25e6 Real-time behavior: inference and model update\n    \u25e6 Search\n    \u25e6 Filtering\n    \u25e6 Summarization\n\u2022 Next steps: \n    \u25e6 REST API: team to share the API (with Readme, instructions, evaluation results) with Yogesh. Yogesh to test and integrate with the App. \n    \u25e6 Yogesh to share the pending problem statement(s), including downstream applications\n    \u25e6 Team to review the problem(s), and share proposals for the solution space",
        "150": "<@U06RU2JKMDG> <@U05B5FXLP62> <@U047WSYBE5T> Next Steps so that I can test\/integrate REST API.\n\n1. Source code of REST endpoint (if needed, I will modify the API to better integrate with the app)\n2. Source code for the model (if needed, I might tweak the prompt)\n3. README on how to deploy the models on runpod, both server and serverless.\n4. The link to the currently installed REST endpoints\n5. README on how to run model evaluations on the REST API endpoints.\nOnce I have this, I will test and let you know if I observe anything weird.\n\nMay be this will good exercise for Taras as well to use the README and try to test everything (so he comes up to the speed)??",
        "151": "<@U066Q9JAU3B> Thank you for the follow up. Regarding the exercise for Taras, I believe it is.",
        "152": "<@U047WSYBE5T> could you please share the meeting recording? It will help me with testing\/integration",
        "153": "<@U06RU2JKMDG> do you have the code checked in on Github? If yes, you could give me access (yksoni-transcendtech is my username).\nOr just share the code and I will check it in a new repo. Going forward, we should use Github, a standard practice for modern teams.",
        "154": "<@U06RU2JKMDG> I will also test what <@U05B5FXLP62> mentioned in the meeting (at around 7:30 in the recording). Will give a chat-name to an api call with initial set of data. Then provide new messages to that exact same chat-name in a second api call. Expected result is a combined data as far as I understood.\nAs discussed during the call, future work may involve combining the topics from multiple chat-names. I will create a proper product requirements doc for this.",
        "155": "Hi <@U066Q9JAU3B>. I sent you access to the GitHub repository with the source code. I'll add descriptions and README files tomorrow morning. The repository also contains some JSON files as examples. If you have any questions, feel free to ask.",
        "156": "thank you Max :saluting_face:",
        "157": "Hi <@U066Q9JAU3B>, I've added instructions for RunPod and the Evaluation folder in the GitHub repo: <https:\/\/github.com\/Maksym-Lys\/topic_clustering\/>\n\nCurrently, this repo contains:\n-Source code for REST API endpoints and models (both server and serverless).\n-Instructions for configuring the models on RunPod (both server and serverless).\n-A folder with evaluation scripts , Instructions for running evaluations and results on annotated data (you can use this script for other annotated data as well).\n\nRegarding the currently installed REST endpoints: I've shut down all RunPod servers for now because we've run out of cache balance.\n\nI could provide you with a link to the new server when everything is ready. If you have any questions or notes, I'll be happy to help",
        "158": "Thanks <@U06RU2JKMDG> I just loaded more cash on runpod.\nI didn't quite understand what you meant by this\n&lt;&lt;&lt;I could provide you with a link to the new server when everything is ready.&gt;&gt;",
        "159": "Hi <@U066Q9JAU3B>, I meant that I will be able to create a new server once the RunPod cash balance is loaded again. My apologies for any confusion.",
        "160": "no worries. Please go ahead and create new links. there is $50 of cash now.",
        "161": "Thanks <@U066Q9JAU3B>. There is an issue with Cloudflare when deploying a server on a public URL.\nA \"524 Timeout\" error in Cloudflare occurs when the origin web server takes more than 100 seconds to respond to a request.\n\nCurrently, our model can't process medium and large-sized JSON files in less than 100 seconds.\n\nOur current solution is to communicate with the REST API at the local network level to avoid using Cloudflare altogether.\nAdditionally, we are working on a solution that will respond to requests in a streaming manner to overcome the 100-second restriction.",
        "162": "noted.\n&gt; Additionally, we are working on a solution that will respond to requests in a streaming manner to overcome the 100-second restriction.",
        "163": "<@U06RU2JKMDG> I had some time to play around with the code. So far, I have only tried the serverless version.\n\u2022 I followed the part 1 of README to deploy serverless llm.\n\u2022 I followed part 2 to create a pod but then couldn't figure out the ip or domain name of the pod. So, I thought, may be I don't need the pod to run the FastAPI server.\n\u2022 So I ran the code locally, while successfully making llm calls to the serverless runpod.\n\u2022 I added a small dataset with only 20 messages so it is easier to work with. I used real slack messages.\n\u2022 While topic disentanglement works, we definitely need to improve the prompt for better topic assignment. I need to try with different prompts.\n\u2022 Then I tried to add another message to the input file, with the same chat_message_name. The code either gives me following error, or is stuck in a while loop (waiting temp_data_file to be removed).  *I think we need to fix the data preprocessing and make it simpler.* \n\u2022 I still need to try the dedicated GPU server version and eval scripts.\n```{\n  \"success\": false,\n  \"error\": \"Error tokenizing data. C error: Expected 1 fields in line 3, saw 2\\n\"\n}```",
        "164": "Hi <@U066Q9JAU3B>. We rewrote the dedicated server to work with streaming data. Now, we can deploy it and provide you with a public link. The drawback of this solution is that it returns an unstructured list of JSON files that should be sorted by timestamp from the client side. We also fixed the error which occurred when adding the same chat_message_name for dedicated server. As for the serverless solution, serverless endpoints often require more than 100 seconds just to complete a cold start and sometimes return a response after the 100-second timeout in an idle state. Therefore, we didn't rewrite the serverless solution for streaming responses.",
        "165": "ok. I will retry the dedicated server version today.",
        "166": "<@U06RU2JKMDG> are there certain cases where using a SaaS endpoint from an LLM vendor (hugging face, anthropics, openAI) is cheaper than dedicated GPU? I am guessing it depends on the usage right?",
        "167": "<@U066Q9JAU3B> Yes, besides this, it depends on the duration of GPU server rental. In general, a dedicated server is cheaper in the long-term, but all details should be carefully calculated using concrete examples.",
        "168": "ok. Do you have a link to the dedicated server endpoint for me to test?",
        "169": "I'll be able to create a new server once I return near a PC again. It will take around 2.5 hours.",
        "170": "no problem",
        "171": "Hi <@U066Q9JAU3B>, here is a link to the dedicated server: <https:\/\/1324uts5ekg9jh-4000.proxy.runpod.net\/docs>",
        "172": "<@U05R0KHPU8N> I get the following error from time to time, after a successful api response.",
        "173": "Hi <@U066Q9JAU3B> <@U06855K24SE>\nThank you for the meeting and your time today.\nPlease find the comments from our side as follows.\n\u2022 Next steps:\n    \u25e6 REST API: team to share the API (with Readme, instructions, evaluation results) with Yogesh. Yogesh to test and integrate with the App. - Max will share that in the slack project slack channel asap tomorrow.\n    \u25e6 Yogesh to share the pending problem statement(s), including downstream applications - thank you, we'll be waiting\n    \u25e6 Team to review the problem(s), and share proposals for the solution space - we'll do that asap, based on the information we'll get from Yogesh.\nAlso, I will share more information with Yogesh in our chat with him and Mariia.",
        "174": "<!here> Recording of today's meeting: <https:\/\/drive.google.com\/file\/d\/1Hjyy3n4RcgJ3mmjhLjeoR5J8jLRFCGC7\/view?usp=drive_link>",
        "175": "Hello <@U066Q9JAU3B> <@U06855K24SE>\nAs having agreed, all the information has been shared with <@U05R0KHPU8N>.\nTomorrow (Tuesday) Taras will prepare his overview of the project and his vision of the next steps.\nCould you let us know if having a call at 6:30 AM tomorrow (Tuesday) would be convenient for you, to discuss in detail?",
        "176": "yes. let's keep the meeting on.",
        "177": "I will send over a document for the team to preview before the meeting.",
        "178": "Hi everyone!\n\nAdditionally, <@U066Q9JAU3B> <@U06855K24SE>\nIt would be nice if you add me to the team on runpod. I would like to test Max's server implementation before our tomorrow meeting. Here's my account: <mailto:taras.svystun@litslink.com|taras.svystun@litslink.com>.",
        "179": "Hi <@U05R0KHPU8N>,\nWelcome to the team. Here is a rundpod invite link for you with admin permissions.. <https:\/\/www.runpod.io\/console\/user\/invite\/clvv2n7q90000ld08gh8xdt6a>",
        "180": "<@U05R0KHPU8N> <@UU43NJY8K> Here is the project specs document for your preview. We will discuss this in the meeting. Please have your questions ready.",
        "181": "Thanks, I\u2019ll take a closer look.",
        "182": "Hi Team, thank you for the productive meeting today :slightly_smiling_face:\n\nDuring the meeting we created a plan for the next two days:\n - We agreed that the team will work on establishing a connection with MongoDB and testing it.\nFurthermore, we aim to adjust the existing implementation to read and write from MongoDB. We\u2019re going to start working on it tomorrow.\n<@U066Q9JAU3B> Please provide <@U05R0KHPU8N> [<mailto:taras.svystun@litslink.com|taras.svystun@litslink.com>] with the MongoDB connection link.\n- Prior to setting up the connection with MongoDB, the team will perform a test of the existing server implementation.\n- Yogesh is planning to update the list of requirements and share it with the team before the next meeting.\nLink: <https:\/\/drive.google.com\/file\/d\/1ZP4znc1JIwFOQ_4njXn9ZhBufwL06FPE\/view?usp=drive_link>",
        "183": "<@U05R0KHPU8N> <@U06RU2JKMDG>\n\nHere is information about mongodb.\n\n\u2022 connection string:  \n```<mongodb+srv:\/\/max:vHOakaINcU7rjMb0@cluster0.lmny2np.mongodb.net\/?retryWrites=true&w=majority&appName=Cluster0>```\nYou connect to the db using following python code snippet.\n```from pymongo import MongoClient\n\n# Connection string format:\n# \"mongodb:\/\/[username]:[password]@[host]:[port]\/[database]\"\nconnection_string = \"mongodb:\/\/[username]:[password]@[host]:[port]\/[database]\"\n\n# Create a MongoClient instance using the connection string\nclient = MongoClient(connection_string)\n\n# Access the database\ndb = client[\"[database]\"]\n\n# Access a collection within the database\ncollection = db[\"[collection_name]\"]\n\n# Perform operations on the collection\n# e.g. insert a document\ndocument = {\"key\": \"value\"}\nresult = collection.insert_one(document)\n\nprint(\"Connected to MongoDB successfully!\")```\nI have also invited both of you to be the project admin. You may have to whitelist your ip address on mongodb in the following link so that the mongodb allows you to connect to the database programmatically using connection string\n<https:\/\/cloud.mongodb.com\/v2\/663ab1f50b7d3a26fef9b80a#\/security\/network\/accessList>",
        "184": "You will find 2 collections inside 'chats' database in this this cluster.\n\u2022 'threads' - stores the metadata of a chat thread\n\u2022 'blocks' - stores the actual messages (also known as blocks)\nThis is the schema I am using in the product. If you could stick to it, great!. You don't have to use all the fields.\n\nplease note that I am using str(uuid) values for _id (instead of default ObjectId by mongodb, because Pydantic freaks out with ObjectId.) Same reason I use the timestamps in str(datatime.datetime.isoformat())",
        "185": "Here is the pydantic model for blocks\n\n```class BlockModel(BaseModel):\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()), alias=\"_id\")\n    content: str = Field(...)\n    created_at: datetime = Field(default_factory=datetime.now)\n    last_modified: datetime = Field(default_factory=datetime.now)\n    creator_id: str = Field(default=\"unknown id\")\n    main_thread_id: str = Field(default=\"\")\n    position: int = Field(default=0)\n    child_thread_id: str = Field(default=\"\")\n    task_status: str = Field(\n        default=\"todo\", pattern=\"^(todo|inprogress|done)$\")\n    tenant_id: str = Field(default=\"\")```",
        "186": "Here is the pydantic model for thread\n\n```class ThreadModel(BaseModel):\n    id: str = Field(default_factory=lambda: str(uuid.uuid4()), alias=\"_id\")\n    creator_id: str\n    created_at: datetime = Field(default_factory=datetime.now)\n    last_modified: str\n    type: ThreadType\n    title: str = Field(..., min_length=1, max_length=100,\n                       pattern=\"^[a-zA-Z0-9]+$\")\n    \n    headline: str = Field(default=None)\n    tenant_id: str\n    num_blocks: int = Field(default=0)\n    parent_block_id: Optional[str] = Field(default=None, null=True)\n    slack_thread_ts: Optional[float] = Field(default=None, null=True)\n    model_config = ConfigDict(extra='ignore',\n                              populate_by_name=True,\n                              arbitrary_types_allowed=True,\n                              )```",
        "187": "in blocks collection, we identify which thread a block belongs to by fetching 'main_thread_id' value and looking up it in '_id' field of the threads collection.",
        "188": "Let me know if you face any issues.",
        "189": "Hey, <@U066Q9JAU3B>,\nto manage IP Access List entries, I must have `Organization Owner` or `Project Owner` access to the project. I have neither, can you please grant me one of those?",
        "190": "If that's not possible, please add the following to ip access list:\n176.36.20.40",
        "191": "Made you a project owner. Please check. <@U05R0KHPU8N>",
        "192": "Thanks, now it works!",
        "193": "<@U06RU2JKMDG> I have forked the github repository you created because it helps me to maintain it in the long run and add access control for additional users.\n\nplease clone and add all future commits from the followinf repository.\n\n<https:\/\/github.com\/TranscendHQ\/InformationIntelligence>\n\nI have added you as a user.\n\n<@U05R0KHPU8N> can you please provide me your github handle so I can add you as well?\n\nThanks",
        "194": "Sure, <@U066Q9JAU3B>, here's my github:\ntaras-svystun",
        "195": "<@U05R0KHPU8N> added you to github. Were you able to access the mongodb?",
        "196": "I cloned the repo and all my future commits will come from 'taras' branch.",
        "197": "Hi <@U066Q9JAU3B> and <@U06855K24SE>, hope you are both doing well.\n\nI wanted to provide you with a quick update on our project progress:\n\u2022 Due to challenges with errors occurring in one-third of cases, we dedicated time to rectify the issue. I'm pleased to report that we've successfully reduced the error rate to 3%. Maksym dedicated 8 hours of work yesterday and today to achieve this outcome. If you don\u2019t mind we will add this time to Taras's Upwork contract manually.\n\u2022 In parallel, we're currently hard at work connecting and testing the integration with MongoDB. We're planning to wrap this up by tomorrow so we can smoothly move on to the next challenges.\nAdditionally, I was wondering if we could have the meeting on Thursday instead of Friday. It would be great to keep the team on track. If Thursday doesn't work, please let me know so I\u2019ll reschedule the meeting.\n\nLooking forward to hearing from you.",
        "198": "Hi <@U047WSYBE5T>\n\u2022 Appreciate the work. Please go ahead and add the hours. Also, which error rate is this? Is this about receiving 'null' topic form the model for the streaming case? <@U06RU2JKMDG> \n\u2022 Let me know if you have any trouble accessing mongodb. I have given permissions to a new cluster.\n\u2022 Thursday works fine for me. <@U06855K24SE> ?",
        "199": "Hi <@U066Q9JAU3B>, this error rate represents the probability of error occurrence when receiving an internal JSON file from the model output. It doesn't directly influence 'null' topics; instead, it demonstrates that the server has become much more stable and robust in responding to users. Additionally, this has had an influence on the speed of message processing. Currently, the speed is approximately 1500 messages per minute.",
        "200": "got it. Could you please navigate to <https:\/\/www.mongodb.com\/> , login and confirm you are able to see data on the 'InformationIntelligence' cluster? If not, we will need to do your IP whitelisting.",
        "201": "<@U066Q9JAU3B>, yes, I'm able to access 'ClusterO' in 'InformationIntelligence'.  Thanks",
        "202": "I also can access the 'InformationIntelligence' cluster. Thanks.",
        "203": "<@U06RU2JKMDG> is this change committed to the InformationIntelligence Github repo? <https:\/\/github.com\/TranscendHQ\/InformationIntelligence>\n&gt; this error rate represents the probability of error occurrence when receiving an internal JSON file from the model output. It doesn't directly influence 'null' topics; instead, it demonstrates that the server has become much more stable and robust in responding to users. Additionally, this has had an influence on the speed of message processing. Currently, the speed is approximately 1500 messages per minute.",
        "204": "Yes <@U066Q9JAU3B>, i had made this change before you forked a repo",
        "205": "<@U047WSYBE5T> let's stick to Friday for our meeting. We need slightly more time to properly define the 'recommendation engine' piece of the project. It's a complex undertaking, and half-baked requirements will hurt the team more. It's worth taking an extra day to define it concretely.",
        "206": "<@U047WSYBE5T> <@U05R0KHPU8N> <@U06RU2JKMDG>\nMeanwhile, we have 2 items on the list, until we finalize the next steps\n\u2022 mongodb integration\n\u2022 fixing the topic clustering model - it should not assign 'null' values. all messages have a valid topic.",
        "207": "Gotcha, Yogesh.\nI'm on it!",
        "208": "Hi <@U066Q9JAU3B>, noted, thanks for the update",
        "209": "<@U047WSYBE5T> Srikanth is still working hard to go through several academic papers and define the recommendation engine product requirements.\nHe will be working late night today and we will be working through the weekend to finish the specifications.\n\nFor tomorrow morning's meeting only I will join. We can discuss the status of mongodb integration and 'null title' bug fix.\n\nPlease hold off on other activities. I will get you the PRD by sunday night PST so that Monday onwards the team can start working on it.\n\nLastly, I would appreciate if we can meet a bit later (8:30 am PST if possible, because I am also planning to work late.)",
        "210": "Hi <@U066Q9JAU3B>, thanks for the update. We're currently focusing on those two tasks. Taras will share the status shortly.\nRegarding today's meeting, I'm rescheduling it to 8:30 am PST for a more detailed progress discussion. Unfortunately, I won\u2019t be able to attend, but Taras will join.\nNoted that on Monday, we'll have a scope to start working on.\nQuick question: Is the meeting time change only for today, or is it permanent?",
        "211": "Hey, <@U066Q9JAU3B>.\n\nRegarding mongo, I plan to add new db called \"archive\" and each collection inside will correspond to a chat. When the user want to update the existing chat with new messages, instead of reading from csv file (as it currently is), reading from mongo will take place.\n\nAs for null handling, the mentioned approach with asking LLM to fill null values based on set of all topics is not the best solution, as the number of topics grows with increasing number of messages. Therefore I am currently investigating when and why the LLM returns null. I had a hypothesis that most of nulls appear in the middle of a batch (LLM usually tend to forget info at the middle), but that's not the case and null indexes inside the batch seem to have uniform distribution.\n\nFeel free to prepare your questions\/suggestions for today meeting.",
        "212": "<@U05R0KHPU8N> are you working on the mongodb integration or the null issue?",
        "213": "MongoDB",
        "214": "ok. look forward to our meeting today and update on the progress :+1:",
        "215": "<@U047WSYBE5T> Meeting time request was just for today as we were both working late nights yesterday.",
        "216": "Good meeting <@U05R0KHPU8N> <@U047WSYBE5T>, here are my notes\n1. Mongodb integration\n    a. The model now stores the output in 'archive' database of mongodb. Please do checkin this code on GitHub, so I can test it out.\n    b. Next step: Ensure input messages are also in mongodb. We will need to modify the api such that we can pass only the collection name, number of messages etc. as input.\n2. null topics\n    a. LLM is weird. It seems to generate null topics about 10% currently. Even changing the message_id length changes the behavior significantly.\n    b. We could experiment if decreasing the BATCH_SIZE from 40 to a lower value reduces null size.\n    c. We could also experiment with having overlap between input messages. Overlap should be chosen intelligently. For example: for new batch,  append all the messages from the previous 2 topics (instead of appending constant number of messages)\n    d. If after all of this, we still get null topic for a given message, we could simple ask another LLM to compare the probabiliy score of that message being part of previous 3 topics (by providing messages of 3 topics ). Then we simply classify the message into the group with highest probability. \n3. We briefly discussed upon the 'semantic filtering' topic.\n    a. Me and <@U06855K24SE> are reviewing the literature. The space is quite complicated. We will work through the weekend to define the project scope with maximum chances of success.\n    b. We are currently posing the problem as recommendation system, where user's interactions (like, dislike, etc.) are a modality. LLMs have not been trained on this modality, so they will not perform well.\n    c. We could instead choose to redefine the problem as a classification problem. For example: user's input &lt;I want to be kept informed about all discussions regarding the Australian customers\". Now for every new topic, we can ask the LLM to classify whether or not this topic is related to Australian customers.\n    d. Similarly, we can transform the user's action into language task in the following manner.\n        i. for each topic the user likes, put all the messages in these topics and put them in group A. Calculate the embeddingA.\n        ii. for each topic the user dislikes, put all the messages in these topics and put them in group B. Calculate the embeddingB.\n        iii. For new topic, calculate the embeddingNew.\n        iv. Calculate the similarity between embeddingNew, embeddingA, embeddingB.\n        v. If similarity(embeddingA, embeddingNew) &gt; similarity(embeddingB, embeddingNew), show it to the user. Otherwise, don't. ",
        "217": "Hi <@U05R0KHPU8N>\nI took a stab at some LLM experimentation for 2.c and 2.d while <@U06855K24SE> is researching literature.\nPlease find the following pdf attachments with prompts\/inputs\/results.\nHopefully they are self-explanatory.\nI would love to get your feedback on them and see if we could take these approaches to build a reasonable solution.\nHappy to get on a quick call as needed.",
        "218": "Hi, <@U066Q9JAU3B> thanks!\nI will take a look. ",
        "219": "edit: above attachment is for 3.c and 3.d and not 2.c and 2.d as previously mentioned.",
        "220": "The <https:\/\/medium.com\/etoai\/hybrid-search-combining-bm25-and-semantic-search-for-better-results-with-lan-1358038fe7e6|article> about hybrid search.",
        "221": "Hi <@U066Q9JAU3B> and <@U06855K24SE>, hope you both doing well.\n*Follow up:*\n1. *Mongodb integration*. We discussed the status of the task and we aim to complete the task by tomorrow. \n2. *Null topics.* As we discussed in the meeting, we will be conducting experiments based on the information <https:\/\/litslink.slack.com\/archives\/C06TV6JD06P\/p1715359275393119|above>. Please note that there is a risk that short messages such as \"yes\" and \"ok\" still might be difficult to classify.\n3. *Semantic filtering topic.* We are planning to start working on it closer to the end of the week once the previous two tasks are completed.\n4. Yogesh will share *testing examples* for two cases *(relevance and like signal)*\n5. We agreed to share *status reports* every workday at 5pm (7am your time), including what we are working on today, what we plan to do next, and any blockers we have.\n<https:\/\/drive.google.com\/file\/d\/1A6al9bwYMude5JfqidbDMlBhxR-B3l1D\/view?usp=sharing>",
        "222": "Thanks for the meeting notes <@U047WSYBE5T>\nregarding point #2, I don't think it matters if the message is short as long as we compare the message against the most recent 2-3 topics. We can ask LLM to consider 'who' sent the message, context of the discussion etc.\nFor example.\n\nTopic 1\n\u2022 <User1>Go programming is great for building web server\n\u2022 <User2> I agree. However it has a steep learning curve\nTopic 2\n\u2022 <User3> who is working on the clustering problem?\n\u2022 <User4> Taras\nUnclassified message\n\u2022 <User1> Yes.\nI am pretty sure that LLM is able to detect that the unclassified message belongs to topic one. Please feel free to try this with the openai playground.\n<https:\/\/platform.openai.com\/playground\/chat?mode=chat&model=gpt-4o&models=gpt-3.5-turbo-16k-0613>\n\n\nRegarding 4, I am going to generate some synthetic test data",
        "223": "<@U05R0KHPU8N> please take a look :slightly_smiling_face:",
        "224": "<@U05R0KHPU8N> <@U047WSYBE5T> <@U06855K24SE> See, I tried with prompting with short message , and LLM behaves exactly how I predicted. It is time we start giving the reasoning capability of LLMs little bit more credit.\nI suspect that if we modified the original prompt in topic clustering to include the reason for every message, we might not have null topics at all!!",
        "225": "Reminder: Don\u2019t forget to send a status report following the conscience: 1. Yesterday I did... 2. I am gonna focus on...3. I am blocked by... at 5 PM every weekday.!",
        "226": "Hey.\nToday I am blocked again by problems with the server api. I spent 2 hours investigating and have no clues what can go wrong... Will continue\nIf someone is able to help -- let me know. I plan to have a call with Vlad in 1.5 hours, maybe he know what is the problem.",
        "227": "FYI. We decided that runpod is too unreliable. Taras is unblocked now, but he had to create a whole new VM instance. It is an unnecessary waste of time.\n\nYogesh will deploy an LLM endpoint and share the link.\n\n<@U05R0KHPU8N> please let me know the exact model\/version you want to use",
        "228": "ok.. I saw it from the code.. <@U05R0KHPU8N> please confirm if this is the model\/version you want to use\n```mistralai\/Mistral-7B-Instruct-v0.2```",
        "229": "Yes, <@U066Q9JAU3B> that is correct.",
        "230": "Also we talked about Llama 8b as a second option, just in case",
        "231": "ok. I will start with mistral first. If needed, we can add llama 8b support.\n\nWill the prompt template be the same for these 2 models be exactly the same and we can switch between the models?",
        "232": "I am not sure about that. The prompt template could be the same, but it's not guaranteed it'll work.\nBut mistral is ok for now. No need in Llama currently.",
        "233": "<@U05R0KHPU8N>\nI have checked in mistral.py to our git repo in 'yogesh' branch.\nThis example code calls mistral api hosted serverless on cloudflare. It is way cheaper to use this vs dedicated server (especially when dedicated server is highly unreliable).\n\nBefore running this code, you will need to set CLOUDFLARE_CLOUDFLARE_AUTH_TOKEN env variable\n``` export CLOUDFLARE_CLOUDFLARE_AUTH_TOKEN=Xtt1g1-zL_I_vHRAcLkCrEZLdNgCAqinUY152fNO```\nOnce we have a created a good enough working solution, that meets project goals, I will install the model on a dedicated cluster with GPU.\n\nAlso, when we are playing around with prompting, we can literally play around in cloudflare playground. No need to write any python code.\nsee here.\n<https:\/\/playground.ai.cloudflare.com\/?model=@hf\/mistral\/mistral-7b-instruct-v0.2>\n\nLastly, the mistral api returns the response in a single shot. THere is also a provision to get the streaming response. THen you will have to handle the streaming response at the client level. I didn't want to deal with all that. <https:\/\/developers.cloudflare.com\/workers-ai\/models\/mistral-7b-instruct-v0.2\/>\n\nWith using cloudflare endpoint, we no longer need to run the fastapi server on runpod, bypassing all the reliability issues. I haven't integrated this endpoint with our topic clustering code, as I am not familiar with it.  Hope it is 1 or 2 lines of code change.\n\nplease let me know any questions.",
        "234": "<@U066Q9JAU3B> thanks.\nOk, I can see `mistral.py`  . I will replace a bit later the current LLM call with the one you suggested. It's a bit more than 2 line of code change, but still it's not a lot of work.\nAlso thanks for sharing prompting playground, that will come in handy.\nAdditionally, regarding streaming response: as we are moving away from runpod, we no longer need to provide 8 Kb reponse each 100 secs (this is currently a limitation). And I believe single shot response should work fine.",
        "235": "<@U05R0KHPU8N> let's get on a call and try to investigate together.",
        "236": "Sure, I\u2019ll send a link in a minute",
        "237": "I am available now",
        "238": "<https:\/\/meet.google.com\/vyq-kzva-kra>",
        "239": "Hey, my update for today: I finished mongodb integration and now it's available on the github on my branch. Feel free to test or ask questions.\n\nAlso using mongodb allowed to simplify the logic of the server api, so the number of lines of code dropped from 325 to 195.\n\nAccording to the plan tomorrow I'll start working on null handling (experiments with lower batch size and adjusting prompts to utilize reasoning etc.)\n\nAs for later I can adjust the code base so that the server can be run locally in combination with cloudflare endpoint. It will even more simplimfy the server api logic, because currently the server will crush if less than 8 Kb of data is sent in 100 secs (which is obviously a limitation). One concern in my mind: may switching from vllm to cloudflare endpoint slow down the total time?:man-shrugging:\n\nI am off for today, bb",
        "240": "thanks for the update <@U05R0KHPU8N>.\nIt's a possibility that switching from runpod dedicated to serverless may slow down the inference throughput.\nIt will be good to keep our options open - as in, keep the code\/functionality to call the llm instantiated by model deployed on dedicated VM\/GPU.\n\nand just using a flag to switch between the two. Here is the psuedo code\n\n```launch_fast_api_server_on_port_8000()\n\nuse_runpod = os.getenv(\"USE_RUNPOD\")\n\n\napi_to_create_topic_cluster(input):\n   if use_runpod:\n      llm = LLM('mistral on vLLM\")\n      do_topic_clustering_on_runpod(input, llm)\n   else:\n      do_topic_clustering_on_cloudflare(input) ## use cloudflare rest api```\nWe can then seamlessly switch between running on runpod (or even Azure) and cloudflare. Hope this helps.",
        "241": "I got what you mean.\nIt's nice to have 2 options and being able to switch easily.",
        "242": "Reminder: Don\u2019t forget to send a status report following the conscience: 1. Yesterday I did... 2. I am gonna focus on...3. I am blocked by... at 5 PM every weekday.!",
        "243": "Hey, my update for today:\nI experimented with batch size and this proved out hypothesis that the batch size should be less than 40. But if it too small (e.g. 5), LLM starts to break formatting of the output json, consequently messages need to be clustered by LLM again.\nIt looks like the number between 5-15 is optimal.\nbatch size -- null %\n40 -- 34.3%\n20 -- 25.7%\n10 -- 16.9%\n5 -- 7%\n\n\nI plan to spend next 4-5 hours on experiments with overlapping and adding reasoning to prompt. If null still appear in the output, I'll create a second call to LLM to classify nulls, having messages from previous 2-3 topics.\nI'll write another update at the end of my workday today.",
        "244": "yeah.. making batch size too small will mean we are not casting big enough net to catch all messages in a conversation.. looks like a second llm may be a better option.\nAre your batches overlapping? they should be i think.",
        "245": "No, currently there is no overlap (I need to adjust the backend to allow overlap and handle it correctly).",
        "246": "without an overlapping window, we have no way to determine conversation boundaries and we risk forcing a message inside a conversation it does not belong.\nThe overlap should be more intelligent (instead of hardcoding to, say, 10 messages, we could simply include all messages from last 2 topics with the new batch.. this gives a chance to the LLMs to pick up new topics for the messages misclassified in the previous run).",
        "247": "<@U066Q9JAU3B> Indeed, my recent experiments show this:\nbatch size 12\n\noverlap\n0 -- 17.4%\n2 -- 14.6%\n4 -- 11.9%\n6 -- 4%\n8 -- 1.5%\n10 -- 0.2%\n\nwhich I believe supports your statement.",
        "248": "with the overlap, we can even increase the batch size .. infact I would love to see now the figures with batch size of 25, 30, 40, 50, 60 etc..",
        "249": "Ok, I'll start with that tomorrow.\nI am of for today, bb"
    }
}